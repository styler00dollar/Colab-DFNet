{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFNet-lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsxYKkWLXju5"
      },
      "source": [
        "# Colab-DFNet (pytorch lightning)\n",
        "\n",
        "Porting my pytorch repo [styler00dollar/Colab-DFNet](https://github.com/styler00dollar/Colab-DFNet) to pytorch-lightning. Uses [Yukariin/DFNet](https://github.com/Yukariin/DFNet) as a base. [This tutorial](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09), [this issue](https://stackoverflow.com/questions/65387967/misconfigurationerror-no-tpu-devices-were-found-even-when-tpu-is-connected-in)  and [this Colab](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/03-basic-gan.ipynb#scrollTo=3vKszYf6y1Vv) were very helpful. This Colab does support single-GPU, multi-GPU and TPU training.\n",
        "\n",
        "Can use various loss functions and has the context_encoder discriminator as default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPB6KmEOPQW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBmBFu3vTlTX",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "!pip install pytorch-lightning -U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWDUI7DqflrY",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0I03TFfDaqtR"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fSZHn8ayFM"
      },
      "source": [
        "# copy data somehow\n",
        "!mkdir '/content/data'\n",
        "!mkdir '/content/data/images'\n",
        "!cp \"/content/drive/MyDrive/data.7z\" \"/content/data/images/data.7z\"\n",
        "%cd /content/data/images\n",
        "!7z x \"data.7z\"\n",
        "!rm -rf /content/data/images/data.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQYzPLBdbD-w",
        "cellView": "form"
      },
      "source": [
        "#@title getting pytorch-loss-functions\n",
        "%cd /content\n",
        "!git clone https://github.com/styler00dollar/pytorch-loss-functions pytorchloss\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-vTO-jUfMhT"
      },
      "source": [
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EfvO0fBULzf",
        "cellView": "form"
      },
      "source": [
        "#@title utils.py\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_like(x, target, mode='bilinear'):\n",
        "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
        "\n",
        "\n",
        "def list2nparray(lst, dtype=None):\n",
        "    \"\"\"fast conversion from nested list to ndarray by pre-allocating space\"\"\"\n",
        "    if isinstance(lst, np.ndarray):\n",
        "        return lst\n",
        "    assert isinstance(lst, (list, tuple)), 'bad type: {}'.format(type(lst))\n",
        "    assert lst, 'attempt to convert empty list to np array'\n",
        "    if isinstance(lst[0], np.ndarray):\n",
        "        dim1 = lst[0].shape\n",
        "        assert all(i.shape == dim1 for i in lst)\n",
        "        if dtype is None:\n",
        "            dtype = lst[0].dtype\n",
        "            assert all(i.dtype == dtype for i in lst), \\\n",
        "                'bad dtype: {} {}'.format(dtype, set(i.dtype for i in lst))\n",
        "    elif isinstance(lst[0], (int, float, complex, np.number)):\n",
        "        return np.array(lst, dtype=dtype)\n",
        "    else:\n",
        "        dim1 = list2nparray(lst[0])\n",
        "        if dtype is None:\n",
        "            dtype = dim1.dtype\n",
        "        dim1 = dim1.shape\n",
        "    shape = [len(lst)] + list(dim1)\n",
        "    rst = np.empty(shape, dtype=dtype)\n",
        "    for idx, i in enumerate(lst):\n",
        "        rst[idx] = i\n",
        "    return rst\n",
        "\n",
        "\n",
        "def get_img_list(path):\n",
        "    return sorted(list(Path(path).glob('*.png'))) + \\\n",
        "        sorted(list(Path(path).glob('*.jpg'))) + \\\n",
        "        sorted(list(Path(path).glob('*.jpeg')))\n",
        "\n",
        "\n",
        "def gen_miss(img, mask, output):\n",
        "\n",
        "    imgs = get_img_list(img)\n",
        "    masks = get_img_list(mask)\n",
        "    print('Total images:', len(imgs), len(masks))\n",
        "\n",
        "    out = Path(output)\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for i, (img, mask) in tqdm(enumerate(zip(imgs, masks))):\n",
        "        path = out.joinpath('miss_%04d.png' % (i+1))\n",
        "        img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
        "        mask = cv2.imread(str(mask), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, img.shape[:2][::-1])\n",
        "        mask = mask[..., np.newaxis]\n",
        "        miss = img * (mask > 127) + 255 * (mask <= 127)\n",
        "        cv2.imwrite(str(path), miss)\n",
        "\n",
        "def merge_imgs(dirs, output, row=1, gap=2, res=512):\n",
        "\n",
        "    image_list = [get_img_list(path) for path in dirs]\n",
        "    img_count = [len(image) for image in image_list]\n",
        "    print('Total images:', img_count)\n",
        "    assert min(img_count) > 0, 'Please check the path of empty folder.'\n",
        "\n",
        "    output_dir = Path(output)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    n_img = len(dirs)\n",
        "    row = row\n",
        "    column = (n_img - 1) // row + 1\n",
        "    print('Row:', row)\n",
        "    print('Column:', column)\n",
        "\n",
        "    for i, unit in tqdm(enumerate(zip(*image_list))):\n",
        "        name = output_dir.joinpath('merge_%04d.png' % i)\n",
        "        merge = np.ones([\n",
        "            res*row + (row+1)*gap, res*column + (column+1)*gap, 3], np.uint8) * 255\n",
        "        for j, img in enumerate(unit):\n",
        "            r = j // column\n",
        "            c = j - r * column\n",
        "            img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
        "            if img.shape[:2] != (res, res):\n",
        "                img = cv2.resize(img, (res, res))\n",
        "            start_h, start_w = (r + 1) * gap + r * res, (c + 1) * gap + c * res\n",
        "            merge[start_h: start_h + res, start_w: start_w + res] = img\n",
        "        cv2.imwrite(str(name), merge)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "SzAz39HJuJGy"
      },
      "source": [
        "#@title metrics.py (removing lpips import)\n",
        "%%writefile /content/pytorchloss/metrics.py\n",
        "#https://github.com/huster-wgm/Pytorch-metrics/blob/master/metrics.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\"\"\"\n",
        "  @Email:  guangmingwu2010@gmail.com \\\n",
        "           guozhilingty@gmail.com\n",
        "  @Copyright: go-hiroaki & Chokurei\n",
        "  @License: MIT\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import lpips\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "def _binarize(y_data, threshold):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : [float] 4-d tensor in [batch_size, channels, img_rows, img_cols]\n",
        "        threshold : [float] [0.0, 1.0]\n",
        "    return 4-d binarized y_data\n",
        "    \"\"\"\n",
        "    y_data[y_data < threshold] = 0.0\n",
        "    y_data[y_data >= threshold] = 1.0\n",
        "    return y_data\n",
        "\n",
        "def _argmax(y_data, dim):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : 4-d tensor in [batch_size, chs, img_rows, img_cols]\n",
        "        dim : int\n",
        "    return 3-d [int] y_data\n",
        "    \"\"\"\n",
        "    return torch.argmax(y_data, dim).int()\n",
        "\n",
        "\n",
        "def _get_tp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "        y_pred : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_positive\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_fp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_positive\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_tn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_negative\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_negative\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_weights(y_true, nb_ch):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        nb_ch : int\n",
        "    return [float] weights\n",
        "    \"\"\"\n",
        "    batch_size, img_rows, img_cols = y_true.shape\n",
        "    pixels = batch_size * img_rows * img_cols\n",
        "    weights = [torch.sum(y_true==ch).item() / pixels for ch in range(nb_ch)]\n",
        "    return weights\n",
        "\n",
        "\n",
        "class CFMatrix(object):\n",
        "    def __init__(self, des=None):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ConfusionMatrix\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return confusion matrix\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 4).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch), :] = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class OAAcc(object):\n",
        "    def __init__(self, des=\"Overall Accuracy\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"OAcc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (tp+tn)/total\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "\n",
        "        nb_tp_tn = torch.sum(y_true == y_pred).float()\n",
        "        mperforms = nb_tp_tn / (batch_size * img_rows * img_cols)\n",
        "        performs = None\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Precision(object):\n",
        "    def __init__(self, des=\"Precision\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Prec\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fp)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Recall(object):\n",
        "    def __init__(self, des=\"Recall\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Reca\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fn)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class F1Score(object):\n",
        "    def __init__(self, des=\"F1Score\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"F1Sc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return 2*precision*recall/(precision+recall)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = 2 * _precision * _recall / (_precision + _recall + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "                _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "                performs[int(ch)] = 2 * _precision * \\\n",
        "                    _recall / (_precision + _recall + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Kappa(object):\n",
        "    def __init__(self, des=\"Kappa\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Kapp\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (Po-Pe)/(1-Pe)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "            Po = (nb_tp + nb_tn) / nb_total\n",
        "            Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn) +\n",
        "                  (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "            mperforms = (Po - Pe) / (1 - Pe + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "                Po = (nb_tp + nb_tn) / nb_total\n",
        "                Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn)\n",
        "                      + (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "                performs[int(ch)] = (Po - Pe) / (1 - Pe + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Jaccard(object):\n",
        "    def __init__(self, des=\"Jaccard\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Jacc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return intersection / (sum-intersection)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            _intersec = torch.sum(y_true * y_pred).float()\n",
        "            _sum = torch.sum(y_true + y_pred).float()\n",
        "            mperforms = _intersec / (_sum - _intersec + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                _intersec = torch.sum(y_true_ch * y_pred_ch).float()\n",
        "                _sum = torch.sum(y_true_ch + y_pred_ch).float()\n",
        "                performs[int(ch)] = _intersec / (_sum - _intersec + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class MSE(object):\n",
        "    def __init__(self, des=\"Mean Square Error\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return mean_squared_error, smaller the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "\n",
        "class PSNR(object):\n",
        "    def __init__(self, des=\"Peak Signal to Noise Ratio\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"PSNR\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return PSNR, larger the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        mse = torch.mean((y_pred - y_true) ** 2)\n",
        "        return 10 * torch.log10(1 / mse)\n",
        "\n",
        "\n",
        "class SSIM(object):\n",
        "    '''\n",
        "    modified from https://github.com/jorge-pessoa/pytorch-msssim\n",
        "    '''\n",
        "    def __init__(self, des=\"structural similarity index\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SSIM\"\n",
        "\n",
        "    def gaussian(self, w_size, sigma):\n",
        "        gauss = torch.Tensor([math.exp(-(x - w_size//2)**2/float(2*sigma**2)) for x in range(w_size)])\n",
        "        return gauss/gauss.sum()\n",
        "\n",
        "    def create_window(self, w_size, channel=1):\n",
        "        _1D_window = self.gaussian(w_size, 1.5).unsqueeze(1)\n",
        "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "        window = _2D_window.expand(channel, 1, w_size, w_size).contiguous()\n",
        "        return window\n",
        "\n",
        "    def __call__(self, y_pred, y_true, w_size=11, size_average=True, full=False):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            w_size : int, default 11\n",
        "            size_average : boolean, default True\n",
        "            full : boolean, default False\n",
        "        return ssim, larger the better\n",
        "        \"\"\"\n",
        "        # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "        if torch.max(y_pred) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(y_pred) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "\n",
        "        padd = 0\n",
        "        (_, channel, height, width) = y_pred.size()\n",
        "        window = self.create_window(w_size, channel=channel).to(y_pred.device)\n",
        "\n",
        "        mu1 = F.conv2d(y_pred, window, padding=padd, groups=channel)\n",
        "        mu2 = F.conv2d(y_true, window, padding=padd, groups=channel)\n",
        "\n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "\n",
        "        sigma1_sq = F.conv2d(y_pred * y_pred, window, padding=padd, groups=channel) - mu1_sq\n",
        "        sigma2_sq = F.conv2d(y_true * y_true, window, padding=padd, groups=channel) - mu2_sq\n",
        "        sigma12 = F.conv2d(y_pred * y_true, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "        C1 = (0.01 * L) ** 2\n",
        "        C2 = (0.03 * L) ** 2\n",
        "\n",
        "        v1 = 2.0 * sigma12 + C2\n",
        "        v2 = sigma1_sq + sigma2_sq + C2\n",
        "        cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "        ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "        if size_average:\n",
        "            ret = ssim_map.mean()\n",
        "        else:\n",
        "            ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "        if full:\n",
        "            return ret, cs\n",
        "        return ret\n",
        "\n",
        "\n",
        "class LPIPS(object):\n",
        "    '''\n",
        "    borrowed from https://github.com/richzhang/PerceptualSimilarity\n",
        "    '''\n",
        "    def __init__(self, cuda, des=\"Learned Perceptual Image Patch Similarity\", version=\"0.1\"):\n",
        "        self.des = des\n",
        "        self.version = version\n",
        "        self.model = lpips.PerceptualLoss(model='net-lin',net='alex',use_gpu=cuda)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LPIPS\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, normalized=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            normalized : change [0,1] => [-1,1] (default by LPIPS)\n",
        "        return LPIPS, smaller the better\n",
        "        \"\"\"\n",
        "        if normalized:\n",
        "            y_pred = y_pred * 2.0 - 1.0\n",
        "            y_true = y_true * 2.0 - 1.0\n",
        "        return self.model.forward(y_pred, y_true)\n",
        "\n",
        "\n",
        "class AE(object):\n",
        "    \"\"\"\n",
        "    Modified from matlab : colorangle.m, MATLAB V2019b\n",
        "    angle = acos(RGB1' * RGB2 / (norm(RGB1) * norm(RGB2)));\n",
        "    angle = 180 / pi * angle;\n",
        "    \"\"\"\n",
        "    def __init__(self, des='average Angular Error'):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "        return average AE, smaller the better\n",
        "        \"\"\"\n",
        "        dotP = torch.sum(y_pred * y_true, dim=1)\n",
        "        Norm_pred = torch.sqrt(torch.sum(y_pred * y_pred, dim=1))\n",
        "        Norm_true = torch.sqrt(torch.sum(y_true * y_true, dim=1))\n",
        "        ae = 180 / math.pi * torch.acos(dotP / (Norm_pred * Norm_true + eps))\n",
        "        return ae.mean(1).mean(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for ch in [3, 1]:\n",
        "        batch_size, img_row, img_col = 1, 224, 224\n",
        "        y_true = torch.rand(batch_size, ch, img_row, img_col)\n",
        "        noise = torch.zeros(y_true.size()).data.normal_(0, std=0.1)\n",
        "        y_pred = y_true + noise\n",
        "        for cuda in [False, True]:\n",
        "            if cuda:\n",
        "                y_pred = y_pred.cuda()\n",
        "                y_true = y_true.cuda()\n",
        "\n",
        "            print('#'*20, 'Cuda : {} ; size : {}'.format(cuda, y_true.size()))\n",
        "            ########### similarity metrics\n",
        "            metric = MSE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = PSNR()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = SSIM()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = LPIPS(cuda)\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = AE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            ########### accuracy metrics\n",
        "            metric = OAAcc()\n",
        "            maccu, accu = metric(y_pred, y_true)\n",
        "            print('mAccu:', maccu, 'Accu', accu)\n",
        "\n",
        "            metric = Precision()\n",
        "            mprec, prec = metric(y_pred, y_true)\n",
        "            print('mPrec:', mprec, 'Prec', prec)\n",
        "\n",
        "            metric = Recall()\n",
        "            mreca, reca = metric(y_pred, y_true)\n",
        "            print('mReca:', mreca, 'Reca', reca)\n",
        "\n",
        "            metric = F1Score()\n",
        "            mf1sc, f1sc = metric(y_pred, y_true)\n",
        "            print('mF1sc:', mf1sc, 'F1sc', f1sc)\n",
        "\n",
        "            metric = Kappa()\n",
        "            mkapp, kapp = metric(y_pred, y_true)\n",
        "            print('mKapp:', mkapp, 'Kapp', kapp)\n",
        "\n",
        "            metric = Jaccard()\n",
        "            mjacc, jacc = metric(y_pred, y_true)\n",
        "            print('mJacc:', mjacc, 'Jacc', jacc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HozYJLs0B67",
        "cellView": "form"
      },
      "source": [
        "#@title partialconv.py\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "#\n",
        "# Source: https://github.com/NVIDIA/partialconv/blob/master/models/partialconv2d.py\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, cuda\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False  \n",
        "\n",
        "        if 'return_mask' in kwargs:\n",
        "            self.return_mask = kwargs['return_mask']\n",
        "            kwargs.pop('return_mask')\n",
        "        else:\n",
        "            self.return_mask = False\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "            \n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None, None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask_in=None):\n",
        "        assert len(input.shape) == 4\n",
        "        if mask_in is not None or self.last_size != tuple(input.shape):\n",
        "            self.last_size = tuple(input.shape)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask_in is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                else:\n",
        "                    mask = mask_in\n",
        "                        \n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                #make sure the value of self.mask_ratio for the entries in the interior (no need for padding) have value 1. If not, you replace with the line below.\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        # if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "        #     self.update_mask.to(input)\n",
        "        #     self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "x-5xc1MR0F-K"
      },
      "source": [
        "#@title deformconv2d.py\n",
        "import torch.nn as nn\n",
        "import torchvision.ops as O\n",
        "\n",
        "\n",
        "class DeformConv2d(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
        "        super(DeformConv2d, self).__init__()\n",
        "\n",
        "        self.conv_offset = nn.Conv2d(in_nc, 2 * (kernel_size**2), kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.conv_offset.weight.data.zero_()\n",
        "        self.conv_offset.bias.data.zero_()\n",
        "\n",
        "        self.dcn_conv = O.DeformConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset = self.conv_offset(x)\n",
        "        return self.dcn_conv(x, offset=offset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_-I2mZx9Ru"
      },
      "source": [
        "Inside the model it is possible to configure loss functions and weights. Warning: Don't use AMP with StyleLoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfjYt7dyOcvQ",
        "cellView": "form"
      },
      "source": [
        "#@title model.py\n",
        "# https://github.com/hughplay/DFNet\n",
        "# https://github.com/Yukariin/DFNet\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "#from utils import resize_like\n",
        "\n",
        "#from .convolutions import partialconv2d\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "\n",
        "def resize_like(x, target, mode='bilinear'):\n",
        "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
        "\n",
        "\n",
        "def get_norm(name, out_channels):\n",
        "    if name == 'batch':\n",
        "        norm = nn.BatchNorm2d(out_channels)\n",
        "    elif name == 'instance':\n",
        "        norm = nn.InstanceNorm2d(out_channels)\n",
        "    else:\n",
        "        norm = None\n",
        "    return norm\n",
        "\n",
        "\n",
        "def get_activation(name):\n",
        "    if name == 'relu':\n",
        "        activation = nn.ReLU()\n",
        "    elif name == 'elu':\n",
        "        activation == nn.ELU()\n",
        "    elif name == 'leaky_relu':\n",
        "        activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "    elif name == 'tanh':\n",
        "        activation = nn.Tanh()\n",
        "    elif name == 'sigmoid':\n",
        "        activation = nn.Sigmoid()\n",
        "    else:\n",
        "        activation = None\n",
        "    return activation\n",
        "\n",
        "\n",
        "class Conv2dSame(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        padding = self.conv_same_pad(kernel_size, stride)\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          # original\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = nn.Conv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  nn.Conv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "        elif conv_type == 'partial':\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = PartialConv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  PartialConv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "\n",
        "        elif conv_type == 'deform':\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = PartialConv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  DeformConv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "\n",
        "    def conv_same_pad(self, ksize, stride):\n",
        "        if (ksize - stride) % 2 == 0:\n",
        "            return (ksize - stride) // 2\n",
        "        else:\n",
        "            left = (ksize - stride) // 2\n",
        "            right = left + 1\n",
        "            return left, right\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ConvTranspose2dSame(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        padding, output_padding = self.deconv_same_pad(kernel_size, stride)\n",
        "        self.trans_conv = nn.ConvTranspose2d(\n",
        "            in_channels, out_channels, kernel_size, stride,\n",
        "            padding, output_padding)\n",
        "\n",
        "    def deconv_same_pad(self, ksize, stride):\n",
        "        pad = (ksize - stride + 1) // 2\n",
        "        outpad = 2 * pad + stride - ksize\n",
        "        return pad, outpad\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.trans_conv(x)\n",
        "\n",
        "\n",
        "class UpBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, mode='nearest', scale=2, channel=None, kernel_size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        if mode == 'deconv':\n",
        "            self.up = ConvTranspose2dSame(\n",
        "                channel, channel, kernel_size, stride=scale)\n",
        "        else:\n",
        "            def upsample(x):\n",
        "                return F.interpolate(x, scale_factor=scale, mode=mode)\n",
        "            self.up = upsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "\n",
        "class EncodeBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, conv_type, kernel_size, stride,\n",
        "            normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_in = in_channels\n",
        "        self.c_out = out_channels\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Conv2dSame(self.c_in, self.c_out, conv_type, kernel_size, stride))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, self.c_out))\n",
        "        if activation:\n",
        "            layers.append(get_activation(activation))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class DecodeBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, c_from_up, c_from_down, conv_type, c_out, mode='nearest',\n",
        "            kernel_size=4, scale=2, normalization='batch', activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_from_up = c_from_up\n",
        "        self.c_from_down = c_from_down\n",
        "        self.c_in = c_from_up + c_from_down\n",
        "        self.c_out = c_out\n",
        "\n",
        "        self.up = UpBlock(mode, scale, c_from_up, kernel_size=scale)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Conv2dSame(self.c_in, self.c_out, conv_type, kernel_size, stride=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, self.c_out))\n",
        "        if activation:\n",
        "            layers.append(get_activation(activation))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, concat=None):\n",
        "        out = self.up(x)\n",
        "        if self.c_from_down > 0:\n",
        "            out = torch.cat([out, concat], dim=1)\n",
        "        out = self.decode(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BlendBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, c_in, c_out, conv_type, ksize_mid=3, norm='batch', act='leaky_relu'):\n",
        "        super().__init__()\n",
        "        c_mid = max(c_in // 2, 32)\n",
        "        self.blend = nn.Sequential(\n",
        "            Conv2dSame(c_in, c_mid, conv_type, 1, 1),\n",
        "            get_norm(norm, c_mid),\n",
        "            get_activation(act),\n",
        "            Conv2dSame(c_mid, c_out, conv_type, ksize_mid, 1),\n",
        "            get_norm(norm, c_out),\n",
        "            get_activation(act),\n",
        "            Conv2dSame(c_out, c_out, conv_type, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blend(x)\n",
        "\n",
        "\n",
        "class FusionBlock(pl.LightningModule):\n",
        "    def __init__(self, c_feat, conv_type, c_alpha=1):\n",
        "        super().__init__()\n",
        "        c_img = 3\n",
        "        self.map2img = nn.Sequential(\n",
        "            Conv2dSame(c_feat, c_img, conv_type, 1, 1),\n",
        "            nn.Sigmoid())\n",
        "        self.blend = BlendBlock(c_img*2, c_alpha, conv_type)\n",
        "\n",
        "    def forward(self, img_miss, feat_de):\n",
        "        img_miss = resize_like(img_miss, feat_de)\n",
        "        raw = self.map2img(feat_de)\n",
        "        alpha = self.blend(torch.cat([img_miss, raw], dim=1))\n",
        "        result = alpha * raw + (1 - alpha) * img_miss\n",
        "        return result, alpha, raw\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class DFNet(pl.LightningModule):\n",
        "    def __init__(\n",
        "            self, c_img=3, c_mask=1, c_alpha=3,\n",
        "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3]*8,\n",
        "            blend_layers=[0, 1, 2, 3, 4, 5], conv_type = 'normal'):\n",
        "        super().__init__()\n",
        "\n",
        "        c_init = c_img + c_mask\n",
        "\n",
        "        self.n_en = len(en_ksize)\n",
        "        self.n_de = len(de_ksize)\n",
        "        assert self.n_en == self.n_de, (\n",
        "            'The number layer of Encoder and Decoder must be equal.')\n",
        "        assert self.n_en >= 1, (\n",
        "            'The number layer of Encoder and Decoder must be greater than 1.')\n",
        "\n",
        "        assert 0 in blend_layers, 'Layer 0 must be blended.'\n",
        "\n",
        "        self.en = []\n",
        "        c_in = c_init\n",
        "        self.en.append(\n",
        "            EncodeBlock(c_in, 64, conv_type, en_ksize[0], 2, None, None))\n",
        "        for k_en in en_ksize[1:]:\n",
        "            c_in = self.en[-1].c_out\n",
        "            c_out = min(c_in*2, 512)\n",
        "            self.en.append(EncodeBlock(\n",
        "                c_in, c_out, conv_type, k_en, stride=2,\n",
        "                normalization=norm, activation=act_en))\n",
        "\n",
        "        # register parameters\n",
        "        for i, en in enumerate(self.en):\n",
        "            self.__setattr__('en_{}'.format(i), en)\n",
        "\n",
        "        self.de = []\n",
        "        self.fuse = []\n",
        "        for i, k_de in enumerate(de_ksize):\n",
        "\n",
        "            c_from_up = self.en[-1].c_out if i == 0 else self.de[-1].c_out\n",
        "            c_out = c_from_down = self.en[-i-1].c_in\n",
        "            layer_idx = self.n_de - i - 1\n",
        "\n",
        "            self.de.append(DecodeBlock(\n",
        "                c_from_up, c_from_down, conv_type, c_out, mode, k_de, scale=2,\n",
        "                normalization=norm, activation=act_de))\n",
        "            if layer_idx in blend_layers:\n",
        "                self.fuse.append(FusionBlock(c_out, conv_type, c_alpha))\n",
        "            else:\n",
        "                self.fuse.append(None)\n",
        "\n",
        "        # register parameters\n",
        "        for i, de in enumerate(self.de[::-1]):\n",
        "            self.__setattr__('de_{}'.format(i), de)\n",
        "        for i, fuse in enumerate(self.fuse[::-1]):\n",
        "            if fuse:\n",
        "                self.__setattr__('fuse_{}'.format(i), fuse)\n",
        "\n",
        "    def forward(self, img_miss, mask):\n",
        "\n",
        "        out = torch.cat([img_miss, mask], dim=1)\n",
        "        out_en = [out]\n",
        "\n",
        "        for encode in self.en:\n",
        "            out = encode(out)\n",
        "            out_en.append(out)\n",
        "\n",
        "        results = []\n",
        "        for i, (decode, fuse) in enumerate(zip(self.de, self.fuse)):\n",
        "            out = decode(out, out_en[-i-2])\n",
        "            if fuse:\n",
        "                result, alpha, raw = fuse(img_miss, out)\n",
        "                results.append(result)\n",
        "        return results[::-1][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bwPuRzjW40R",
        "cellView": "form"
      },
      "source": [
        "#@title init.py\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    print('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ccjLoMPGoT",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        sample = Image.open(sample_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        mask = DS.random_mask()\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        # apply mask\n",
        "        masked = sample * mask\n",
        "\n",
        "        return masked, mask, sample\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_mask(height=256, width=256,\n",
        "                    min_stroke=1, max_stroke=4,\n",
        "                    min_vertex=1, max_vertex=12,\n",
        "                    min_brush_width_divisor=16, max_brush_width_divisor=10):\n",
        "        mask = np.ones((height, width))\n",
        "\n",
        "        min_brush_width = height // min_brush_width_divisor\n",
        "        max_brush_width = height // max_brush_width_divisor\n",
        "        max_angle = 2*np.pi\n",
        "        num_stroke = np.random.randint(min_stroke, max_stroke+1)\n",
        "        average_length = np.sqrt(height*height + width*width) / 8\n",
        "\n",
        "        for _ in range(num_stroke):\n",
        "            num_vertex = np.random.randint(min_vertex, max_vertex+1)\n",
        "            start_x = np.random.randint(width)\n",
        "            start_y = np.random.randint(height)\n",
        "\n",
        "            for _ in range(num_vertex):\n",
        "                angle = np.random.uniform(max_angle)\n",
        "                length = np.clip(np.random.normal(average_length, average_length//2), 0, 2*average_length)\n",
        "                brush_width = np.random.randint(min_brush_width, max_brush_width+1)\n",
        "                end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
        "                end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
        "\n",
        "                cv2.line(mask, (start_y, start_x), (end_y, end_x), 0., brush_width)\n",
        "\n",
        "                start_x, start_y = end_x, end_y\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.fliplr(mask)\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.flipud(mask)\n",
        "        return mask.reshape((1,)+mask.shape).astype(np.float32) \n",
        "\n",
        "\n",
        "class DS_green_from_mask(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        #sample = Image.open(sample_path).convert('RGB')\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        green_mask = 1-np.all(sample == [0,255,0], axis=-1).astype(int)\n",
        "        #sample = sample*green_mask\n",
        "        green_mask = torch.from_numpy(green_mask)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, green_mask.unsqueeze(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWnqGVcPcvm",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', validation_path: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 256\n",
        "    def setup(self, stage=None):\n",
        "        img_tf = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFNetdataset_train = DS(self.training_dir, img_tf)\n",
        "        self.DFNetdataset_validation = DS(self.validation_dir, img_tf)\n",
        "        self.DFNetdataset_test = DS(self.test_dir)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNKbsu4VrQD4",
        "cellView": "form"
      },
      "source": [
        "#@title discriminator.py\n",
        "\"\"\"\n",
        "models.py (21-12-20)\n",
        "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/context_encoder/models.py\n",
        "\"\"\"\n",
        "class context_encoder(pl.LightningModule):\n",
        "    def __init__(self, channels=3):\n",
        "        super(context_encoder, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = channels\n",
        "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYAV2wG-rbLY",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.netG = DFNet(c_img=3, c_mask=1, c_alpha=3,\n",
        "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3, 3, 3, 3, 3, 3, 3, 3],\n",
        "            blend_layers=[0, 1, 2, 3, 4, 5], conv_type='partial')\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "    \n",
        "    self.netD = context_encoder()\n",
        "    weights_init(self.netD, 'kaiming')\n",
        "\n",
        "    # loss functions\n",
        "    self.l1 = nn.L1Loss()\n",
        "    l_hfen_type = L1CosineSim()\n",
        "    self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "    self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "    self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "    self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "    self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "    self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "    self.OFLoss = OFLoss()\n",
        "    self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "    self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "    self.StyleLoss = StyleLoss()\n",
        "    self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "    self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "    layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "        distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "        use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "    self.MSELoss = torch.nn.MSELoss()\n",
        "\n",
        "    # metrics\n",
        "    self.psnr_metric = PSNR()\n",
        "    self.ssim_metric = SSIM()\n",
        "    self.ae_metric = AE()\n",
        "    self.mse_metric = MSE()\n",
        "\n",
        "\n",
        "  def forward(self, image, masks):\n",
        "      return self.netG(image, masks)\n",
        "\n",
        "  #def adversarial_loss(self, y_hat, y):\n",
        "  #    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "\n",
        "      # train generator\n",
        "      #if optimizer_idx == 0:\n",
        "\n",
        "      # train_batch[0][0] = batch_size\n",
        "      # train_batch[0] = masked\n",
        "      # train_batch[1] = mask\n",
        "      # train_batch[2] = original\n",
        "      #out = self.forward(train_batch[0],train_batch[1])\n",
        "\n",
        "      # generate fake\n",
        "      out = self(train_batch[0],train_batch[1])\n",
        "      # masking, taking original content from HR\n",
        "      out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "      #save_image(out, \"output_after_generator.png\")\n",
        "\n",
        "      total_loss = 0\n",
        "      \"\"\"\n",
        "      HFENLoss_forward = self.HFENLoss(out, train_batch[0])\n",
        "      total_loss += HFENLoss_forward\n",
        "      ElasticLoss_forward = self.ElasticLoss(out, train_batch[0])\n",
        "      total_loss += ElasticLoss_forward\n",
        "      RelativeL1_forward = self.RelativeL1(out, train_batch[0])\n",
        "      total_loss += RelativeL1_forward\n",
        "      \"\"\"\n",
        "      L1CosineSim_forward = 5*self.L1CosineSim(out, train_batch[2])\n",
        "      total_loss += L1CosineSim_forward\n",
        "      self.log('loss/L1CosineSim', L1CosineSim_forward)\n",
        "\n",
        "      \"\"\"\n",
        "      ClipL1_forward = self.ClipL1(out, train_batch[0])\n",
        "      total_loss += ClipL1_forward\n",
        "      FFTloss_forward = self.FFTloss(out, train_batch[0])\n",
        "      total_loss += FFTloss_forward\n",
        "      OFLoss_forward = self.OFLoss(out)\n",
        "      total_loss += OFLoss_forward\n",
        "      GPLoss_forward = self.GPLoss(out, train_batch[0])\n",
        "      total_loss += GPLoss_forward\n",
        "      \n",
        "      CPLoss_forward = 0.1*self.CPLoss(out, train_batch[0])\n",
        "      total_loss += CPLoss_forward\n",
        "      \n",
        "\n",
        "      Contextual_Loss_forward = self.Contextual_Loss(out, train_batch[0])\n",
        "      total_loss += Contextual_Loss_forward\n",
        "      self.log('loss/contextual', Contextual_Loss_forward)\n",
        "      \"\"\"\n",
        "\n",
        "      #style_forward = 240*self.StyleLoss(out, train_batch[2])\n",
        "      #total_loss += style_forward\n",
        "      #self.log('loss/style', style_forward)\n",
        "\n",
        "      tv_forward = 0.0000005*self.TVLoss(out)\n",
        "      total_loss += tv_forward\n",
        "      self.log('loss/tv', tv_forward)\n",
        "\n",
        "      perceptual_forward = 2*self.PerceptualLoss(out, train_batch[2])\n",
        "      total_loss += perceptual_forward\n",
        "      self.log('loss/perceptual', perceptual_forward)\n",
        "\n",
        "      self.log('loss/g_loss', total_loss)\n",
        "\n",
        "      #return total_loss\n",
        "\n",
        "      # train discriminator\n",
        "      #if optimizer_idx == 1:\n",
        "\n",
        "      Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "      valid = Variable(Tensor(out.shape).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(out.shape).fill_(0.0), requires_grad=False)\n",
        "      dis_real_loss = self.MSELoss(train_batch[2], valid)\n",
        "      dis_fake_loss = self.MSELoss(out, fake)\n",
        "\n",
        "      d_loss = (dis_real_loss + dis_fake_loss) / 2\n",
        "      self.log('loss/d_loss', d_loss)\n",
        "\n",
        "      return total_loss+d_loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-3)\n",
        "      return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    out = self(train_batch[0],train_batch[1])\n",
        "    self.log('metrics/PSNR', self.psnr_metric(train_batch[2], out))\n",
        "    self.log('metrics/SSIM', self.ssim_metric(train_batch[2], out))\n",
        "    self.log('metrics/MSE', self.mse_metric(train_batch[2], out))\n",
        "    self.log('metrics/LPIPS', self.PerceptualLoss(out, train_batch[2]))\n",
        "\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    out = self(train_batch[0].unsqueeze(0)*train_batch[1].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "    save_image(out, str(train_idx)+'.png')\n",
        "\n",
        "  \"\"\"\n",
        "  def on_epoch_end(self):\n",
        "      sample_imgs = self.netG(image, masks)\n",
        "\n",
        "      # log sampled images\n",
        "      grid = torchvision.utils.make_grid(sample_imgs)\n",
        "      self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEn4yCogbFaW",
        "cellView": "form"
      },
      "source": [
        "#@title checkpoint.py\n",
        "#https://github.com/PyTorchLightning/pytorch-lightning/issues/2534\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    \"\"\"\n",
        "    Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n",
        "    based on validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_step_frequency,\n",
        "        prefix=\"Checkpoint\",\n",
        "        use_modelcheckpoint_filename=False,\n",
        "        save_path = '/content/'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "                use_modelcheckpoint_filename=False\n",
        "            use_modelcheckpoint_filename: just use the ModelCheckpoint callback's\n",
        "                default filename, don't use ours.\n",
        "        \"\"\"\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "        self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            if self.use_modelcheckpoint_filename:\n",
        "                filename = trainer.checkpoint_callback.filename\n",
        "            else:\n",
        "                filename = f\"{self.prefix}_{epoch}_{global_step}.ckpt\"\n",
        "            #ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            ckpt_path = os.path.join(self.save_path, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "\n",
        "#Trainer(callbacks=[CheckpointEveryNSteps()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYW1pF5RzQc"
      },
      "source": [
        "# Training\n",
        "dm = DFNetDataModule(training_path = '/content/data/images/', validation_path = '/content/data/images/', batch_size=5)\n",
        "model = CustomTrainClass()\n",
        "#weights_init(model, 'kaiming')\n",
        "# GPU\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xREJTZSbsEa8"
      },
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcHYqzCGleIX"
      },
      "source": [
        "# testing the model\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dm = DS_green_from_mask('/content/t/test', img_tf)\n",
        "model = CustomTrainClass()\n",
        "\n",
        "# GPU\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.test(model, dm, ckpt_path='/content/Checkpoint_1_1000.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
