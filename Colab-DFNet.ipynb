{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LX9rquFVL-y",
        "colab_type": "text"
      },
      "source": [
        "# DFNet\n",
        "Original repo: [hughplay/DFNet](https://github.com/hughplay/DFNet)\n",
        "\n",
        "Fork with training code: [Yukariin/DFNet](https://github.com/Yukariin/DFNet)\n",
        "\n",
        "Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "Warning: Black means inpainted area and white means original area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENPA1Aq4tmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWkqbDuLgBeY",
        "colab_type": "text"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWcV-Udp4zqP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title git clone original repo and download models\n",
        "!git clone https://github.com/hughplay/DFNet.git\n",
        "%cd DFNet/model\n",
        "!pip install gdown\n",
        "# places\n",
        "!gdown --id 1SGJ_Z9kpchdnZ3Qwwf4HnN-Cq-AeK7vH\n",
        "# celeba\n",
        "!gdown --id 1e6KVfSdILygDcyL-ps1jckS4Ff18Z3rj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsSglYdtDGeA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title test.py (you can edit the output resolution filesize there)\n",
        "%%writefile /content/DFNet/test.py\n",
        "from collections import defaultdict\n",
        "from itertools import islice\n",
        "from multiprocessing.pool import ThreadPool as Pool\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "from utils import list2nparray, gen_miss, merge_imgs\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class Tester:\n",
        "\n",
        "    def __init__(self, model_path, input_size, batch_size):\n",
        "        self.model_path = model_path\n",
        "        self._input_size = input_size\n",
        "        self.batch_size = batch_size\n",
        "        self.init_model(model_path)\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        if self._input_size > 0:\n",
        "            return (self._input_size, self._input_size)\n",
        "        elif 'celeba' in self.model_path:\n",
        "            return (1024, 1024) # edit these values for resolution, must be 2^x\n",
        "        else:\n",
        "            return (1024, 1024)\n",
        "\n",
        "    def init_model(self, path):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "            print('Using gpu.')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            print('Using cpu.')\n",
        "\n",
        "        self.model = DFNet().to(self.device)\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint)\n",
        "        self.model.eval()\n",
        "\n",
        "        print('Model %s loaded.' % path)\n",
        "\n",
        "    def get_name(self, path):\n",
        "        return '.'.join(path.name.split('.')[:-1])\n",
        "\n",
        "    def results_path(self, output, img_path, mask_path, prefix='result'):\n",
        "        img_name = self.get_name(img_path)\n",
        "        mask_name = self.get_name(mask_path)\n",
        "        return {\n",
        "            'result_path': self.sub_dir('result').joinpath(\n",
        "                'result-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'raw_path': self.sub_dir('raw').joinpath(\n",
        "                'raw-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'alpha_path': self.sub_dir('alpha').joinpath(\n",
        "                'alpha-{}-{}.png'.format(img_name, mask_name))\n",
        "        }\n",
        "\n",
        "    def inpaint_instance(self, img, mask):\n",
        "        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n",
        "        img = img.view(1, *img.shape)\n",
        "        mask = mask.view(1, 1, *mask.shape)\n",
        "        return self.inpaint_batch(img, mask).squeeze()\n",
        "\n",
        "    def inpaint_batch(self, imgs, masks):\n",
        "        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n",
        "        imgs = np.transpose(imgs, [0, 3, 1, 2])\n",
        "        masks = np.transpose(masks, [0, 3, 1, 2])\n",
        "\n",
        "        imgs = torch.from_numpy(imgs).to(self.device)\n",
        "        masks = torch.from_numpy(masks).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "        results = self.model(imgs_miss, masks)\n",
        "        if type(results) is list:\n",
        "            results = results[0]\n",
        "        results = results.mul(255).byte().data.cpu().numpy()\n",
        "        results = np.transpose(results, [0, 2, 3, 1])\n",
        "        return results\n",
        "\n",
        "    def _process_file(self, output, img_path, mask_path):\n",
        "        item = {\n",
        "            'img_path': img_path,\n",
        "            'mask_path': mask_path,\n",
        "        }\n",
        "        item.update(self.results_path(output, img_path, mask_path))\n",
        "        self.path_pair.append(item)\n",
        "\n",
        "    def process_single_file(self, output, img_path, mask_path):\n",
        "        self.path_pair = []\n",
        "        self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def process_dir(self, output, img_dir, mask_dir):\n",
        "        img_dir = Path(img_dir)\n",
        "        mask_dir = Path(mask_dir)\n",
        "        imgs_path = sorted(\n",
        "            list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))\n",
        "        masks_path = sorted(\n",
        "            list(mask_dir.glob('*.jpg')) + list(mask_dir.glob('*.png')))\n",
        "\n",
        "        n_img = len(imgs_path)\n",
        "        n_mask = len(masks_path)\n",
        "        n_pair = min(n_img, n_mask)\n",
        "\n",
        "        self.path_pair = []\n",
        "        for i in range(n_pair):\n",
        "            img_path = imgs_path[i % n_img]\n",
        "            mask_path = masks_path[i % n_mask]\n",
        "            self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def get_process(self, input_size):\n",
        "        def process(pair):\n",
        "            img = cv2.imread(str(pair['img_path']), cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(str(pair['mask_path']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if input_size:\n",
        "                img = cv2.resize(img, input_size)\n",
        "                mask = cv2.resize(mask, input_size)\n",
        "\n",
        "            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n",
        "            mask = np.ascontiguousarray(\n",
        "                np.expand_dims(mask, 0)).astype(np.uint8)\n",
        "\n",
        "            pair['img'] = img\n",
        "            pair['mask'] = mask\n",
        "            return pair\n",
        "        return process\n",
        "\n",
        "    def _file_batch(self):\n",
        "        pool = Pool()\n",
        "\n",
        "        n_pair = len(self.path_pair)\n",
        "        n_batch = (n_pair-1) // self.batch_size + 1\n",
        "\n",
        "        for i in tqdm.trange(n_batch, leave=False):\n",
        "            _buffer = defaultdict(list)\n",
        "            start = i * self.batch_size\n",
        "            stop = start + self.batch_size\n",
        "            process = self.get_process(self.input_size)\n",
        "            batch = pool.imap_unordered(\n",
        "                process, islice(self.path_pair, start, stop))\n",
        "            for instance in batch:\n",
        "                for k, v in instance.items():\n",
        "                    _buffer[k].append(v)\n",
        "            yield _buffer\n",
        "\n",
        "    def batch_generator(self):\n",
        "        generator = self._file_batch\n",
        "\n",
        "        for _buffer in generator():\n",
        "            for key in _buffer:\n",
        "                if key in ['img', 'mask']:\n",
        "                    _buffer[key] = list2nparray(_buffer[key])\n",
        "            yield _buffer\n",
        "\n",
        "    def to_numpy(self, tensor):\n",
        "        tensor = tensor.mul(255).byte().data.cpu().numpy()\n",
        "        tensor = np.transpose(tensor, [0, 2, 3, 1])\n",
        "        return tensor\n",
        "\n",
        "    def process_batch(self, batch, output):\n",
        "        imgs = torch.from_numpy(batch['img']).to(self.device)\n",
        "        masks = torch.from_numpy(batch['mask']).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "\n",
        "        result, alpha, raw = self.model(imgs_miss, masks)\n",
        "        result, alpha, raw = result[0], alpha[0], raw[0]\n",
        "        result = imgs * masks + result * (1 - masks)\n",
        "\n",
        "        result = self.to_numpy(result)\n",
        "        alpha = self.to_numpy(alpha)\n",
        "        raw = self.to_numpy(raw)\n",
        "\n",
        "        for i in range(result.shape[0]):\n",
        "            cv2.imwrite(str(batch['result_path'][i]), result[i])\n",
        "            cv2.imwrite(str(batch['raw_path'][i]), raw[i])\n",
        "            cv2.imwrite(str(batch['alpha_path'][i]), alpha[i])\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        return Path(self.output)\n",
        "\n",
        "    def sub_dir(self, sub):\n",
        "        return self.root.joinpath(sub)\n",
        "\n",
        "    def prepare_folders(self, folders):\n",
        "        for folder in folders:\n",
        "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def inpaint(self, output, img, mask, merge_result=False):\n",
        "\n",
        "        self.output = output\n",
        "        self.prepare_folders([\n",
        "            self.sub_dir('result'), self.sub_dir('alpha'),\n",
        "            self.sub_dir('raw')])\n",
        "\n",
        "        if os.path.isfile(img) and os.path.isfile(mask):\n",
        "            if img.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                self.process_single_file(output, img, mask)\n",
        "                _type = 'file'\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "        elif os.path.isdir(img) and os.path.isdir(mask):\n",
        "            self.process_dir(output, img, mask)\n",
        "            _type = 'dir'\n",
        "        else:\n",
        "            print('Img: ', img)\n",
        "            print('Mask: ', mask)\n",
        "            raise NotImplementedError(\n",
        "                'img and mask should be both file or directory.')\n",
        "\n",
        "        print('# Inpainting...')\n",
        "        print('Input size:', self.input_size)\n",
        "        for batch in self.batch_generator():\n",
        "            self.process_batch(batch, output)\n",
        "        print('Inpainting finished.')\n",
        "\n",
        "        if merge_result and _type == 'dir':\n",
        "            miss = self.sub_dir('miss')\n",
        "            merge = self.sub_dir('merge')\n",
        "\n",
        "            print('# Preparing input images...')\n",
        "            gen_miss(img, mask, miss)\n",
        "            print('# Merging...')\n",
        "            merge_imgs([\n",
        "                miss, self.sub_dir('raw'), self.sub_dir('alpha'),\n",
        "                self.sub_dir('result'), img], merge, res=self.input_size[0])\n",
        "            print('Merging finished.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '-m', '--model', default='./model/model_places2.pth',\n",
        "        help='Select a checkpoint.')\n",
        "    parser.add_argument(\n",
        "        '-i', '--input_size', default=0, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '-b', '--batch_size', default=8, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '--img', default='./samples/places2/img',\n",
        "        help='Image or Image folder.')\n",
        "    parser.add_argument(\n",
        "        '--mask', default='./samples/places2/mask',\n",
        "        help='Mask or Mask folder.')\n",
        "    parser.add_argument('--output', default='./output/places2',\n",
        "        help='Output dir')\n",
        "    parser.add_argument(\n",
        "        '--merge', action='store_true',\n",
        "        help='Whether merge input and results for better viewing.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    tester = Tester(args.model, args.input_size, args.batch_size)\n",
        "\n",
        "    tester.inpaint(args.output, args.img, args.mask, merge_result=args.merge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsY6cznc5e-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/DFNet\n",
        "!python test.py --model /content/DFNet/model/model_places2.pth --img samples/places2/img --mask samples/places2/mask --output output/places2 --merge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-7zQWEOTmjn",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzSKGiaLTlxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using a fork, since the original repo does not provide training code\n",
        "!git clone https://github.com/Yukariin/DFNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvBWyHnTUNcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxsl2_ohXk-z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title differentiable augmentation (not sure if it is applied correctly)\n",
        "%%writefile /content/DFNet/train.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data import DS\n",
        "from loss import InpaintingLoss\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class InfiniteSampler(data.sampler.Sampler):\n",
        "    def __init__(self, num_samples):\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.loop())\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "    def loop(self):\n",
        "        i = 0\n",
        "        order = np.random.permutation(self.num_samples)\n",
        "        while True:\n",
        "            yield order[i]\n",
        "            i += 1\n",
        "            if i >= self.num_samples:\n",
        "                np.random.seed()\n",
        "                order = np.random.permutation(self.num_samples)\n",
        "                i = 0\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, default='/srv/datasets/Places2')\n",
        "parser.add_argument('--save_dir', type=str, default='./snapshots/default')\n",
        "parser.add_argument('--log_dir', type=str, default='./logs/default')\n",
        "parser.add_argument('--lr', type=float, default=2e-3)\n",
        "parser.add_argument('--max_iter', type=int, default=200000)\n",
        "parser.add_argument('--batch_size', type=int, default=6)\n",
        "parser.add_argument('--n_threads', type=int, default=16)\n",
        "parser.add_argument('--save_model_interval', type=int, default=1000)\n",
        "parser.add_argument('--vis_interval', type=int, default=1000)\n",
        "parser.add_argument('--log_interval', type=int, default=10)\n",
        "parser.add_argument('--image_size', type=int, default=256)\n",
        "parser.add_argument('--resume', type=str)\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs('{:s}/images'.format(args.save_dir))\n",
        "    os.makedirs('{:s}/ckpt'.format(args.save_dir))\n",
        "\n",
        "writer = SummaryWriter(logdir=args.log_dir)\n",
        "\n",
        "size = (args.image_size, args.image_size)\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize(size=size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = DS(args.root, img_tf)\n",
        "\n",
        "iterator_train = iter(data.DataLoader(\n",
        "    dataset, batch_size=args.batch_size,\n",
        "    sampler=InfiniteSampler(len(dataset)),\n",
        "    num_workers=args.n_threads\n",
        "))\n",
        "print(len(dataset))\n",
        "model = DFNet().to(device)\n",
        "\n",
        "lr = args.lr\n",
        "\n",
        "start_iter = 0\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "criterion = InpaintingLoss().to(device)\n",
        "\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "for i in tqdm(range(start_iter, args.max_iter)):\n",
        "    model.train()\n",
        "\n",
        "    img, mask = [x.to(device) for x in next(iterator_train)]\n",
        "    masked = img * mask\n",
        "\n",
        "    results, alpha, raw = model(masked, mask)\n",
        "\n",
        "    loss = criterion(results, img)\n",
        "\n",
        "    # Diffaugment\n",
        "    img = DiffAugment(img, policy=policy))\n",
        "    results = DiffAugment(results, policy=policy))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % args.log_interval == 0:\n",
        "        writer.add_scalar('loss', loss.item(), i + 1)\n",
        "\n",
        "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n",
        "        torch.save(model.state_dict(), '{:s}/ckpt/{:d}.pth'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % args.vis_interval == 0:\n",
        "        s_img = torch.cat([img, masked, results[0]])\n",
        "        s_img = make_grid(s_img, nrow=args.batch_size)\n",
        "        save_image(s_img, '{:s}/images/test_{:d}.png'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % 10000:\n",
        "        scheduler.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQQrK_JTyMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/DFNet\n",
        "!python train.py --root /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YBkhsLcW_6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/content/DFNet/logs/default"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}