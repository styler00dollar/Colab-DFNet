{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_DFNet_P_v2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LX9rquFVL-y"
      },
      "source": [
        "# DFNet\n",
        "Original repo: [hughplay/DFNet](https://github.com/hughplay/DFNet)\n",
        "\n",
        "Fork with training code: [Yukariin/DFNet](https://github.com/Yukariin/DFNet)\n",
        "\n",
        "~~Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)~~ Currently no GAN loss.\n",
        "\n",
        "Warning: Black means inpainted area and white means original area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENPA1Aq4tmT"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-7zQWEOTmjn"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtvgGPeyx-mb",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzSKGiaLTlxC",
        "cellView": "form"
      },
      "source": [
        "#@title install\n",
        "#!git clone https://github.com/Yukariin/DFNet\n",
        "!git clone https://github.com/styler00dollar/Colab-DFNet\n",
        "!pip install tensorboardX\n",
        "#!pip install LPIPS\n",
        "\n",
        "%cd /content/Colab-DFNet\n",
        "!pip install gdown\n",
        "# places\n",
        "!gdown --id 1SGJ_Z9kpchdnZ3Qwwf4HnN-Cq-AeK7vH\n",
        "!mkdir /content/train_data\n",
        "\n",
        "# You must create /images and /ckpt in your path, or training will crash\n",
        "!mkdir '/content/drive/MyDrive/Colab-DFNet'\n",
        "!mkdir '/content/drive/MyDrive/Colab-DFNet/images/'\n",
        "!mkdir '/content/drive/MyDrive/Colab-DFNet/ckpt/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXURbm6F0Q7m"
      },
      "source": [
        "[Experimental] Additional losses: HFENLoss (high frequency error norm), ElasticLoss, RelativeL1, L1CosineSim, ClipL1, FFTloss, OFLoss (Overflow loss), GPLoss (Gradient Profile (GP) loss), CPLoss (Color Profile (CP) loss), Contextual_Loss and LPIPS. Config weight value and combination in ```loss.py```.\n",
        "\n",
        "Warning: If AMP is used together with Style loss, then it will result in Nan errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro7UoojWN-TY",
        "cellView": "form"
      },
      "source": [
        "#@title loss.py\n",
        "%%writefile /content/Colab-DFNet/loss.py\n",
        "\n",
        "resume_iteration = 0\n",
        "\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from vic.filters import *\n",
        "from vic.colors import *\n",
        "from vic.discriminators import *\n",
        "\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "logdir='/content/drive/MyDrive/Colab-DFNet'\n",
        "\n",
        "writer = SummaryWriter(logdir=logdir)\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "from utils import resize_like\n",
        "from metrics import *\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "class InpaintingLoss(nn.Module):\n",
        "    def __init__(self, p=[0, 1, 2,3,4,5], q=[0, 1, 2,3,4,5],\n",
        "                 w=[6., 0.1, 240., 0.1]):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.l1 = nn.L1Loss()\n",
        "        #self.perceptual = PerceptualLoss()\n",
        "        #self.style = StyleLoss()\n",
        "\n",
        "        # new loss\n",
        "        \"\"\"\n",
        "        if self.config.HFEN_TYPE == 'L1':\n",
        "          l_hfen_type = nn.L1Loss()\n",
        "        if self.config.HFEN_TYPE == 'MSE':\n",
        "          l_hfen_type = nn.MSELoss()\n",
        "        if self.config.HFEN_TYPE == 'Charbonnier':\n",
        "          l_hfen_type = CharbonnierLoss()\n",
        "        if self.config.HFEN_TYPE == 'ElasticLoss':\n",
        "          l_hfen_type = ElasticLoss()\n",
        "        if self.config.HFEN_TYPE == 'RelativeL1':\n",
        "          l_hfen_type = RelativeL1()\n",
        "        if self.config.HFEN_TYPE == 'L1CosineSim':\n",
        "          l_hfen_type = L1CosineSim()\n",
        "        \"\"\"\n",
        "\n",
        "        l_hfen_type = L1CosineSim()\n",
        "        self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "\n",
        "        self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "\n",
        "        self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "\n",
        "        self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "\n",
        "        self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "\n",
        "        self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "\n",
        "        self.OFLoss = OFLoss()\n",
        "\n",
        "        self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "\n",
        "        self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "\n",
        "        self.StyleLoss = StyleLoss()\n",
        "\n",
        "        self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "\n",
        "        self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "        self.psnr_metric = PSNR()\n",
        "        self.ssim_metric = SSIM()\n",
        "        self.ae_metric = AE()\n",
        "        self.mse_metric = MSE()\n",
        "\n",
        "\n",
        "    def forward(self, input, gt, iteration):\n",
        "\n",
        "        # just one loop\n",
        "        total_loss = 0.0\n",
        "\n",
        "        L1CosineSim_forward = 0.0\n",
        "        perceptual_forward = 0.0\n",
        "        style_forward = 0.0\n",
        "        tv_forward = 0.0\n",
        "        PSNR_value = 0.0\n",
        "\n",
        "\n",
        "        # Input batchsize here\n",
        "        for i in range(6):\n",
        "          out = input[0][i]\n",
        "          gt_res = gt[i]\n",
        "\n",
        "          #gt_res = resize_like(gt, out)\n",
        "\n",
        "          out = out.unsqueeze(0)\n",
        "          gt_res = gt_res.unsqueeze(0)\n",
        "\n",
        "          # new loss\n",
        "          \"\"\"\n",
        "          HFENLoss_forward = self.HFENLoss(out, gt_res)\n",
        "          total_loss += HFENLoss_forward\n",
        "\n",
        "          ElasticLoss_forward = self.ElasticLoss(out, gt_res)\n",
        "          total_loss += ElasticLoss_forward\n",
        "\n",
        "          RelativeL1_forward = self.RelativeL1(out, gt_res)\n",
        "          total_loss += RelativeL1_forward\n",
        "          \"\"\"\n",
        "          L1CosineSim_forward += 6*self.L1CosineSim(out, gt_res)\n",
        "          #total_loss += L1CosineSim_forward\n",
        "\n",
        "          #writer.add_scalar('loss/L1CosineSim', L1CosineSim_forward, iteration)\n",
        "          #total_loss += L1CosineSim_forward\n",
        "          \"\"\"\n",
        "          ClipL1_forward = self.ClipL1(out, gt_res)\n",
        "          total_loss += ClipL1_forward\n",
        "\n",
        "          FFTloss_forward = self.FFTloss(out, gt_res)\n",
        "          total_loss += FFTloss_forward\n",
        "\n",
        "          OFLoss_forward = self.OFLoss(out)\n",
        "          total_loss += OFLoss_forward\n",
        "\n",
        "          GPLoss_forward = self.GPLoss(out, gt_res)\n",
        "          total_loss += GPLoss_forward\n",
        "\n",
        "          CPLoss_forward = 0.1*self.CPLoss(out, gt_res)\n",
        "          total_loss += CPLoss_forward\n",
        "\n",
        "          Contextual_Loss_forward = self.Contextual_Loss(out, gt_res)\n",
        "          total_loss += Contextual_Loss_forward\n",
        "          \"\"\"\n",
        "\n",
        "          style_forward += 240*self.StyleLoss(out, gt_res)\n",
        "          #total_loss += style_forward\n",
        "\n",
        "          tv_forward += 0.1*self.TVLoss(out)\n",
        "          #total_loss += tv_forward\n",
        "\n",
        "          perceptual_forward += 0.1*self.PerceptualLoss(out, gt_res)\n",
        "          #total_loss += perceptual_forward\n",
        "\n",
        "          PSNR_value += self.psnr_metric(gt_res, out)\n",
        "\n",
        "\n",
        "        writer.add_scalar('loss/Perceptual', perceptual_forward, iteration)\n",
        "        writer.add_scalar('loss/Style', style_forward, iteration)\n",
        "        writer.add_scalar('loss/TV', tv_forward, iteration)\n",
        "        writer.add_scalar('loss/L1CosineSim', L1CosineSim_forward, iteration)\n",
        "\n",
        "        total_loss = perceptual_forward + style_forward + tv_forward + L1CosineSim_forward\n",
        "\n",
        "        #total_loss += loss_rec + loss_PerceptualLoss + loss_style\n",
        "        #loss_text += (self.w[1] * loss_prc) + (self.w[2] * loss_style) + (self.w[3] * loss_tv)\n",
        "\n",
        "\n",
        "        writer.add_scalar('Total', total_loss, iteration)\n",
        "\n",
        "\n",
        "        # PSNR (Peak Signal-to-Noise Ratio)\n",
        "        #writer.add_scalar('metrics/PSNR', self.psnr_metric(gt_res, out), iteration)\n",
        "        writer.add_scalar('metrics/PSNR', PSNR_value, iteration+resume_iteration)\n",
        "\n",
        "        # SSIM (Structural Similarity)\n",
        "        writer.add_scalar('metrics/SSIM', self.ssim_metric(gt_res, out), iteration)\n",
        "\n",
        "        # AE (Average Angular Error)\n",
        "        writer.add_scalar('metrics/AE', self.ae_metric(gt_res, out), iteration)\n",
        "\n",
        "        # MSE (Mean Square Error)\n",
        "        writer.add_scalar('metrics/MSE', self.mse_metric(gt_res, out), iteration)\n",
        "\n",
        "        # LPIPS (Learned Perceptual Image Patch Similarity)\n",
        "        # pip install LPIPS\n",
        "        #writer.add_scalar('metrics/SSIM', lpips_metric(gt_res, out), iteration)\n",
        "\n",
        "        return total_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DoEMGs9OT7V",
        "cellView": "form"
      },
      "source": [
        "#@title train.py\n",
        "%%writefile /content/Colab-DFNet/train.py\n",
        "#from diffaug import *\n",
        "\n",
        "resume_iteration = 0\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data import DS\n",
        "from loss import InpaintingLoss\n",
        "from model import DFNet\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class InfiniteSampler(data.sampler.Sampler):\n",
        "    def __init__(self, num_samples):\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.loop())\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "    def loop(self):\n",
        "        i = 0\n",
        "        order = np.random.permutation(self.num_samples)\n",
        "        while True:\n",
        "            yield order[i]\n",
        "            i += 1\n",
        "            if i >= self.num_samples:\n",
        "                np.random.seed()\n",
        "                order = np.random.permutation(self.num_samples)\n",
        "                i = 0\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, default='/path/')\n",
        "parser.add_argument('--save_dir', type=str, default='/path/')\n",
        "#parser.add_argument('--log_dir', type=str, default='./logs/default')\n",
        "parser.add_argument('--lr', type=float, default=2e-3)\n",
        "parser.add_argument('--max_iter', type=int, default=5000000)\n",
        "parser.add_argument('--batch_size', type=int, default=6)\n",
        "parser.add_argument('--n_threads', type=int, default=16)\n",
        "parser.add_argument('--save_model_interval', type=int, default=500)\n",
        "parser.add_argument('--vis_interval', type=int, default=500)\n",
        "parser.add_argument('--log_interval', type=int, default=1)\n",
        "parser.add_argument('--image_size', type=int, default=256)\n",
        "parser.add_argument('--resume', type=str)\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs('{:s}/images'.format(args.save_dir))\n",
        "    os.makedirs('{:s}/ckpt'.format(args.save_dir))\n",
        "\n",
        "#writer = SummaryWriter(logdir=args.log_dir)\n",
        "\n",
        "size = (args.image_size, args.image_size)\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize(size=size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = DS(args.root, img_tf)\n",
        "\n",
        "iterator_train = iter(data.DataLoader(\n",
        "    dataset, batch_size=args.batch_size,\n",
        "    sampler=InfiniteSampler(len(dataset)),\n",
        "    num_workers=args.n_threads\n",
        "))\n",
        "print(len(dataset))\n",
        "model = DFNet().to(device)\n",
        "\n",
        "lr = args.lr\n",
        "\n",
        "start_iter = 0\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "criterion = InpaintingLoss().to(device)\n",
        "\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "for i in tqdm(range(start_iter, args.max_iter)):\n",
        "    model.train()\n",
        "\n",
        "    img, mask = [x.to(device) for x in next(iterator_train)]\n",
        "\n",
        "    # inpainting\n",
        "    masked = img * mask\n",
        "\n",
        "\n",
        "    # mosaic\n",
        "    \"\"\"\n",
        "    MOSAIC_MIN = 0.01\n",
        "    MOSAIC_MID =  0.2\n",
        "    MOSAIC_MAX = 0.0625\n",
        "\n",
        "    mosaic_size = int(random.triangular(int(min(256*MOSAIC_MIN, 256*MOSAIC_MIN)), int(min(256*MOSAIC_MID, 256*MOSAIC_MID)), int(min(256*MOSAIC_MAX, 256*MOSAIC_MAX))))\n",
        "    images_mosaic = nnf.interpolate(img, size=(mosaic_size, mosaic_size), mode='nearest')\n",
        "    images_mosaic = nnf.interpolate(images_mosaic, size=(256, 256), mode='nearest')\n",
        "    #masked = (img * (1 - mask).float()) + (images_mosaic * (mask).float())\n",
        "    masked = (images_mosaic * (1 - mask).float()) + (img * (mask).float())\n",
        "    \"\"\"\n",
        "\n",
        "    results, alpha, raw = model(masked, mask)\n",
        "\n",
        "    #with torch.cuda.amp.autocast():\n",
        "    loss = criterion(results, img, i)\n",
        "\n",
        "\n",
        "    # no amp\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # amp\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    optimizer.zero_grad()\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    if (i + 1) % args.log_interval == 0:\n",
        "        writer.add_scalar('loss', loss.item(), i + 1)\n",
        "    \"\"\"\n",
        "\n",
        "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n",
        "        torch.save(model.state_dict(), '{:s}/ckpt/{:d}.pth'.format(args.save_dir, i + 1 + resume_iteration))\n",
        "\n",
        "    if (i + 1) % args.vis_interval == 0:\n",
        "        s_img = torch.cat([img, masked, results[0]])\n",
        "        s_img = make_grid(s_img, nrow=args.batch_size)\n",
        "        save_image(s_img, '{:s}/images/test_{:d}.png'.format(args.save_dir, i + 1 + resume_iteration))\n",
        "\n",
        "    if (i + 1) % 10000:\n",
        "        scheduler.step()\n",
        "\n",
        "    # amp\n",
        "    #scaler.update()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQQrK_JTyMc"
      },
      "source": [
        "%cd /content/Colab-DFNet\n",
        "!python train.py --root /content/train_data --resume '/content/Colab-DFNet/model_places2.pth' --save_dir \"/content/drive/MyDrive/Colab-DFNet/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQuYRKOc7Nx"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U_GDGa8Z3AA",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWcV-Udp4zqP",
        "cellView": "form"
      },
      "source": [
        "#@title git clone original repo and download models\n",
        "!git clone https://github.com/hughplay/DFNet.git\n",
        "%cd DFNet/model\n",
        "!pip install gdown\n",
        "# places\n",
        "!gdown --id 1SGJ_Z9kpchdnZ3Qwwf4HnN-Cq-AeK7vH\n",
        "# celeba\n",
        "!gdown --id 1e6KVfSdILygDcyL-ps1jckS4Ff18Z3rj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsSglYdtDGeA",
        "cellView": "form"
      },
      "source": [
        "#@title test.py (you can edit the output resolution filesize there)\n",
        "%%writefile /content/DFNet/test.py\n",
        "from collections import defaultdict\n",
        "from itertools import islice\n",
        "from multiprocessing.pool import ThreadPool as Pool\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "from utils import list2nparray, gen_miss, merge_imgs\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class Tester:\n",
        "\n",
        "    def __init__(self, model_path, input_size, batch_size):\n",
        "        self.model_path = model_path\n",
        "        self._input_size = input_size\n",
        "        self.batch_size = batch_size\n",
        "        self.init_model(model_path)\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        if self._input_size > 0:\n",
        "            return (self._input_size, self._input_size)\n",
        "        elif 'celeba' in self.model_path:\n",
        "            return (1024, 1024) # edit these values for resolution, must be 2^x\n",
        "        else:\n",
        "            return (1024, 1024)\n",
        "\n",
        "    def init_model(self, path):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "            print('Using gpu.')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            print('Using cpu.')\n",
        "\n",
        "        self.model = DFNet().to(self.device)\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint)\n",
        "        self.model.eval()\n",
        "\n",
        "        print('Model %s loaded.' % path)\n",
        "\n",
        "    def get_name(self, path):\n",
        "        return '.'.join(path.name.split('.')[:-1])\n",
        "\n",
        "    def results_path(self, output, img_path, mask_path, prefix='result'):\n",
        "        img_name = self.get_name(img_path)\n",
        "        mask_name = self.get_name(mask_path)\n",
        "        return {\n",
        "            'result_path': self.sub_dir('result').joinpath(\n",
        "                'result-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'raw_path': self.sub_dir('raw').joinpath(\n",
        "                'raw-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'alpha_path': self.sub_dir('alpha').joinpath(\n",
        "                'alpha-{}-{}.png'.format(img_name, mask_name))\n",
        "        }\n",
        "\n",
        "    def inpaint_instance(self, img, mask):\n",
        "        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n",
        "        img = img.view(1, *img.shape)\n",
        "        mask = mask.view(1, 1, *mask.shape)\n",
        "        return self.inpaint_batch(img, mask).squeeze()\n",
        "\n",
        "    def inpaint_batch(self, imgs, masks):\n",
        "        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n",
        "        imgs = np.transpose(imgs, [0, 3, 1, 2])\n",
        "        masks = np.transpose(masks, [0, 3, 1, 2])\n",
        "\n",
        "        imgs = torch.from_numpy(imgs).to(self.device)\n",
        "        masks = torch.from_numpy(masks).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "        results = self.model(imgs_miss, masks)\n",
        "        if type(results) is list:\n",
        "            results = results[0]\n",
        "        results = results.mul(255).byte().data.cpu().numpy()\n",
        "        results = np.transpose(results, [0, 2, 3, 1])\n",
        "        return results\n",
        "\n",
        "    def _process_file(self, output, img_path, mask_path):\n",
        "        item = {\n",
        "            'img_path': img_path,\n",
        "            'mask_path': mask_path,\n",
        "        }\n",
        "        item.update(self.results_path(output, img_path, mask_path))\n",
        "        self.path_pair.append(item)\n",
        "\n",
        "    def process_single_file(self, output, img_path, mask_path):\n",
        "        self.path_pair = []\n",
        "        self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def process_dir(self, output, img_dir, mask_dir):\n",
        "        img_dir = Path(img_dir)\n",
        "        mask_dir = Path(mask_dir)\n",
        "        imgs_path = sorted(\n",
        "            list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))\n",
        "        masks_path = sorted(\n",
        "            list(mask_dir.glob('*.jpg')) + list(mask_dir.glob('*.png')))\n",
        "\n",
        "        n_img = len(imgs_path)\n",
        "        n_mask = len(masks_path)\n",
        "        n_pair = min(n_img, n_mask)\n",
        "\n",
        "        self.path_pair = []\n",
        "        for i in range(n_pair):\n",
        "            img_path = imgs_path[i % n_img]\n",
        "            mask_path = masks_path[i % n_mask]\n",
        "            self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def get_process(self, input_size):\n",
        "        def process(pair):\n",
        "            img = cv2.imread(str(pair['img_path']), cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(str(pair['mask_path']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if input_size:\n",
        "                img = cv2.resize(img, input_size)\n",
        "                mask = cv2.resize(mask, input_size)\n",
        "\n",
        "            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n",
        "            mask = np.ascontiguousarray(\n",
        "                np.expand_dims(mask, 0)).astype(np.uint8)\n",
        "\n",
        "            pair['img'] = img\n",
        "            pair['mask'] = mask\n",
        "            return pair\n",
        "        return process\n",
        "\n",
        "    def _file_batch(self):\n",
        "        pool = Pool()\n",
        "\n",
        "        n_pair = len(self.path_pair)\n",
        "        n_batch = (n_pair-1) // self.batch_size + 1\n",
        "\n",
        "        for i in tqdm.trange(n_batch, leave=False):\n",
        "            _buffer = defaultdict(list)\n",
        "            start = i * self.batch_size\n",
        "            stop = start + self.batch_size\n",
        "            process = self.get_process(self.input_size)\n",
        "            batch = pool.imap_unordered(\n",
        "                process, islice(self.path_pair, start, stop))\n",
        "            for instance in batch:\n",
        "                for k, v in instance.items():\n",
        "                    _buffer[k].append(v)\n",
        "            yield _buffer\n",
        "\n",
        "    def batch_generator(self):\n",
        "        generator = self._file_batch\n",
        "\n",
        "        for _buffer in generator():\n",
        "            for key in _buffer:\n",
        "                if key in ['img', 'mask']:\n",
        "                    _buffer[key] = list2nparray(_buffer[key])\n",
        "            yield _buffer\n",
        "\n",
        "    def to_numpy(self, tensor):\n",
        "        tensor = tensor.mul(255).byte().data.cpu().numpy()\n",
        "        tensor = np.transpose(tensor, [0, 2, 3, 1])\n",
        "        return tensor\n",
        "\n",
        "    def process_batch(self, batch, output):\n",
        "        imgs = torch.from_numpy(batch['img']).to(self.device)\n",
        "        masks = torch.from_numpy(batch['mask']).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "\n",
        "        result, alpha, raw = self.model(imgs_miss, masks)\n",
        "        result, alpha, raw = result[0], alpha[0], raw[0]\n",
        "        result = imgs * masks + result * (1 - masks)\n",
        "\n",
        "        result = self.to_numpy(result)\n",
        "        alpha = self.to_numpy(alpha)\n",
        "        raw = self.to_numpy(raw)\n",
        "\n",
        "        for i in range(result.shape[0]):\n",
        "            cv2.imwrite(str(batch['result_path'][i]), result[i])\n",
        "            cv2.imwrite(str(batch['raw_path'][i]), raw[i])\n",
        "            cv2.imwrite(str(batch['alpha_path'][i]), alpha[i])\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        return Path(self.output)\n",
        "\n",
        "    def sub_dir(self, sub):\n",
        "        return self.root.joinpath(sub)\n",
        "\n",
        "    def prepare_folders(self, folders):\n",
        "        for folder in folders:\n",
        "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def inpaint(self, output, img, mask, merge_result=False):\n",
        "\n",
        "        self.output = output\n",
        "        self.prepare_folders([\n",
        "            self.sub_dir('result'), self.sub_dir('alpha'),\n",
        "            self.sub_dir('raw')])\n",
        "\n",
        "        if os.path.isfile(img) and os.path.isfile(mask):\n",
        "            if img.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                self.process_single_file(output, img, mask)\n",
        "                _type = 'file'\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "        elif os.path.isdir(img) and os.path.isdir(mask):\n",
        "            self.process_dir(output, img, mask)\n",
        "            _type = 'dir'\n",
        "        else:\n",
        "            print('Img: ', img)\n",
        "            print('Mask: ', mask)\n",
        "            raise NotImplementedError(\n",
        "                'img and mask should be both file or directory.')\n",
        "\n",
        "        print('# Inpainting...')\n",
        "        print('Input size:', self.input_size)\n",
        "        for batch in self.batch_generator():\n",
        "            self.process_batch(batch, output)\n",
        "        print('Inpainting finished.')\n",
        "\n",
        "        if merge_result and _type == 'dir':\n",
        "            miss = self.sub_dir('miss')\n",
        "            merge = self.sub_dir('merge')\n",
        "\n",
        "            print('# Preparing input images...')\n",
        "            gen_miss(img, mask, miss)\n",
        "            print('# Merging...')\n",
        "            merge_imgs([\n",
        "                miss, self.sub_dir('raw'), self.sub_dir('alpha'),\n",
        "                self.sub_dir('result'), img], merge, res=self.input_size[0])\n",
        "            print('Merging finished.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '-m', '--model', default='./model/model_places2.pth',\n",
        "        help='Select a checkpoint.')\n",
        "    parser.add_argument(\n",
        "        '-i', '--input_size', default=0, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '-b', '--batch_size', default=8, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '--img', default='./samples/places2/img',\n",
        "        help='Image or Image folder.')\n",
        "    parser.add_argument(\n",
        "        '--mask', default='./samples/places2/mask',\n",
        "        help='Mask or Mask folder.')\n",
        "    parser.add_argument('--output', default='./output/places2',\n",
        "        help='Output dir')\n",
        "    parser.add_argument(\n",
        "        '--merge', action='store_true',\n",
        "        help='Whether merge input and results for better viewing.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    tester = Tester(args.model, args.input_size, args.batch_size)\n",
        "\n",
        "    tester.inpaint(args.output, args.img, args.mask, merge_result=args.merge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWHX6hN5jKB"
      },
      "source": [
        "# extract archive with data\n",
        "%cd /content/\n",
        "!7z x /content/archive.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQu7AZfK7_x1"
      },
      "source": [
        "# install imagemagick\n",
        "!sudo apt-get install imagemagick imagemagick-doc "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrdE0U9S8C70"
      },
      "source": [
        "# negate masks, if needed\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "mask_dir = '/content/masks'\n",
        "files = glob.glob(mask_dir + '/**/*.png', recursive=True)\n",
        "\n",
        "for f in tqdm(files):\n",
        "  os.system('convert {} -negate {}'.format(f, f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsDNGY0v9QVi"
      },
      "source": [
        "# delete output\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/output\n",
        "!mkdir /content/output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjO2k9HD5k0r"
      },
      "source": [
        "# create archive with results \n",
        "!tar -czvf /content/archive.tar.gz /content/output/result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56eiUuTI5Lul"
      },
      "source": [
        "# test\n",
        "%cd /content/DFNet\n",
        "!python test.py --model /content/DFNet/model/model_places2.pth --img /content/input --mask /content/masks --output /content/output --merge -b 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMt6k98EAVSi"
      },
      "source": [
        "# delete everything\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/output\n",
        "!sudo rm -rf /content/input\n",
        "!sudo rm -rf /content/masks\n",
        "!mkdir /content/output\n",
        "!mkdir /content/input\n",
        "!mkdir /content/masks"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
