{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "FWkqbDuLgBeY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LX9rquFVL-y"
      },
      "source": [
        "# DFNet\n",
        "Original repo: [hughplay/DFNet](https://github.com/hughplay/DFNet)\n",
        "\n",
        "Fork with training code: [Yukariin/DFNet](https://github.com/Yukariin/DFNet)\n",
        "\n",
        "Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "Warning: Black means inpainted area and white means original area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENPA1Aq4tmT"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWkqbDuLgBeY"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWcV-Udp4zqP",
        "cellView": "form"
      },
      "source": [
        "#@title git clone original repo and download models\n",
        "!git clone https://github.com/hughplay/DFNet.git\n",
        "%cd DFNet/model\n",
        "!pip install gdown\n",
        "# places\n",
        "!gdown --id 1SGJ_Z9kpchdnZ3Qwwf4HnN-Cq-AeK7vH\n",
        "# celeba\n",
        "!gdown --id 1e6KVfSdILygDcyL-ps1jckS4Ff18Z3rj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsSglYdtDGeA",
        "cellView": "form"
      },
      "source": [
        "#@title test.py (you can edit the output resolution filesize there)\n",
        "%%writefile /content/DFNet/test.py\n",
        "from collections import defaultdict\n",
        "from itertools import islice\n",
        "from multiprocessing.pool import ThreadPool as Pool\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "from utils import list2nparray, gen_miss, merge_imgs\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class Tester:\n",
        "\n",
        "    def __init__(self, model_path, input_size, batch_size):\n",
        "        self.model_path = model_path\n",
        "        self._input_size = input_size\n",
        "        self.batch_size = batch_size\n",
        "        self.init_model(model_path)\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        if self._input_size > 0:\n",
        "            return (self._input_size, self._input_size)\n",
        "        elif 'celeba' in self.model_path:\n",
        "            return (1024, 1024) # edit these values for resolution, must be 2^x\n",
        "        else:\n",
        "            return (1024, 1024)\n",
        "\n",
        "    def init_model(self, path):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "            print('Using gpu.')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            print('Using cpu.')\n",
        "\n",
        "        self.model = DFNet().to(self.device)\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint)\n",
        "        self.model.eval()\n",
        "\n",
        "        print('Model %s loaded.' % path)\n",
        "\n",
        "    def get_name(self, path):\n",
        "        return '.'.join(path.name.split('.')[:-1])\n",
        "\n",
        "    def results_path(self, output, img_path, mask_path, prefix='result'):\n",
        "        img_name = self.get_name(img_path)\n",
        "        mask_name = self.get_name(mask_path)\n",
        "        return {\n",
        "            'result_path': self.sub_dir('result').joinpath(\n",
        "                'result-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'raw_path': self.sub_dir('raw').joinpath(\n",
        "                'raw-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'alpha_path': self.sub_dir('alpha').joinpath(\n",
        "                'alpha-{}-{}.png'.format(img_name, mask_name))\n",
        "        }\n",
        "\n",
        "    def inpaint_instance(self, img, mask):\n",
        "        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n",
        "        img = img.view(1, *img.shape)\n",
        "        mask = mask.view(1, 1, *mask.shape)\n",
        "        return self.inpaint_batch(img, mask).squeeze()\n",
        "\n",
        "    def inpaint_batch(self, imgs, masks):\n",
        "        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n",
        "        imgs = np.transpose(imgs, [0, 3, 1, 2])\n",
        "        masks = np.transpose(masks, [0, 3, 1, 2])\n",
        "\n",
        "        imgs = torch.from_numpy(imgs).to(self.device)\n",
        "        masks = torch.from_numpy(masks).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "        results = self.model(imgs_miss, masks)\n",
        "        if type(results) is list:\n",
        "            results = results[0]\n",
        "        results = results.mul(255).byte().data.cpu().numpy()\n",
        "        results = np.transpose(results, [0, 2, 3, 1])\n",
        "        return results\n",
        "\n",
        "    def _process_file(self, output, img_path, mask_path):\n",
        "        item = {\n",
        "            'img_path': img_path,\n",
        "            'mask_path': mask_path,\n",
        "        }\n",
        "        item.update(self.results_path(output, img_path, mask_path))\n",
        "        self.path_pair.append(item)\n",
        "\n",
        "    def process_single_file(self, output, img_path, mask_path):\n",
        "        self.path_pair = []\n",
        "        self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def process_dir(self, output, img_dir, mask_dir):\n",
        "        img_dir = Path(img_dir)\n",
        "        mask_dir = Path(mask_dir)\n",
        "        imgs_path = sorted(\n",
        "            list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))\n",
        "        masks_path = sorted(\n",
        "            list(mask_dir.glob('*.jpg')) + list(mask_dir.glob('*.png')))\n",
        "\n",
        "        n_img = len(imgs_path)\n",
        "        n_mask = len(masks_path)\n",
        "        n_pair = min(n_img, n_mask)\n",
        "\n",
        "        self.path_pair = []\n",
        "        for i in range(n_pair):\n",
        "            img_path = imgs_path[i % n_img]\n",
        "            mask_path = masks_path[i % n_mask]\n",
        "            self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def get_process(self, input_size):\n",
        "        def process(pair):\n",
        "            img = cv2.imread(str(pair['img_path']), cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(str(pair['mask_path']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if input_size:\n",
        "                img = cv2.resize(img, input_size)\n",
        "                mask = cv2.resize(mask, input_size)\n",
        "\n",
        "            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n",
        "            mask = np.ascontiguousarray(\n",
        "                np.expand_dims(mask, 0)).astype(np.uint8)\n",
        "\n",
        "            pair['img'] = img\n",
        "            pair['mask'] = mask\n",
        "            return pair\n",
        "        return process\n",
        "\n",
        "    def _file_batch(self):\n",
        "        pool = Pool()\n",
        "\n",
        "        n_pair = len(self.path_pair)\n",
        "        n_batch = (n_pair-1) // self.batch_size + 1\n",
        "\n",
        "        for i in tqdm.trange(n_batch, leave=False):\n",
        "            _buffer = defaultdict(list)\n",
        "            start = i * self.batch_size\n",
        "            stop = start + self.batch_size\n",
        "            process = self.get_process(self.input_size)\n",
        "            batch = pool.imap_unordered(\n",
        "                process, islice(self.path_pair, start, stop))\n",
        "            for instance in batch:\n",
        "                for k, v in instance.items():\n",
        "                    _buffer[k].append(v)\n",
        "            yield _buffer\n",
        "\n",
        "    def batch_generator(self):\n",
        "        generator = self._file_batch\n",
        "\n",
        "        for _buffer in generator():\n",
        "            for key in _buffer:\n",
        "                if key in ['img', 'mask']:\n",
        "                    _buffer[key] = list2nparray(_buffer[key])\n",
        "            yield _buffer\n",
        "\n",
        "    def to_numpy(self, tensor):\n",
        "        tensor = tensor.mul(255).byte().data.cpu().numpy()\n",
        "        tensor = np.transpose(tensor, [0, 2, 3, 1])\n",
        "        return tensor\n",
        "\n",
        "    def process_batch(self, batch, output):\n",
        "        imgs = torch.from_numpy(batch['img']).to(self.device)\n",
        "        masks = torch.from_numpy(batch['mask']).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "\n",
        "        result, alpha, raw = self.model(imgs_miss, masks)\n",
        "        result, alpha, raw = result[0], alpha[0], raw[0]\n",
        "        result = imgs * masks + result * (1 - masks)\n",
        "\n",
        "        result = self.to_numpy(result)\n",
        "        alpha = self.to_numpy(alpha)\n",
        "        raw = self.to_numpy(raw)\n",
        "\n",
        "        for i in range(result.shape[0]):\n",
        "            cv2.imwrite(str(batch['result_path'][i]), result[i])\n",
        "            cv2.imwrite(str(batch['raw_path'][i]), raw[i])\n",
        "            cv2.imwrite(str(batch['alpha_path'][i]), alpha[i])\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        return Path(self.output)\n",
        "\n",
        "    def sub_dir(self, sub):\n",
        "        return self.root.joinpath(sub)\n",
        "\n",
        "    def prepare_folders(self, folders):\n",
        "        for folder in folders:\n",
        "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def inpaint(self, output, img, mask, merge_result=False):\n",
        "\n",
        "        self.output = output\n",
        "        self.prepare_folders([\n",
        "            self.sub_dir('result'), self.sub_dir('alpha'),\n",
        "            self.sub_dir('raw')])\n",
        "\n",
        "        if os.path.isfile(img) and os.path.isfile(mask):\n",
        "            if img.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                self.process_single_file(output, img, mask)\n",
        "                _type = 'file'\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "        elif os.path.isdir(img) and os.path.isdir(mask):\n",
        "            self.process_dir(output, img, mask)\n",
        "            _type = 'dir'\n",
        "        else:\n",
        "            print('Img: ', img)\n",
        "            print('Mask: ', mask)\n",
        "            raise NotImplementedError(\n",
        "                'img and mask should be both file or directory.')\n",
        "\n",
        "        print('# Inpainting...')\n",
        "        print('Input size:', self.input_size)\n",
        "        for batch in self.batch_generator():\n",
        "            self.process_batch(batch, output)\n",
        "        print('Inpainting finished.')\n",
        "\n",
        "        if merge_result and _type == 'dir':\n",
        "            miss = self.sub_dir('miss')\n",
        "            merge = self.sub_dir('merge')\n",
        "\n",
        "            print('# Preparing input images...')\n",
        "            gen_miss(img, mask, miss)\n",
        "            print('# Merging...')\n",
        "            merge_imgs([\n",
        "                miss, self.sub_dir('raw'), self.sub_dir('alpha'),\n",
        "                self.sub_dir('result'), img], merge, res=self.input_size[0])\n",
        "            print('Merging finished.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '-m', '--model', default='./model/model_places2.pth',\n",
        "        help='Select a checkpoint.')\n",
        "    parser.add_argument(\n",
        "        '-i', '--input_size', default=0, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '-b', '--batch_size', default=8, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '--img', default='./samples/places2/img',\n",
        "        help='Image or Image folder.')\n",
        "    parser.add_argument(\n",
        "        '--mask', default='./samples/places2/mask',\n",
        "        help='Mask or Mask folder.')\n",
        "    parser.add_argument('--output', default='./output/places2',\n",
        "        help='Output dir')\n",
        "    parser.add_argument(\n",
        "        '--merge', action='store_true',\n",
        "        help='Whether merge input and results for better viewing.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    tester = Tester(args.model, args.input_size, args.batch_size)\n",
        "\n",
        "    tester.inpaint(args.output, args.img, args.mask, merge_result=args.merge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsY6cznc5e-k"
      },
      "source": [
        "%cd /content/DFNet\n",
        "!python test.py --model /content/DFNet/model/model_places2.pth --img samples/places2/img --mask samples/places2/mask --output output/places2 --merge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-7zQWEOTmjn"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzSKGiaLTlxC"
      },
      "source": [
        "# using a fork, since the original repo does not provide training code\n",
        "!git clone https://github.com/Yukariin/DFNet\n",
        "!pip install tensorboardX\n",
        "!pip install LPIPS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXURbm6F0Q7m"
      },
      "source": [
        "[Experimental] Additional losses: HFENLoss (high frequency error norm), ElasticLoss, RelativeL1, L1CosineSim, ClipL1, FFTloss, OFLoss (Overflow loss), GPLoss (Gradient Profile (GP) loss), CPLoss (Color Profile (CP) loss) and Contextual_Loss. Config weight value and combination in ```loss.py```.\n",
        "\n",
        "Warning: If AMP is used together with Style loss, then it will result in Nan errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQQrK_JTyMc"
      },
      "source": [
        "%cd /content/DFNet\n",
        "!python train.py --root /content/train_data --batch_size 2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
