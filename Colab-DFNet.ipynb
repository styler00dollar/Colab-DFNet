{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LX9rquFVL-y"
      },
      "source": [
        "# DFNet\n",
        "Original repo: [hughplay/DFNet](https://github.com/hughplay/DFNet)\n",
        "\n",
        "Fork with training code: [Yukariin/DFNet](https://github.com/Yukariin/DFNet)\n",
        "\n",
        "Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "Warning: Black means inpainted area and white means original area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENPA1Aq4tmT"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWkqbDuLgBeY"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWcV-Udp4zqP",
        "cellView": "form"
      },
      "source": [
        "#@title git clone original repo and download models\n",
        "!git clone https://github.com/hughplay/DFNet.git\n",
        "%cd DFNet/model\n",
        "!pip install gdown\n",
        "# places\n",
        "!gdown --id 1SGJ_Z9kpchdnZ3Qwwf4HnN-Cq-AeK7vH\n",
        "# celeba\n",
        "!gdown --id 1e6KVfSdILygDcyL-ps1jckS4Ff18Z3rj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsSglYdtDGeA",
        "cellView": "form"
      },
      "source": [
        "#@title test.py (you can edit the output resolution filesize there)\n",
        "%%writefile /content/DFNet/test.py\n",
        "from collections import defaultdict\n",
        "from itertools import islice\n",
        "from multiprocessing.pool import ThreadPool as Pool\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "from utils import list2nparray, gen_miss, merge_imgs\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class Tester:\n",
        "\n",
        "    def __init__(self, model_path, input_size, batch_size):\n",
        "        self.model_path = model_path\n",
        "        self._input_size = input_size\n",
        "        self.batch_size = batch_size\n",
        "        self.init_model(model_path)\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        if self._input_size > 0:\n",
        "            return (self._input_size, self._input_size)\n",
        "        elif 'celeba' in self.model_path:\n",
        "            return (1024, 1024) # edit these values for resolution, must be 2^x\n",
        "        else:\n",
        "            return (1024, 1024)\n",
        "\n",
        "    def init_model(self, path):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "            print('Using gpu.')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            print('Using cpu.')\n",
        "\n",
        "        self.model = DFNet().to(self.device)\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint)\n",
        "        self.model.eval()\n",
        "\n",
        "        print('Model %s loaded.' % path)\n",
        "\n",
        "    def get_name(self, path):\n",
        "        return '.'.join(path.name.split('.')[:-1])\n",
        "\n",
        "    def results_path(self, output, img_path, mask_path, prefix='result'):\n",
        "        img_name = self.get_name(img_path)\n",
        "        mask_name = self.get_name(mask_path)\n",
        "        return {\n",
        "            'result_path': self.sub_dir('result').joinpath(\n",
        "                'result-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'raw_path': self.sub_dir('raw').joinpath(\n",
        "                'raw-{}-{}.png'.format(img_name, mask_name)),\n",
        "            'alpha_path': self.sub_dir('alpha').joinpath(\n",
        "                'alpha-{}-{}.png'.format(img_name, mask_name))\n",
        "        }\n",
        "\n",
        "    def inpaint_instance(self, img, mask):\n",
        "        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n",
        "        img = img.view(1, *img.shape)\n",
        "        mask = mask.view(1, 1, *mask.shape)\n",
        "        return self.inpaint_batch(img, mask).squeeze()\n",
        "\n",
        "    def inpaint_batch(self, imgs, masks):\n",
        "        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n",
        "        imgs = np.transpose(imgs, [0, 3, 1, 2])\n",
        "        masks = np.transpose(masks, [0, 3, 1, 2])\n",
        "\n",
        "        imgs = torch.from_numpy(imgs).to(self.device)\n",
        "        masks = torch.from_numpy(masks).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "        results = self.model(imgs_miss, masks)\n",
        "        if type(results) is list:\n",
        "            results = results[0]\n",
        "        results = results.mul(255).byte().data.cpu().numpy()\n",
        "        results = np.transpose(results, [0, 2, 3, 1])\n",
        "        return results\n",
        "\n",
        "    def _process_file(self, output, img_path, mask_path):\n",
        "        item = {\n",
        "            'img_path': img_path,\n",
        "            'mask_path': mask_path,\n",
        "        }\n",
        "        item.update(self.results_path(output, img_path, mask_path))\n",
        "        self.path_pair.append(item)\n",
        "\n",
        "    def process_single_file(self, output, img_path, mask_path):\n",
        "        self.path_pair = []\n",
        "        self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def process_dir(self, output, img_dir, mask_dir):\n",
        "        img_dir = Path(img_dir)\n",
        "        mask_dir = Path(mask_dir)\n",
        "        imgs_path = sorted(\n",
        "            list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))\n",
        "        masks_path = sorted(\n",
        "            list(mask_dir.glob('*.jpg')) + list(mask_dir.glob('*.png')))\n",
        "\n",
        "        n_img = len(imgs_path)\n",
        "        n_mask = len(masks_path)\n",
        "        n_pair = min(n_img, n_mask)\n",
        "\n",
        "        self.path_pair = []\n",
        "        for i in range(n_pair):\n",
        "            img_path = imgs_path[i % n_img]\n",
        "            mask_path = masks_path[i % n_mask]\n",
        "            self._process_file(output, img_path, mask_path)\n",
        "\n",
        "    def get_process(self, input_size):\n",
        "        def process(pair):\n",
        "            img = cv2.imread(str(pair['img_path']), cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(str(pair['mask_path']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if input_size:\n",
        "                img = cv2.resize(img, input_size)\n",
        "                mask = cv2.resize(mask, input_size)\n",
        "\n",
        "            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n",
        "            mask = np.ascontiguousarray(\n",
        "                np.expand_dims(mask, 0)).astype(np.uint8)\n",
        "\n",
        "            pair['img'] = img\n",
        "            pair['mask'] = mask\n",
        "            return pair\n",
        "        return process\n",
        "\n",
        "    def _file_batch(self):\n",
        "        pool = Pool()\n",
        "\n",
        "        n_pair = len(self.path_pair)\n",
        "        n_batch = (n_pair-1) // self.batch_size + 1\n",
        "\n",
        "        for i in tqdm.trange(n_batch, leave=False):\n",
        "            _buffer = defaultdict(list)\n",
        "            start = i * self.batch_size\n",
        "            stop = start + self.batch_size\n",
        "            process = self.get_process(self.input_size)\n",
        "            batch = pool.imap_unordered(\n",
        "                process, islice(self.path_pair, start, stop))\n",
        "            for instance in batch:\n",
        "                for k, v in instance.items():\n",
        "                    _buffer[k].append(v)\n",
        "            yield _buffer\n",
        "\n",
        "    def batch_generator(self):\n",
        "        generator = self._file_batch\n",
        "\n",
        "        for _buffer in generator():\n",
        "            for key in _buffer:\n",
        "                if key in ['img', 'mask']:\n",
        "                    _buffer[key] = list2nparray(_buffer[key])\n",
        "            yield _buffer\n",
        "\n",
        "    def to_numpy(self, tensor):\n",
        "        tensor = tensor.mul(255).byte().data.cpu().numpy()\n",
        "        tensor = np.transpose(tensor, [0, 2, 3, 1])\n",
        "        return tensor\n",
        "\n",
        "    def process_batch(self, batch, output):\n",
        "        imgs = torch.from_numpy(batch['img']).to(self.device)\n",
        "        masks = torch.from_numpy(batch['mask']).to(self.device)\n",
        "        imgs = imgs.float().div(255)\n",
        "        masks = masks.float().div(255)\n",
        "        imgs_miss = imgs * masks\n",
        "\n",
        "        result, alpha, raw = self.model(imgs_miss, masks)\n",
        "        result, alpha, raw = result[0], alpha[0], raw[0]\n",
        "        result = imgs * masks + result * (1 - masks)\n",
        "\n",
        "        result = self.to_numpy(result)\n",
        "        alpha = self.to_numpy(alpha)\n",
        "        raw = self.to_numpy(raw)\n",
        "\n",
        "        for i in range(result.shape[0]):\n",
        "            cv2.imwrite(str(batch['result_path'][i]), result[i])\n",
        "            cv2.imwrite(str(batch['raw_path'][i]), raw[i])\n",
        "            cv2.imwrite(str(batch['alpha_path'][i]), alpha[i])\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        return Path(self.output)\n",
        "\n",
        "    def sub_dir(self, sub):\n",
        "        return self.root.joinpath(sub)\n",
        "\n",
        "    def prepare_folders(self, folders):\n",
        "        for folder in folders:\n",
        "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def inpaint(self, output, img, mask, merge_result=False):\n",
        "\n",
        "        self.output = output\n",
        "        self.prepare_folders([\n",
        "            self.sub_dir('result'), self.sub_dir('alpha'),\n",
        "            self.sub_dir('raw')])\n",
        "\n",
        "        if os.path.isfile(img) and os.path.isfile(mask):\n",
        "            if img.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                self.process_single_file(output, img, mask)\n",
        "                _type = 'file'\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "        elif os.path.isdir(img) and os.path.isdir(mask):\n",
        "            self.process_dir(output, img, mask)\n",
        "            _type = 'dir'\n",
        "        else:\n",
        "            print('Img: ', img)\n",
        "            print('Mask: ', mask)\n",
        "            raise NotImplementedError(\n",
        "                'img and mask should be both file or directory.')\n",
        "\n",
        "        print('# Inpainting...')\n",
        "        print('Input size:', self.input_size)\n",
        "        for batch in self.batch_generator():\n",
        "            self.process_batch(batch, output)\n",
        "        print('Inpainting finished.')\n",
        "\n",
        "        if merge_result and _type == 'dir':\n",
        "            miss = self.sub_dir('miss')\n",
        "            merge = self.sub_dir('merge')\n",
        "\n",
        "            print('# Preparing input images...')\n",
        "            gen_miss(img, mask, miss)\n",
        "            print('# Merging...')\n",
        "            merge_imgs([\n",
        "                miss, self.sub_dir('raw'), self.sub_dir('alpha'),\n",
        "                self.sub_dir('result'), img], merge, res=self.input_size[0])\n",
        "            print('Merging finished.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '-m', '--model', default='./model/model_places2.pth',\n",
        "        help='Select a checkpoint.')\n",
        "    parser.add_argument(\n",
        "        '-i', '--input_size', default=0, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '-b', '--batch_size', default=8, type=int,\n",
        "        help='Batch size for testing.')\n",
        "    parser.add_argument(\n",
        "        '--img', default='./samples/places2/img',\n",
        "        help='Image or Image folder.')\n",
        "    parser.add_argument(\n",
        "        '--mask', default='./samples/places2/mask',\n",
        "        help='Mask or Mask folder.')\n",
        "    parser.add_argument('--output', default='./output/places2',\n",
        "        help='Output dir')\n",
        "    parser.add_argument(\n",
        "        '--merge', action='store_true',\n",
        "        help='Whether merge input and results for better viewing.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    tester = Tester(args.model, args.input_size, args.batch_size)\n",
        "\n",
        "    tester.inpaint(args.output, args.img, args.mask, merge_result=args.merge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsY6cznc5e-k"
      },
      "source": [
        "%cd /content/DFNet\n",
        "!python test.py --model /content/DFNet/model/model_places2.pth --img samples/places2/img --mask samples/places2/mask --output output/places2 --merge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-7zQWEOTmjn"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzSKGiaLTlxC"
      },
      "source": [
        "# using a fork, since the original repo does not provide training code\n",
        "!git clone https://github.com/Yukariin/DFNet\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXURbm6F0Q7m"
      },
      "source": [
        "[Experimental] Additional losses: HFENLoss (high frequency error norm), ElasticLoss, RelativeL1, L1CosineSim, ClipL1, FFTloss, OFLoss (Overflow loss), GPLoss (Gradient Profile (GP) loss), CPLoss (Color Profile (CP) loss) and Contextual_Loss. Config weight value and combination in ```loss.py```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGEHLG7BhQgz",
        "cellView": "form"
      },
      "source": [
        "#@title loss.py mod\n",
        "%%writefile /content/DFNet/loss.py\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "from utils import resize_like\n",
        "\n",
        "\n",
        "def gram_matrix(y):\n",
        "    # https://github.com/pytorch/examples/blob/master/fast_neural_style/neural_style/utils.py\n",
        "    (b, ch, h, w) = y.size()\n",
        "    features = y.view(b, ch, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t) / (ch * h * w)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def total_variation_loss(y):\n",
        "    loss = (\n",
        "        torch.mean(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) +\n",
        "        torch.mean(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.add_module('vgg', VGG16())\n",
        "        self.criterion = nn.L1Loss()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "\n",
        "        content_loss = 0.0\n",
        "        for x_feat, y_feat in zip(x_vgg, y_vgg):\n",
        "            content_loss += self.criterion(x_feat, y_feat)\n",
        "\n",
        "        return content_loss\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.add_module('vgg', VGG16())\n",
        "        self.criterion = nn.L1Loss()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "\n",
        "        style_loss = 0.0\n",
        "        for x_feat, y_feat in zip(x_vgg, y_vgg):\n",
        "            style_loss += self.criterion(gram_matrix(x_feat), gram_matrix(y_feat))\n",
        "\n",
        "        return style_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VGG16(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        features = models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), features[x])\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.slice1(x)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AdversarialLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Adversarial loss\n",
        "    https://arxiv.org/abs/1711.10337\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, type='nsgan', target_real_label=1.0, target_fake_label=0.0):\n",
        "        r\"\"\"\n",
        "        type = nsgan | lsgan | hinge\n",
        "        \"\"\"\n",
        "        super(AdversarialLoss, self).__init__()\n",
        "\n",
        "        self.type = type\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "\n",
        "        if type == 'nsgan':\n",
        "            self.criterion = nn.BCELoss()\n",
        "\n",
        "        elif type == 'lsgan':\n",
        "            self.criterion = nn.MSELoss()\n",
        "\n",
        "        elif type == 'hinge':\n",
        "            self.criterion = nn.ReLU()\n",
        "\n",
        "    def __call__(self, outputs, is_real, is_disc=None):\n",
        "        if self.type == 'hinge':\n",
        "            if is_disc:\n",
        "                if is_real:\n",
        "                    outputs = -outputs\n",
        "                return self.criterion(1 + outputs).mean()\n",
        "            else:\n",
        "                return (-outputs).mean()\n",
        "\n",
        "        else:\n",
        "            labels = (self.real_label if is_real else self.fake_label).expand_as(outputs)\n",
        "            # old loss\n",
        "            #loss = self.criterion(outputs, labels)\n",
        "            \n",
        "            # old loss is not compatible with amp. 2 loss alternatives.\n",
        "            # using BCE loss\n",
        "            #loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, labels)\n",
        "            \n",
        "            criterion = torch.nn.BCEWithLogitsLoss()\n",
        "            loss = criterion(outputs, labels)\n",
        "            return loss\n",
        "\n",
        "class VGG19(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG19, self).__init__()\n",
        "        features = models.vgg19(pretrained=True).features\n",
        "        self.relu1_1 = torch.nn.Sequential()\n",
        "        self.relu1_2 = torch.nn.Sequential()\n",
        "\n",
        "        self.relu2_1 = torch.nn.Sequential()\n",
        "        self.relu2_2 = torch.nn.Sequential()\n",
        "\n",
        "        self.relu3_1 = torch.nn.Sequential()\n",
        "        self.relu3_2 = torch.nn.Sequential()\n",
        "        self.relu3_3 = torch.nn.Sequential()\n",
        "        self.relu3_4 = torch.nn.Sequential()\n",
        "\n",
        "        self.relu4_1 = torch.nn.Sequential()\n",
        "        self.relu4_2 = torch.nn.Sequential()\n",
        "        self.relu4_3 = torch.nn.Sequential()\n",
        "        self.relu4_4 = torch.nn.Sequential()\n",
        "\n",
        "        self.relu5_1 = torch.nn.Sequential()\n",
        "        self.relu5_2 = torch.nn.Sequential()\n",
        "        self.relu5_3 = torch.nn.Sequential()\n",
        "        self.relu5_4 = torch.nn.Sequential()\n",
        "\n",
        "        for x in range(2):\n",
        "            self.relu1_1.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(2, 4):\n",
        "            self.relu1_2.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(4, 7):\n",
        "            self.relu2_1.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(7, 9):\n",
        "            self.relu2_2.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(9, 12):\n",
        "            self.relu3_1.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(12, 14):\n",
        "            self.relu3_2.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(14, 16):\n",
        "            self.relu3_3.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(16, 18):\n",
        "            self.relu3_4.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(18, 21):\n",
        "            self.relu4_1.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(21, 23):\n",
        "            self.relu4_2.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(23, 25):\n",
        "            self.relu4_3.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(25, 27):\n",
        "            self.relu4_4.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(27, 30):\n",
        "            self.relu5_1.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(30, 32):\n",
        "            self.relu5_2.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(32, 34):\n",
        "            self.relu5_3.add_module(str(x), features[x])\n",
        "\n",
        "        for x in range(34, 36):\n",
        "            self.relu5_4.add_module(str(x), features[x])\n",
        "\n",
        "        # don't need the gradients, just want the features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        relu1_1 = self.relu1_1(x)\n",
        "        relu1_2 = self.relu1_2(relu1_1)\n",
        "\n",
        "        relu2_1 = self.relu2_1(relu1_2)\n",
        "        relu2_2 = self.relu2_2(relu2_1)\n",
        "\n",
        "        relu3_1 = self.relu3_1(relu2_2)\n",
        "        relu3_2 = self.relu3_2(relu3_1)\n",
        "        relu3_3 = self.relu3_3(relu3_2)\n",
        "        relu3_4 = self.relu3_4(relu3_3)\n",
        "\n",
        "        relu4_1 = self.relu4_1(relu3_4)\n",
        "        relu4_2 = self.relu4_2(relu4_1)\n",
        "        relu4_3 = self.relu4_3(relu4_2)\n",
        "        relu4_4 = self.relu4_4(relu4_3)\n",
        "\n",
        "        relu5_1 = self.relu5_1(relu4_4)\n",
        "        relu5_2 = self.relu5_2(relu5_1)\n",
        "        relu5_3 = self.relu5_3(relu5_2)\n",
        "        relu5_4 = self.relu5_4(relu5_3)\n",
        "\n",
        "        out = {\n",
        "            'relu1_1': relu1_1,\n",
        "            'relu1_2': relu1_2,\n",
        "\n",
        "            'relu2_1': relu2_1,\n",
        "            'relu2_2': relu2_2,\n",
        "\n",
        "            'relu3_1': relu3_1,\n",
        "            'relu3_2': relu3_2,\n",
        "            'relu3_3': relu3_3,\n",
        "            'relu3_4': relu3_4,\n",
        "\n",
        "            'relu4_1': relu4_1,\n",
        "            'relu4_2': relu4_2,\n",
        "            'relu4_3': relu4_3,\n",
        "            'relu4_4': relu4_4,\n",
        "\n",
        "            'relu5_1': relu5_1,\n",
        "            'relu5_2': relu5_2,\n",
        "            'relu5_3': relu5_3,\n",
        "            'relu5_4': relu5_4,\n",
        "        }\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.models.vgg as vgg\n",
        "from collections import OrderedDict\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "# VGG MODEL\n",
        "# models.modules.architectures.perceptual\n",
        "\n",
        "vgg_layer19 = {\n",
        "    'conv_1_1': 0, 'conv_1_2': 2, 'pool_1': 4, 'conv_2_1': 5, 'conv_2_2': 7, 'pool_2': 9, 'conv_3_1': 10, 'conv_3_2': 12, 'conv_3_3': 14, 'conv_3_4': 16, 'pool_3': 18, 'conv_4_1': 19, 'conv_4_2': 21, 'conv_4_3': 23, 'conv_4_4': 25, 'pool_4': 27, 'conv_5_1': 28, 'conv_5_2': 30, 'conv_5_3': 32, 'conv_5_4': 34, 'pool_5': 36\n",
        "}\n",
        "vgg_layer_inv19 = {\n",
        "    0: 'conv_1_1', 2: 'conv_1_2', 4: 'pool_1', 5: 'conv_2_1', 7: 'conv_2_2', 9: 'pool_2', 10: 'conv_3_1', 12: 'conv_3_2', 14: 'conv_3_3', 16: 'conv_3_4', 18: 'pool_3', 19: 'conv_4_1', 21: 'conv_4_2', 23: 'conv_4_3', 25: 'conv_4_4', 27: 'pool_4', 28: 'conv_5_1', 30: 'conv_5_2', 32: 'conv_5_3', 34: 'conv_5_4', 36: 'pool_5'\n",
        "}\n",
        "# VGG 16 layers to listen to\n",
        "vgg_layer16 = {\n",
        "    'conv_1_1': 0, 'conv_1_2': 2, 'pool_1': 4, 'conv_2_1': 5, 'conv_2_2': 7, 'pool_2': 9, 'conv_3_1': 10, 'conv_3_2': 12, 'conv_3_3': 14, 'pool_3': 16, 'conv_4_1': 17, 'conv_4_2': 19, 'conv_4_3': 21, 'pool_4': 23, 'conv_5_1': 24, 'conv_5_2': 26, 'conv_5_3': 28, 'pool_5': 30\n",
        "}\n",
        "vgg_layer_inv16 = {\n",
        "    0: 'conv_1_1', 2: 'conv_1_2', 4: 'pool_1', 5: 'conv_2_1', 7: 'conv_2_2', 9: 'pool_2', 10: 'conv_3_1', 12: 'conv_3_2', 14: 'conv_3_3', 16: 'pool_3', 17: 'conv_4_1', 19: 'conv_4_2', 21: 'conv_4_3', 23: 'pool_4', 24: 'conv_5_1', 26: 'conv_5_2', 28: 'conv_5_3', 30: 'pool_5'\n",
        "}\n",
        "\n",
        "class VGG_Model(nn.Module):\n",
        "    \"\"\"\n",
        "        A VGG model with listerners in the layers. \n",
        "        Will return a dictionary of outputs that correspond to the \n",
        "        layers set in \"listen_list\".\n",
        "    \"\"\"\n",
        "    def __init__(self, listen_list=None, net='vgg19', use_input_norm=True, z_norm=False):\n",
        "        super(VGG_Model, self).__init__()\n",
        "        #vgg = vgg16(pretrained=True)\n",
        "        if net == 'vgg19':\n",
        "            vgg_net = vgg.vgg19(pretrained=True)\n",
        "            vgg_layer = vgg_layer19\n",
        "            self.vgg_layer_inv = vgg_layer_inv19\n",
        "        elif net == 'vgg16':\n",
        "            vgg_net = vgg.vgg16(pretrained=True)\n",
        "            vgg_layer = vgg_layer16\n",
        "            self.vgg_layer_inv = vgg_layer_inv16\n",
        "        self.vgg_model = vgg_net.features\n",
        "        self.use_input_norm = use_input_norm\n",
        "        # image normalization\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.tensor(\n",
        "                    [[[0.485-1]], [[0.456-1]], [[0.406-1]]], requires_grad=False)\n",
        "                std = torch.tensor(\n",
        "                    [[[0.229*2]], [[0.224*2]], [[0.225*2]]], requires_grad=False)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.tensor(\n",
        "                    [[[0.485]], [[0.456]], [[0.406]]], requires_grad=False)\n",
        "                std = torch.tensor(\n",
        "                    [[[0.229]], [[0.224]], [[0.225]]], requires_grad=False)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "\n",
        "        vgg_dict = vgg_net.state_dict()\n",
        "        vgg_f_dict = self.vgg_model.state_dict()\n",
        "        vgg_dict = {k: v for k, v in vgg_dict.items() if k in vgg_f_dict}\n",
        "        vgg_f_dict.update(vgg_dict)\n",
        "        # no grad\n",
        "        for p in self.vgg_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        if listen_list == []:\n",
        "            self.listen = []\n",
        "        else:\n",
        "            self.listen = set()\n",
        "            for layer in listen_list:\n",
        "                self.listen.add(vgg_layer[layer])\n",
        "        self.features = OrderedDict()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean.detach()) / self.std.detach()\n",
        "\n",
        "        for index, layer in enumerate(self.vgg_model):\n",
        "            x = layer(x)\n",
        "            if index in self.listen:\n",
        "                self.features[self.vgg_layer_inv[index]] = x\n",
        "        return self.features\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "# from dataops.filters import *\n",
        "# codes/dataops/filters.py\n",
        "\n",
        "'''\n",
        "    Multiple image filters used by different functions. Can also be used as augmentations.\n",
        "'''\n",
        "\n",
        "import numbers\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from dataops.common import denorm\n",
        "\n",
        "def denorm(x, min_max=(-1.0, 1.0)):\n",
        "    '''\n",
        "        Denormalize from [-1,1] range to [0,1]\n",
        "        formula: xi' = (xi - mu)/sigma\n",
        "        Example: \"out = (x + 1.0) / 2.0\" for denorm \n",
        "            range (-1,1) to (0,1)\n",
        "        for use with proper act in Generator output (ie. tanh)\n",
        "    '''\n",
        "    out = (x - min_max[0]) / (min_max[1] - min_max[0])\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return out.clamp(0, 1)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        return np.clip(out, 0, 1)\n",
        "    else:\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor or \\\n",
        "        np.ndarray\")\n",
        "\n",
        "\n",
        "def get_kernel_size(sigma = 6):\n",
        "    '''\n",
        "        Get optimal gaussian kernel size according to sigma * 6 criterion \n",
        "        (must return an int)\n",
        "        Alternative from Matlab: kernel_size=2*np.ceil(3*sigma)+1\n",
        "        https://stackoverflow.com/questions/3149279/optimal-sigma-for-gaussian-filtering-of-an-image\n",
        "    '''\n",
        "    kernel_size = np.ceil(sigma*6)\n",
        "    return kernel_size\n",
        "\n",
        "def get_kernel_sigma(kernel_size = 5):\n",
        "    '''\n",
        "        Get optimal gaussian kernel sigma (variance) according to kernel_size/6 \n",
        "        Alternative from Matlab: sigma = (kernel_size-1)/6\n",
        "    '''\n",
        "    return kernel_size/6.0\n",
        "\n",
        "def get_kernel_mean(kernel_size = 5):\n",
        "    '''\n",
        "        Get gaussian kernel mean\n",
        "    '''\n",
        "    return (kernel_size - 1) / 2.0\n",
        "\n",
        "def kernel_conv_w(kernel, channels: int =3):\n",
        "    '''\n",
        "        Reshape a H*W kernel to 2d depthwise convolutional \n",
        "            weight (for loading in a Conv2D)\n",
        "    '''\n",
        "\n",
        "    # Dynamic window expansion. expand() does not copy memory, needs contiguous()\n",
        "    kernel = kernel.expand(channels, 1, *kernel.size()).contiguous()\n",
        "    return kernel\n",
        "\n",
        "#@torch.jit.script\n",
        "def get_gaussian_kernel1d(kernel_size: int,\n",
        "                sigma: float = 1.5, \n",
        "                #channel: int = None,\n",
        "                force_even: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that returns 1-D Gaussian filter kernel coefficients.\n",
        "    Args:\n",
        "        kernel_size (int): filter/window size. It should be odd and positive.\n",
        "        sigma (float): gaussian standard deviation, sigma of normal distribution\n",
        "        force_even (bool): overrides requirement for odd kernel size.\n",
        "    Returns:\n",
        "        torch.Tensor: 1D tensor with 1D gaussian filter coefficients.\n",
        "    Shape:\n",
        "        - Output: :math:`(\\text{kernel_size})`\n",
        "    Examples::\n",
        "        >>> get_gaussian_kernel1d(3, 2.5)\n",
        "        tensor([0.3243, 0.3513, 0.3243])\n",
        "        >>> get_gaussian_kernel1d(5, 1.5)\n",
        "        tensor([0.1201, 0.2339, 0.2921, 0.2339, 0.1201])\n",
        "    \"\"\"\n",
        "        \n",
        "    if (not isinstance(kernel_size, int) or (\n",
        "            (kernel_size % 2 == 0) and not force_even) or (\n",
        "            kernel_size <= 0)):\n",
        "        raise TypeError(\n",
        "            \"kernel_size must be an odd positive integer. \"\n",
        "            \"Got {}\".format(kernel_size)\n",
        "        )\n",
        "\n",
        "    if kernel_size % 2 == 0:\n",
        "        x = torch.arange(kernel_size).float() - kernel_size // 2    \n",
        "        x = x + 0.5\n",
        "        gauss = torch.exp((-x.pow(2.0) / float(2 * sigma ** 2)))\n",
        "    else: #much faster\n",
        "        gauss = torch.Tensor([np.exp(-(x - kernel_size//2)**2/float(2*sigma**2)) for x in range(kernel_size)])\n",
        "\n",
        "    gauss /= gauss.sum()\n",
        "    \n",
        "    return gauss\n",
        "\n",
        "#To get the kernel coefficients\n",
        "def get_gaussian_kernel2d(\n",
        "        #kernel_size: Tuple[int, int],\n",
        "        kernel_size,\n",
        "        #sigma: Tuple[float, float],\n",
        "        sigma,\n",
        "        force_even: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that returns Gaussian filter matrix coefficients.\n",
        "         Modified with a faster kernel creation if the kernel size\n",
        "         is odd. \n",
        "    Args:\n",
        "        kernel_size (Tuple[int, int]): filter (window) sizes in the x and y \n",
        "         direction. Sizes should be odd and positive, unless force_even is\n",
        "         used.\n",
        "        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n",
        "         direction.\n",
        "        force_even (bool): overrides requirement for odd kernel size.\n",
        "    Returns:\n",
        "        Tensor: 2D tensor with gaussian filter matrix coefficients.\n",
        "    Shape:\n",
        "        - Output: :math:`(\\text{kernel_size}_x, \\text{kernel_size}_y)`\n",
        "    Examples::\n",
        "        >>> get_gaussian_kernel2d((3, 3), (1.5, 1.5))\n",
        "        tensor([[0.0947, 0.1183, 0.0947],\n",
        "                [0.1183, 0.1478, 0.1183],\n",
        "                [0.0947, 0.1183, 0.0947]])\n",
        "        >>> get_gaussian_kernel2d((3, 5), (1.5, 1.5))\n",
        "        tensor([[0.0370, 0.0720, 0.0899, 0.0720, 0.0370],\n",
        "                [0.0462, 0.0899, 0.1123, 0.0899, 0.0462],\n",
        "                [0.0370, 0.0720, 0.0899, 0.0720, 0.0370]])\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(kernel_size, (int, float)): \n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "    if isinstance(sigma, (int, float)): \n",
        "        sigma = (sigma, sigma)\n",
        "\n",
        "    if not isinstance(kernel_size, tuple) or len(kernel_size) != 2:\n",
        "        raise TypeError(\n",
        "            \"kernel_size must be a tuple of length two. Got {}\".format(\n",
        "                kernel_size\n",
        "            )\n",
        "        )\n",
        "    if not isinstance(sigma, tuple) or len(sigma) != 2:\n",
        "        raise TypeError(\n",
        "            \"sigma must be a tuple of length two. Got {}\".format(sigma)\n",
        "        )\n",
        "    ksize_x, ksize_y = kernel_size\n",
        "    sigma_x, sigma_y = sigma\n",
        "    kernel_x: torch.Tensor = get_gaussian_kernel1d(ksize_x, sigma_x, force_even)\n",
        "    kernel_y: torch.Tensor = get_gaussian_kernel1d(ksize_y, sigma_y, force_even)\n",
        "    \n",
        "    kernel_2d: torch.Tensor = torch.matmul(\n",
        "        kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t()\n",
        "    )\n",
        "    \n",
        "    return kernel_2d\n",
        "\n",
        "def get_gaussian_kernel(kernel_size=5, sigma=3, dim=2):\n",
        "    '''\n",
        "        This function can generate gaussian kernels in any dimension,\n",
        "            but its 3 times slower than get_gaussian_kernel2d()\n",
        "    Arguments:\n",
        "        kernel_size (Tuple[int, int]): filter sizes in the x and y direction.\n",
        "            Sizes should be odd and positive.\n",
        "        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n",
        "            direction.\n",
        "        dim: the image dimension (2D=2, 3D=3, etc)\n",
        "    Returns:\n",
        "        Tensor: tensor with gaussian filter matrix coefficients.\n",
        "    '''\n",
        "\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "    if isinstance(sigma, numbers.Number):\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    kernel = 1\n",
        "    meshgrids = torch.meshgrid(\n",
        "        [\n",
        "            torch.arange(size, dtype=torch.float32)\n",
        "            for size in kernel_size\n",
        "        ]\n",
        "    )\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
        "        mean = (size - 1) / 2\n",
        "        kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
        "                  torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
        "\n",
        "    kernel = kernel / torch.sum(kernel)    \n",
        "    return kernel\n",
        "\n",
        "#TODO: could be modified to generate kernels in different dimensions\n",
        "def get_box_kernel(kernel_size: int = 5, dim=2):\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "\n",
        "    kx: float=  float(kernel_size[0])\n",
        "    ky: float=  float(kernel_size[1])\n",
        "    box_kernel = torch.Tensor(np.ones((kx, ky)) / (kx*ky))\n",
        "\n",
        "    return box_kernel\n",
        "\n",
        "\n",
        "\n",
        "#TODO: Can change HFEN to use either LoG, DoG or XDoG\n",
        "def get_log_kernel_5x5():\n",
        "    '''\n",
        "    This is a precomputed LoG kernel that has already been convolved with\n",
        "    Gaussian, for edge detection. \n",
        "    \n",
        "    http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html\n",
        "    http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm\n",
        "    https://academic.mu.edu/phys/matthysd/web226/Lab02.htm\n",
        "    The 2-D LoG can be approximated by a 5 by 5 convolution kernel such as:\n",
        "    weight_log = torch.Tensor([\n",
        "                    [0, 0, 1, 0, 0],\n",
        "                    [0, 1, 2, 1, 0],\n",
        "                    [1, 2, -16, 2, 1],\n",
        "                    [0, 1, 2, 1, 0],\n",
        "                    [0, 0, 1, 0, 0]\n",
        "                ])\n",
        "    This is an approximate to the LoG kernel with kernel size 5 and optimal \n",
        "    sigma ~6 (0.590155...).\n",
        "    '''\n",
        "    return torch.Tensor([\n",
        "                [0, 0, 1, 0, 0],\n",
        "                [0, 1, 2, 1, 0],\n",
        "                [1, 2, -16, 2, 1],\n",
        "                [0, 1, 2, 1, 0],\n",
        "                [0, 0, 1, 0, 0]\n",
        "            ])\n",
        "\n",
        "#dim is the image dimension (2D=2, 3D=3, etc), but for now the final_kernel is hardcoded to 2D images\n",
        "#Not sure if it would make sense in higher dimensions\n",
        "#Note: Kornia suggests their laplacian kernel can also be used to generate LoG kernel: \n",
        "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html\n",
        "def get_log_kernel2d(kernel_size=5, sigma=None, dim=2): #sigma=0.6; kernel_size=5\n",
        "    \n",
        "    #either kernel_size or sigma are required:\n",
        "    if not kernel_size and sigma:\n",
        "        kernel_size = get_kernel_size(sigma)\n",
        "        kernel_size = [kernel_size] * dim #note: should it be [kernel_size] or [kernel_size-1]? look below \n",
        "    elif kernel_size and not sigma:\n",
        "        sigma = get_kernel_sigma(kernel_size)\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size-1] * dim\n",
        "    if isinstance(sigma, numbers.Number):\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    grids = torch.meshgrid([torch.arange(-size//2,size//2+1,1) for size in kernel_size])\n",
        "\n",
        "    kernel = 1\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, grids):\n",
        "        kernel *= torch.exp(-(mgrid**2/(2.*std**2)))\n",
        "    \n",
        "    #TODO: For now hardcoded to 2 dimensions, test to make it work in any dimension\n",
        "    final_kernel = (kernel) * ((grids[0]**2 + grids[1]**2) - (2*sigma[0]*sigma[1])) * (1/((2*math.pi)*(sigma[0]**2)*(sigma[1]**2)))\n",
        "    \n",
        "    #TODO: Test if normalization has to be negative (the inverted peak should not make a difference)\n",
        "    final_kernel = -final_kernel / torch.sum(final_kernel)\n",
        "    \n",
        "    return final_kernel\n",
        "\n",
        "def get_log_kernel(kernel_size: int = 5, sigma: float = None, dim: int = 2):\n",
        "    '''\n",
        "        Returns a Laplacian of Gaussian (LoG) kernel. If the kernel is known, use it,\n",
        "        else, generate a kernel with the parameters provided (slower)\n",
        "    '''\n",
        "    if kernel_size ==5 and not sigma and dim == 2: \n",
        "        return get_log_kernel_5x5()\n",
        "    else:\n",
        "        return get_log_kernel2d(kernel_size, sigma, dim)\n",
        "\n",
        "#TODO: use\n",
        "# Implementation of binarize operation (for edge detectors)\n",
        "def binarize(bin_img, threshold):\n",
        "  #bin_img = img > threshold\n",
        "  bin_img[bin_img < threshold] = 0.\n",
        "  return bin_img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_laplacian_kernel_3x3(alt=False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a laplacian kernel of 3x3\n",
        "            https://academic.mu.edu/phys/matthysd/web226/Lab02.htm\n",
        "            http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm\n",
        "        \n",
        "        This is called a negative Laplacian because the central peak is negative. \n",
        "        It is just as appropriate to reverse the signs of the elements, using \n",
        "        -1s and a +4, to get a positive Laplacian. It doesn't matter:\n",
        "        laplacian_kernel = torch.Tensor([\n",
        "                                    [0,  -1, 0],\n",
        "                                    [-1, 4, -1],\n",
        "                                    [0,  -1, 0]\n",
        "                                ])\n",
        "        Alternative Laplacian kernel as produced by Kornia (this is positive Laplacian,\n",
        "        like: https://kornia.readthedocs.io/en/latest/filters.html\n",
        "        laplacian_kernel = torch.Tensor([\n",
        "                                    [-1, -1, -1],\n",
        "                                    [-1,  8, -1],\n",
        "                                    [-1, -1, -1]\n",
        "                                ])\n",
        "    \"\"\"\n",
        "    if alt:\n",
        "        return torch.tensor([\n",
        "                    [-1, -1, -1],\n",
        "                    [-1,  8, -1],\n",
        "                    [-1, -1, -1]\n",
        "                ])\n",
        "    else:\n",
        "        return torch.tensor([\n",
        "                    [0, 1, 0],\n",
        "                    [1,-4, 1],\n",
        "                    [0, 1, 0],\n",
        "                ])\n",
        "\n",
        "def get_gradient_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a gradient kernel of 3x3\n",
        "            in x direction (transpose for y direction)\n",
        "            kernel_gradient_v = [[0, -1, 0], \n",
        "                                 [0, 0, 0], \n",
        "                                 [0, 1, 0]]\n",
        "            kernel_gradient_h = [[0, 0, 0], \n",
        "                                 [-1, 0, 1], \n",
        "                                 [0, 0, 0]]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [0, 0, 0], \n",
        "                   [-1, 0, 1], \n",
        "                   [0, 0, 0],\n",
        "            ])\n",
        "\n",
        "def get_scharr_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a scharr kernel of 3x3\n",
        "            in x direction (transpose for y direction)\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [-3, 0, 3],\n",
        "                   [-10,0,10],\n",
        "                   [-3, 0, 3],\n",
        "    ])\n",
        "\n",
        "def get_prewitt_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a prewitt kernel of 3x3\n",
        "            in x direction (transpose for y direction).\n",
        "        \n",
        "        Prewitt in x direction: This mask is called the \n",
        "            (vertical) Prewitt Edge Detector\n",
        "            prewitt_x= np.array([[-1, 0, 1],\n",
        "                                [-1, 0, 1],\n",
        "                                [-1, 0, 1]])\n",
        "        \n",
        "        Prewitt in y direction: This mask is called the \n",
        "            (horizontal) Prewitt Edge Detector\n",
        "            prewitt_y= np.array([[-1,-1,-1],\n",
        "                                 [0, 0, 0],\n",
        "                                 [1, 1, 1]])\n",
        "        Note that a Prewitt operator is a 1D box filter convolved with \n",
        "            a derivative operator \n",
        "            finite_diff = [-1, 0, 1]\n",
        "            simple_box = [1, 1, 1]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "    ])\n",
        "\n",
        "#https://github.com/kornia/kornia/blob/master/kornia/filters/kernels.py\n",
        "def get_sobel_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"Utility function that returns a sobel kernel of 3x3\n",
        "        sobel in x direction\n",
        "            sobel_x= np.array([[-1, 0, 1],\n",
        "                               [-2, 0, 2],\n",
        "                               [-1, 0, 1]])\n",
        "        sobel in y direction\n",
        "            sobel_y= np.array([[-1,-2,-1],\n",
        "                               [0, 0, 0],\n",
        "                               [1, 2, 1]])\n",
        "        \n",
        "        Note that a Sobel operator is a [1 2 1] filter convolved with \n",
        "            a derivative operator.\n",
        "            finite_diff = [1, 2, 1]\n",
        "            simple_box = [1, 1, 1]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "        [-1., 0., 1.],\n",
        "        [-2., 0., 2.],\n",
        "        [-1., 0., 1.],\n",
        "    ])\n",
        "\n",
        "#https://towardsdatascience.com/implement-canny-edge-detection-from-scratch-with-pytorch-a1cccfa58bed\n",
        "def get_sobel_kernel_2d(kernel_size=3):\n",
        "    # get range\n",
        "    range = torch.linspace(-(kernel_size // 2), kernel_size // 2, kernel_size)\n",
        "    # compute a grid the numerator and the axis-distances\n",
        "    y, x = torch.meshgrid(range, range)\n",
        "    #Note: x is edge detector in x, y is edge detector in y, if not dividing by den\n",
        "    den = (x ** 2 + y ** 2)\n",
        "    #den[:, kernel_size // 2] = 1  # avoid division by zero at the center of den\n",
        "    den[kernel_size // 2, kernel_size // 2] = 1  # avoid division by zero at the center of den\n",
        "    #sobel_2D = x / den #produces kernel in range (0,1)\n",
        "    sobel_2D = 2*x / den #produces same kernel as kornia\n",
        "    return sobel_2D\n",
        "\n",
        "def get_sobel_kernel(kernel_size=3):\n",
        "    '''\n",
        "    Sobel kernel\n",
        "        https://en.wikipedia.org/wiki/Sobel_operator\n",
        "    Note: using the Sobel filters needs two kernels, one in X axis and one in Y \n",
        "        axis (which is the transpose of X), to get the gradients in both directions.\n",
        "        The same kernel can be used in both cases.\n",
        "    '''\n",
        "    if kernel_size==3:\n",
        "        return get_sobel_kernel_3x3()\n",
        "    else:\n",
        "        return get_sobel_kernel_2d(kernel_size)\n",
        "\n",
        "\n",
        "\n",
        "#To apply the 1D filter in X and Y axis (For SSIM)\n",
        "#@torch.jit.script\n",
        "def apply_1Dfilter(input, win, use_padding: bool=False):  \n",
        "    r\"\"\" Apply 1-D kernel to input in X and Y axes.\n",
        "         Separable filters like the Gaussian blur can be applied to \n",
        "         a two-dimensional image as two independent one-dimensional \n",
        "         calculations, so a 2-dimensional convolution operation can \n",
        "         be separated into two 1-dimensional filters. This reduces \n",
        "         the cost of computing the operator.\n",
        "           https://en.wikipedia.org/wiki/Separable_filter\n",
        "    Args:\n",
        "        input (torch.Tensor): a batch of tensors to be filtered\n",
        "        window (torch.Tensor): 1-D gauss kernel\n",
        "        use_padding: padding image before conv\n",
        "    Returns:\n",
        "        torch.Tensor: filtered tensors\n",
        "    \"\"\"\n",
        "    #N, C, H, W = input.shape\n",
        "    C = input.shape[1]\n",
        "    \n",
        "    padding = 0\n",
        "    if use_padding:\n",
        "        window_size = win.shape[3]\n",
        "        padding = window_size // 2\n",
        "\n",
        "    #same 1D filter for both axes    \n",
        "    out = F.conv2d(input, win, stride=1, padding=(0, padding), groups=C)\n",
        "    out = F.conv2d(out, win.transpose(2, 3), stride=1, padding=(padding, 0), groups=C)\n",
        "    return out\n",
        "\n",
        "#convenient alias\n",
        "apply_gaussian_filter = apply_1Dfilter\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use this in the initialization of class FilterX, so it can be used on \n",
        "# forward with an image (LoG, Gaussian, etc)\n",
        "def load_filter(kernel, kernel_size=3, in_channels=3, out_channels=3, \n",
        "                stride=1, padding=True, groups=3, dim: int =2, \n",
        "                requires_grad=False):\n",
        "    '''\n",
        "        Loads a kernel's coefficients into a Conv layer that \n",
        "            can be used to convolve an image with, by default, \n",
        "            for depthwise convolution\n",
        "        Can use nn.Conv1d, nn.Conv2d or nn.Conv3d, depending on\n",
        "            the dimension set in dim (1,2,3)\n",
        "        #From Pytorch Conv2D:\n",
        "            https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv2d\n",
        "            When `groups == in_channels` and `out_channels == K * in_channels`,\n",
        "            where `K` is a positive integer, this operation is also termed in\n",
        "            literature as depthwise convolution.\n",
        "             At groups= :attr:`in_channels`, each input channel is convolved with\n",
        "             its own set of filters, of size:\n",
        "             :math:`\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor`.\n",
        "    '''\n",
        "\n",
        "    '''#TODO: check if this is necessary, probably not\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "    '''\n",
        "\n",
        "    # Reshape to 2d depthwise convolutional weight\n",
        "    kernel = kernel_conv_w(kernel, in_channels)\n",
        "    assert(len(kernel.shape)==4 and kernel.shape[0]==in_channels)\n",
        "\n",
        "    if padding:\n",
        "        pad = compute_padding(kernel_size)\n",
        "    else:\n",
        "        pad = 0\n",
        "    \n",
        "    # create filter as convolutional layer\n",
        "    if dim == 1:\n",
        "        conv = nn.Conv1d\n",
        "    elif dim == 2:\n",
        "        conv = nn.Conv2d\n",
        "    elif dim == 3:\n",
        "        conv = nn.Conv3d\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            'Only 1, 2 and 3 dimensions are supported for convolution. \\\n",
        "            Received {}.'.format(dim)\n",
        "        )\n",
        "\n",
        "    filter = conv(in_channels=in_channels, out_channels=out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride, padding=padding, \n",
        "                        groups=groups, bias=False)\n",
        "    filter.weight.data = kernel\n",
        "    filter.weight.requires_grad = requires_grad\n",
        "    return filter\n",
        "\n",
        "\n",
        "def compute_padding(kernel_size):\n",
        "    '''\n",
        "        Computes padding tuple. For square kernels, pad can be an\n",
        "         int, else, a tuple with an element for each dimension\n",
        "    '''\n",
        "    # 4 or 6 ints:  (padding_left, padding_right, padding_top, padding_bottom)\n",
        "    if isinstance(kernel_size, int):\n",
        "        return kernel_size//2\n",
        "    elif isinstance(kernel_size, list):\n",
        "        computed = [k // 2 for k in kernel_size]\n",
        "\n",
        "        out_padding = []\n",
        "\n",
        "        for i in range(len(kernel_size)):\n",
        "            computed_tmp = computed[-(i + 1)]\n",
        "            # for even kernels we need to do asymetric padding\n",
        "            if kernel_size[i] % 2 == 0:\n",
        "                padding = computed_tmp - 1\n",
        "            else:\n",
        "                padding = computed_tmp\n",
        "            out_padding.append(padding)\n",
        "            out_padding.append(computed_tmp)\n",
        "        return out_padding\n",
        "\n",
        "def normalize_kernel2d(input: torch.Tensor) -> torch.Tensor:\n",
        "    r\"\"\"Normalizes kernel.\n",
        "    \"\"\"\n",
        "    if len(input.size()) < 2:\n",
        "        raise TypeError(\"input should be at least 2D tensor. Got {}\"\n",
        "                        .format(input.size()))\n",
        "    norm: torch.Tensor = input.abs().sum(dim=-1).sum(dim=-1)\n",
        "    return input / (norm.unsqueeze(-1).unsqueeze(-1))\n",
        "\n",
        "def filter2D(input: torch.Tensor, kernel: torch.Tensor,\n",
        "             border_type: str = 'reflect', \n",
        "             dim: int =2,\n",
        "             normalized: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that convolves a tensor with a kernel.\n",
        "    The function applies a given kernel to a tensor. The kernel is applied\n",
        "    independently at each depth channel of the tensor. Before applying the\n",
        "    kernel, the function applies padding according to the specified mode so\n",
        "    that the output remains in the same shape.\n",
        "    Args:\n",
        "        input (torch.Tensor): the input tensor with shape of\n",
        "          :math:`(B, C, H, W)`.\n",
        "        kernel (torch.Tensor): the kernel to be convolved with the input\n",
        "          tensor. The kernel shape must be :math:`(1, kH, kW)`.\n",
        "        border_type (str): the padding mode to be applied before convolving.\n",
        "          The expected modes are: ``'constant'``, ``'reflect'``,\n",
        "          ``'replicate'`` or ``'circular'``. Default: ``'reflect'``.\n",
        "        normalized (bool): If True, kernel will be L1 normalized.\n",
        "    Return:\n",
        "        torch.Tensor: the convolved tensor of same size and numbers of channels\n",
        "        as the input.\n",
        "    \"\"\"\n",
        "    if not isinstance(input, torch.Tensor):\n",
        "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(input)))\n",
        "\n",
        "    if not isinstance(kernel, torch.Tensor):\n",
        "        raise TypeError(\"Input kernel type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(kernel)))\n",
        "\n",
        "    if not isinstance(border_type, str):\n",
        "        raise TypeError(\"Input border_type is not string. Got {}\"\n",
        "                        .format(type(kernel)))\n",
        "\n",
        "    #if not len(input.shape) == 4:\n",
        "        #raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\"\n",
        "                         #.format(input.shape))\n",
        "\n",
        "    #if not len(kernel.shape) == 3:\n",
        "        #raise ValueError(\"Invalid kernel shape, we expect 1xHxW. Got: {}\"\n",
        "                         #.format(kernel.shape))\n",
        "\n",
        "    borders_list: List[str] = ['constant', 'reflect', 'replicate', 'circular']\n",
        "    if border_type not in borders_list:\n",
        "        raise ValueError(\"Invalid border_type, we expect the following: {0}.\"\n",
        "                         \"Got: {1}\".format(borders_list, border_type))\n",
        "\n",
        "    # prepare kernel\n",
        "    b, c, h, w = input.shape\n",
        "    tmp_kernel: torch.Tensor = kernel.unsqueeze(0).to(input.device).to(input.dtype)\n",
        "    if normalized:\n",
        "        tmp_kernel = normalize_kernel2d(tmp_kernel) \n",
        "    # pad the input tensor\n",
        "    height, width = tmp_kernel.shape[-2:]\n",
        "    padding_shape: List[int] = compute_padding((height, width))\n",
        "    input_pad: torch.Tensor = F.pad(input, padding_shape, mode=border_type)\n",
        "    b, c, hp, wp = input_pad.shape\n",
        "\n",
        "    tmp_kernel = tmp_kernel.expand(c, -1, -1, -1)\n",
        "\n",
        "    # convolve the tensor with the kernel.\n",
        "    if dim == 1:\n",
        "        conv = F.conv1d\n",
        "    elif dim == 2:\n",
        "        conv = F.conv2d\n",
        "        #TODO: this needs a review, the final sizes don't match with .view(b, c, h, w), (they are larger).\n",
        "            # using .view(b, c, -1, w) results in an output, but it's 3 times larger than it should be\n",
        "        '''\n",
        "        # if kernel_numel > 81 this is a faster algo\n",
        "        kernel_numel: int = height * width #kernel_numel = torch.numel(tmp_kernel[-1:])\n",
        "        if kernel_numel > 81:\n",
        "            return conv(input_pad.reshape(b * c, 1, hp, wp), tmp_kernel, padding=0, stride=1).view(b, c, h, w)\n",
        "        '''\n",
        "    elif dim == 3:\n",
        "        conv = F.conv3d\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
        "        )\n",
        "\n",
        "    return conv(input_pad, tmp_kernel, groups=c, padding=0, stride=1)\n",
        "\n",
        "\n",
        "#TODO: make one class to receive any arbitrary kernel and others that are specific (like gaussian, etc)\n",
        "#class FilterX(nn.Module):\n",
        "  #def __init__(self, ..., kernel_type, dim: int=2):\n",
        "      #r\"\"\"\n",
        "      #Args:\n",
        "          #argument: ...\n",
        "      #\"\"\"\n",
        "      #super(filterXd, self).__init__()\n",
        "      #Here receive an pre-made kernel of any type, load as tensor or as\n",
        "      #convXd layer (class or functional)\n",
        "      # self.filter = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                #in_channels=image_channels, out_channels=image_channels, stride=stride, \n",
        "                #padding=pad, groups=image_channels)\n",
        "  #def forward:\n",
        "      #This would apply the filter that was initialized\n",
        "    \n",
        "\n",
        "\n",
        "class FilterLow(nn.Module):\n",
        "    def __init__(self, recursions=1, kernel_size=9, stride=1, padding=True, \n",
        "                image_channels=3, include_pad=True, filter_type=None):\n",
        "        super(FilterLow, self).__init__()\n",
        "        \n",
        "        if padding:\n",
        "            pad = compute_padding(kernel_size)\n",
        "        else:\n",
        "            pad = 0\n",
        "        \n",
        "        if filter_type == 'gaussian':\n",
        "            sigma = get_kernel_sigma(kernel_size)\n",
        "            kernel = get_gaussian_kernel2d(kernel_size=kernel_size, sigma=sigma)\n",
        "            self.filter = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                    in_channels=image_channels, stride=stride, padding=pad)\n",
        "        #elif filter_type == '': #TODO... box? (the same as average) What else?\n",
        "        else:\n",
        "            self.filter = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, \n",
        "                    padding=pad, count_include_pad=include_pad)\n",
        "        self.recursions = recursions\n",
        "\n",
        "    def forward(self, img):\n",
        "        for i in range(self.recursions):\n",
        "            img = self.filter(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class FilterHigh(nn.Module):\n",
        "    def __init__(self, recursions=1, kernel_size=9, stride=1, include_pad=True, \n",
        "            image_channels=3, normalize=True, filter_type=None, kernel=None):\n",
        "        super(FilterHigh, self).__init__()\n",
        "        \n",
        "        # if is standard freq. separator, will use the same LPF to remove LF from image\n",
        "        if filter_type=='gaussian' or filter_type=='average':\n",
        "            self.type = 'separator'\n",
        "            self.filter_low = FilterLow(recursions=1, kernel_size=kernel_size, stride=stride, \n",
        "                image_channels=image_channels, include_pad=include_pad, filter_type=filter_type)\n",
        "        # otherwise, can use any independent filter\n",
        "        else: #load any other filter for the high pass\n",
        "            self.type = 'independent'\n",
        "            #kernel and kernel_size should be provided. Options for edge detectors:\n",
        "            # In both dimensions: get_log_kernel, get_laplacian_kernel_3x3 \n",
        "            # and get_sobel_kernel\n",
        "            # Single dimension: get_prewitt_kernel_3x3, get_scharr_kernel_3x3 \n",
        "            # get_gradient_kernel_3x3 \n",
        "            if include_pad:\n",
        "                pad = compute_padding(kernel_size)\n",
        "            else:\n",
        "                pad = 0\n",
        "            self.filter_low = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                in_channels=image_channels, out_channels=image_channels, stride=stride, \n",
        "                padding=pad, groups=image_channels)\n",
        "        self.recursions = recursions\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, img):\n",
        "        if self.type == 'separator':\n",
        "            if self.recursions > 1:\n",
        "                for i in range(self.recursions - 1):\n",
        "                    img = self.filter_low(img)\n",
        "            img = img - self.filter_low(img)\n",
        "        elif self.type == 'independent':\n",
        "            img = self.filter_low(img)\n",
        "        if self.normalize:\n",
        "            return denorm(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "#TODO: check how similar getting the gradient with get_gradient_kernel_3x3 is from the alternative displacing the image\n",
        "#ref from TF: https://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/python/ops/image_ops_impl.py#L3423\n",
        "def get_image_gradients(image):\n",
        "    \"\"\"Returns image gradients (dy, dx) for each color channel.\n",
        "    Both output tensors have the same shape as the input: [b, c, h, w]. \n",
        "    Places the gradient [I(x+1,y) - I(x,y)] on the base pixel (x, y). \n",
        "    That means that dy will always have zeros in the last row,\n",
        "    and dx will always have zeros in the last column.\n",
        "    This can be used to implement the anisotropic 2-D version of the \n",
        "    Total Variation formula:\n",
        "        https://en.wikipedia.org/wiki/Total_variation_denoising\n",
        "    (anisotropic is using l1, isotropic is using l2 norm)\n",
        "    \n",
        "    Arguments:\n",
        "        image: Tensor with shape [b, c, h, w].\n",
        "    Returns:\n",
        "        Pair of tensors (dy, dx) holding the vertical and horizontal image\n",
        "        gradients (1-step finite difference).  \n",
        "    Raises:\n",
        "      ValueError: If `image` is not a 3D image or 4D tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_shape = image.shape\n",
        "      \n",
        "    if len(image_shape) == 3:\n",
        "        # The input is a single image with shape [height, width, channels].\n",
        "        # Calculate the difference of neighboring pixel-values.\n",
        "        # The images are shifted one pixel along the height and width by slicing.\n",
        "        dx = image[:, 1:, :] - image[:, :-1, :] #pixel_dif2, f_v_1-f_v_2\n",
        "        dy = image[1:, :, :] - image[:-1, :, :] #pixel_dif1, f_h_1-f_h_2\n",
        "\n",
        "    elif len(image_shape) == 4:    \n",
        "        # Return tensors with same size as original image\n",
        "        #adds one pixel pad to the right and removes one pixel from the left\n",
        "        right = F.pad(image, [0, 1, 0, 0])[..., :, 1:]\n",
        "        #adds one pixel pad to the bottom and removes one pixel from the top\n",
        "        bottom = F.pad(image, [0, 0, 0, 1])[..., 1:, :] \n",
        "\n",
        "        #right and bottom have the same dimensions as image\n",
        "        dx, dy = right - image, bottom - image \n",
        "        \n",
        "        #this is required because otherwise results in the last column and row having \n",
        "        # the original pixels from the image\n",
        "        dx[:, :, :, -1] = 0 # dx will always have zeros in the last column, right-left\n",
        "        dy[:, :, -1, :] = 0 # dy will always have zeros in the last row,    bottom-top\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'image_gradients expects a 3D [h, w, c] or 4D tensor '\n",
        "          '[batch_size, c, h, w], not %s.', image_shape)\n",
        "\n",
        "    return dy, dx\n",
        "\n",
        "\n",
        "def get_4dim_image_gradients(image):\n",
        "    # Return tensors with same size as original image\n",
        "    # Place the gradient [I(x+1,y) - I(x,y)] on the base pixel (x, y).\n",
        "    right = F.pad(image, [0, 1, 0, 0])[..., :, 1:] #adds one pixel pad to the right and removes one pixel from the left\n",
        "    bottom = F.pad(image, [0, 0, 0, 1])[..., 1:, :] #adds one pixel pad to the bottom and removes one pixel from the top\n",
        "    botright = F.pad(image, [0, 1, 0, 1])[..., 1:, 1:] #displaces in diagonal direction\n",
        "\n",
        "    dx, dy = right - image, bottom - image #right and bottom have the same dimensions as image\n",
        "    dn, dp = botright - image, right - bottom\n",
        "    #dp is positive diagonal (bottom left to top right)\n",
        "    #dn is negative diagonal (top left to bottom right)\n",
        "    \n",
        "    #this is required because otherwise results in the last column and row having \n",
        "    # the original pixels from the image\n",
        "    dx[:, :, :, -1] = 0 # dx will always have zeros in the last column, right-left\n",
        "    dy[:, :, -1, :] = 0 # dy will always have zeros in the last row,    bottom-top\n",
        "    dp[:, :, -1, :] = 0 # dp will always have zeros in the last row\n",
        "\n",
        "    return dy, dx, dp, dn\n",
        "\n",
        "#TODO: #https://towardsdatascience.com/implement-canny-edge-detection-from-scratch-with-pytorch-a1cccfa58bed\n",
        "#TODO: https://link.springer.com/article/10.1007/s11220-020-00281-8\n",
        "def grad_orientation(grad_y, grad_x):\n",
        "    go = torch.atan(grad_y / grad_x)\n",
        "    go = go * (360 / np.pi) + 180 # convert to degree\n",
        "    go = torch.round(go / 45) * 45  # keep a split by 45\n",
        "    return go\n",
        "################################################################################################################################################################################################\n",
        "# from dataops.colors import *\n",
        "# codes/dataops/colors.py\n",
        "\n",
        "'''\n",
        "Functions for color operations on tensors.\n",
        "If needed, there are more conversions that can be used:\n",
        "https://github.com/kornia/kornia/tree/master/kornia/color\n",
        "https://github.com/R08UST/Color_Conversion_pytorch/blob/master/differentiable_color_conversion/basic_op.py\n",
        "'''\n",
        "\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "def bgr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n",
        "    # flip image channels\n",
        "    out: torch.Tensor = image.flip(-3) #https://github.com/pytorch/pytorch/issues/229\n",
        "    #out: torch.Tensor = image[[2, 1, 0], :, :] #RGB to BGR #may be faster\n",
        "    return out\n",
        "\n",
        "def rgb_to_bgr(image: torch.Tensor) -> torch.Tensor:\n",
        "    #same operation as bgr_to_rgb(), flip image channels\n",
        "    return bgr_to_rgb(image)\n",
        "\n",
        "def bgra_to_rgba(image: torch.Tensor) -> torch.Tensor:\n",
        "    out: torch.Tensor = image[[2, 1, 0, 3], :, :]\n",
        "    return out\n",
        "\n",
        "def rgba_to_bgra(image: torch.Tensor) -> torch.Tensor:\n",
        "    #same operation as bgra_to_rgba(), flip image channels\n",
        "    return bgra_to_rgba(image)\n",
        "\n",
        "def rgb_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n",
        "    r, g, b = torch.chunk(input, chunks=3, dim=-3)\n",
        "    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    #gray = rgb_to_yuv(input,consts='y')\n",
        "    return gray\n",
        "\n",
        "def bgr_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n",
        "    input_rgb = bgr_to_rgb(input)\n",
        "    gray: torch.Tensor = rgb_to_grayscale(input_rgb)\n",
        "    #gray = rgb_to_yuv(input_rgb,consts='y')\n",
        "    return gray\n",
        "\n",
        "def grayscale_to_rgb(input: torch.Tensor) -> torch.Tensor:\n",
        "    #repeat the gray image to the three channels\n",
        "    rgb: torch.Tensor = input.repeat(3, *[1] * (input.dim() - 1))\n",
        "    return rgb\n",
        "\n",
        "def grayscale_to_bgr(input: torch.Tensor) -> torch.Tensor:\n",
        "    return grayscale_to_rgb(input)\n",
        "\n",
        "def rgb_to_ycbcr(input: torch.Tensor, consts='yuv'):\n",
        "    return rgb_to_yuv(input, consts == 'ycbcr')\n",
        "\n",
        "def rgb_to_yuv(input: torch.Tensor, consts='yuv'):\n",
        "    \"\"\"Converts one or more images from RGB to YUV.\n",
        "    Outputs a tensor of the same shape as the `input` image tensor, containing the YUV\n",
        "    value of the pixels.\n",
        "    The output is only well defined if the value in images are in [0,1].\n",
        "    YCbCr is often confused with the YUV color space, and typically the terms YCbCr \n",
        "    and YUV are used interchangeably, leading to some confusion. The main difference \n",
        "    is that YUV is analog and YCbCr is digital: https://en.wikipedia.org/wiki/YCbCr\n",
        "    Args:\n",
        "      input: 2-D or higher rank. Image data to convert. Last dimension must be\n",
        "        size 3. (Could add additional channels, ie, AlphaRGB = AlphaYUV)\n",
        "      consts: YUV constant parameters to use. BT.601 or BT.709. Could add YCbCr\n",
        "        https://en.wikipedia.org/wiki/YUV\n",
        "    Returns:\n",
        "      images: images tensor with the same shape as `input`.\n",
        "    \"\"\"\n",
        "    \n",
        "    #channels = input.shape[0]\n",
        "    \n",
        "    if consts == 'BT.709': # HDTV YUV\n",
        "        Wr = 0.2126\n",
        "        Wb = 0.0722\n",
        "        Wg = 1 - Wr - Wb #0.7152\n",
        "        Uc = 0.539\n",
        "        Vc = 0.635\n",
        "        delta: float = 0.5 #128 if image range in [0,255]\n",
        "    elif consts == 'ycbcr': # Alt. BT.601 from Kornia YCbCr values, from JPEG conversion\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Uc = 0.564 #(b-y) #cb\n",
        "        Vc = 0.713 #(r-y) #cr\n",
        "        delta: float = .5 #128 if image range in [0,255]\n",
        "    elif consts == 'yuvK': # Alt. yuv from Kornia YUV values: https://github.com/kornia/kornia/blob/master/kornia/color/yuv.py\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Ur = -0.147\n",
        "        Ug = -0.289\n",
        "        Ub = 0.436\n",
        "        Vr = 0.615\n",
        "        Vg = -0.515\n",
        "        Vb = -0.100\n",
        "        #delta: float = 0.0\n",
        "    elif consts == 'y': #returns only Y channel, same as rgb_to_grayscale()\n",
        "        #Note: torchvision uses ITU-R 601-2: Wr = 0.2989, Wg = 0.5870, Wb = 0.1140\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "    else: # Default to 'BT.601', SDTV YUV\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Uc = 0.493 #0.492\n",
        "        Vc = 0.877\n",
        "        delta: float = 0.5 #128 if image range in [0,255]\n",
        "\n",
        "    r: torch.Tensor = input[..., 0, :, :]\n",
        "    g: torch.Tensor = input[..., 1, :, :]\n",
        "    b: torch.Tensor = input[..., 2, :, :]\n",
        "    #TODO\n",
        "    #r, g, b = torch.chunk(input, chunks=3, dim=-3) #Alt. Which one is faster? Appear to be the same. Differentiable? Kornia uses both in different places\n",
        "\n",
        "    if consts == 'y':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        #(0.2989 * input[0] + 0.5870 * input[1] + 0.1140 * input[2]).to(img.dtype)\n",
        "        return y\n",
        "    elif consts == 'yuvK':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        u: torch.Tensor = Ur * r + Ug * g + Ub * b\n",
        "        v: torch.Tensor = Vr * r + Vg * g + Vb * b\n",
        "    else: #if consts == 'ycbcr' or consts == 'yuv' or consts == 'BT.709':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        u: torch.Tensor = (b - y) * Uc + delta #cb\n",
        "        v: torch.Tensor = (r - y) * Vc + delta #cr\n",
        "\n",
        "    if consts == 'uv': #returns only UV channels\n",
        "        return torch.stack((u, v), -2) \n",
        "    else:\n",
        "        return torch.stack((y, u, v), -3) \n",
        "\n",
        "def ycbcr_to_rgb(input: torch.Tensor):\n",
        "    return yuv_to_rgb(input, consts == 'ycbcr')\n",
        "\n",
        "def yuv_to_rgb(input: torch.Tensor, consts='yuv') -> torch.Tensor:\n",
        "    if consts == 'yuvK': # Alt. yuv from Kornia YUV values: https://github.com/kornia/kornia/blob/master/kornia/color/yuv.py\n",
        "        Wr = 1.14 #1.402\n",
        "        Wb = 2.029 #1.772\n",
        "        Wgu = 0.396 #.344136\n",
        "        Wgv = 0.581 #.714136\n",
        "        delta: float = 0.0\n",
        "    elif consts == 'yuv' or consts == 'ycbcr': # BT.601 from Kornia YCbCr values, from JPEG conversion\n",
        "        Wr = 1.403 #1.402\n",
        "        Wb = 1.773 #1.772\n",
        "        Wgu = .344 #.344136\n",
        "        Wgv = .714 #.714136\n",
        "        delta: float = .5 #128 if image range in [0,255]\n",
        "    \n",
        "    #Note: https://github.com/R08UST/Color_Conversion_pytorch/blob/75150c5fbfb283ae3adb85c565aab729105bbb66/differentiable_color_conversion/basic_op.py#L65 has u and v flipped\n",
        "    y: torch.Tensor = input[..., 0, :, :]\n",
        "    u: torch.Tensor = input[..., 1, :, :] #cb\n",
        "    v: torch.Tensor = input[..., 2, :, :] #cr\n",
        "    #TODO\n",
        "    #y, u, v = torch.chunk(input, chunks=3, dim=-3) #Alt. Which one is faster? Appear to be the same. Differentiable? Kornia uses both in different places\n",
        "\n",
        "    u_shifted: torch.Tensor = u - delta #cb\n",
        "    v_shifted: torch.Tensor = v - delta #cr\n",
        "\n",
        "    r: torch.Tensor = y + Wr * v_shifted\n",
        "    g: torch.Tensor = y - Wgv * v_shifted - Wgu * u_shifted\n",
        "    b: torch.Tensor = y + Wb * u_shifted\n",
        "    return torch.stack((r, g, b), -3) \n",
        "\n",
        "#Not tested:\n",
        "def rgb2srgb(imgs):\n",
        "    return torch.where(imgs<=0.04045,imgs/12.92,torch.pow((imgs+0.055)/1.055,2.4))\n",
        "\n",
        "#Not tested:\n",
        "def srgb2rgb(imgs):\n",
        "    return torch.where(imgs<=0.0031308,imgs*12.92,1.055*torch.pow((imgs),1/2.4)-0.055)\n",
        "################################################################################################################################################################################################\n",
        "# from dataops.common import norm, denorm\n",
        "# dnorm shon copy pasted\n",
        "\n",
        "def norm(x): \n",
        "    #Normalize (z-norm) from [0,1] range to [-1,1]\n",
        "    out = (x - 0.5) * 2.0\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return out.clamp(-1, 1)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        return np.clip(out, -1, 1)\n",
        "    else:\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor or \\\n",
        "        np.ndarray\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/discriminators.py\n",
        "\n",
        "class PixelDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        use_bias = False\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.net(input)\n",
        "\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    r\"\"\"\n",
        "    PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1611.07004v3.pdf\n",
        "    https://arxiv.org/pdf/1803.07422.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        #self.getIntermFeat = getIntermFeat # not used for now\n",
        "        #use_sigmoid not used for now\n",
        "        #TODO: test if there are benefits by incorporating the use of intermediate features from pix2pixHD\n",
        "\n",
        "        use_bias = False\n",
        "        kw = 4\n",
        "        padw = 1 # int(np.ceil((kw-1.0)/2))\n",
        "\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "# https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/loss.py\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = y.size()\n",
        "        loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
        "        return loss/(c*b*h*w)\n",
        "    \n",
        "\n",
        "# Define GAN loss: [vanilla | lsgan | wgan-gp | srpgan/nsgan | hinge]\n",
        "# https://tuatini.me/creating-and-shipping-deep-learning-models-into-production/\n",
        "class GANLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Adversarial loss\n",
        "    https://arxiv.org/abs/1711.10337\n",
        "    \"\"\"\n",
        "    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.gan_type = gan_type.lower()\n",
        "        self.real_label_val = real_label_val\n",
        "        self.fake_label_val = fake_label_val\n",
        "\n",
        "        if self.gan_type == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif self.gan_type == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif self.gan_type == 'srpgan' or self.gan_type == 'nsgan':\n",
        "            self.loss = nn.BCELoss()\n",
        "        elif self.gan_type == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        elif self.gan_type == 'wgan-gp':\n",
        "\n",
        "            def wgan_loss(input, target):\n",
        "                # target is boolean\n",
        "                return -1 * input.mean() if target else input.mean()\n",
        "\n",
        "            self.loss = wgan_loss\n",
        "        else:\n",
        "            raise NotImplementedError('GAN type [{:s}] is not found'.format(self.gan_type))\n",
        "\n",
        "    def get_target_label(self, input, target_is_real):\n",
        "        if self.gan_type == 'wgan-gp':\n",
        "            return target_is_real\n",
        "        if target_is_real:\n",
        "            return torch.empty_like(input).fill_(self.real_label_val) #torch.ones_like(d_sr_out)\n",
        "        else:\n",
        "            return torch.empty_like(input).fill_(self.fake_label_val) #torch.zeros_like(d_sr_out)\n",
        "\n",
        "    def forward(self, input, target_is_real, is_disc = None):\n",
        "        if self.gan_type == 'hinge': #TODO: test\n",
        "            if is_disc:\n",
        "                input = -input if target_is_real else input\n",
        "                return self.loss(1 + input).mean()\n",
        "            else:\n",
        "                return (-input).mean()\n",
        "        else:\n",
        "            target_label = self.get_target_label(input, target_is_real)\n",
        "            loss = self.loss(input, target_label)\n",
        "            return loss\n",
        "\n",
        "\n",
        "class GradientPenaltyLoss(nn.Module):\n",
        "    def __init__(self, device=torch.device('cpu')):\n",
        "        super(GradientPenaltyLoss, self).__init__()\n",
        "        self.register_buffer('grad_outputs', torch.Tensor())\n",
        "        self.grad_outputs = self.grad_outputs.to(device)\n",
        "\n",
        "    def get_grad_outputs(self, input):\n",
        "        if self.grad_outputs.size() != input.size():\n",
        "            self.grad_outputs.resize_(input.size()).fill_(1.0)\n",
        "        return self.grad_outputs\n",
        "\n",
        "    def forward(self, interp, interp_crit):\n",
        "        grad_outputs = self.get_grad_outputs(interp_crit)\n",
        "        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp, \\\n",
        "            grad_outputs=grad_outputs, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n",
        "        grad_interp_norm = grad_interp.norm(2, dim=1)\n",
        "\n",
        "        loss = ((grad_interp_norm - 1)**2).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class HFENLoss(nn.Module): # Edge loss with pre_smooth\n",
        "    \"\"\"Calculates high frequency error norm (HFEN) between target and \n",
        "     prediction used to quantify the quality of reconstruction of edges \n",
        "     and fine features. \n",
        "     \n",
        "     Uses a rotationally symmetric LoG (Laplacian of Gaussian) filter to \n",
        "     capture edges. The original filter kernel is of size 1515 pixels, \n",
        "     and has a standard deviation of 1.5 pixels.\n",
        "     ks = 2 * int(truncate * sigma + 0.5) + 1, so use truncate=4.5\n",
        "     \n",
        "     HFEN is computed as the norm of the result obtained by LoG filtering the \n",
        "     difference between the reconstructed and reference images.\n",
        "    [1]: Ravishankar and Bresler: MR Image Reconstruction From Highly\n",
        "    Undersampled k-Space Data by Dictionary Learning, 2011\n",
        "        https://ieeexplore.ieee.org/document/5617283\n",
        "    [2]: Han et al: Image Reconstruction Using Analysis Model Prior, 2016\n",
        "        https://www.hindawi.com/journals/cmmm/2016/7571934/\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    img1 : torch.Tensor or torch.autograd.Variable\n",
        "        Predicted image\n",
        "    img2 : torch.Tensor or torch.autograd.Variable\n",
        "        Target image\n",
        "    norm: if true, follows [2], who define a normalized version of HFEN.\n",
        "        If using RelativeL1 criterion, it's already normalized. \n",
        "    \"\"\"\n",
        "    def __init__(self, loss_f=None, kernel='log', kernel_size=15, sigma = 2.5, norm = False): #1.4 ~ 1.5\n",
        "        super(HFENLoss, self).__init__()\n",
        "        # can use different criteria\n",
        "        self.criterion = loss_f\n",
        "        self.norm = norm\n",
        "        #can use different kernels like DoG instead:\n",
        "        if kernel == 'dog':\n",
        "            kernel = get_dog_kernel(kernel_size, sigma)\n",
        "        else:\n",
        "            kernel = get_log_kernel(kernel_size, sigma)\n",
        "        self.filter = load_filter(kernel=kernel, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        self.filter.to(img1.device)\n",
        "        # HFEN loss\n",
        "        log1 = self.filter(img1)\n",
        "        log2 = self.filter(img2)\n",
        "        hfen_loss = self.criterion(log1, log2)\n",
        "        if self.norm:\n",
        "            hfen_loss /= img2.norm()\n",
        "        return hfen_loss\n",
        "\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_type='tv', p = 1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        assert p in [1, 2]\n",
        "        self.p = p\n",
        "        self.tv_type = tv_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        img_shape = x.shape\n",
        "        if len(img_shape) == 3 or len(img_shape) == 4:\n",
        "            if self.tv_type == 'dtv':\n",
        "                dy, dx, dp, dn  = get_4dim_image_gradients(x)\n",
        "\n",
        "                if len(dy.shape) == 3:\n",
        "                    # Sum for all axis. (None is an alias for all axis.)\n",
        "                    reduce_axes = None\n",
        "                    batch_size = 1\n",
        "                elif len(dy.shape) == 4:\n",
        "                    # Only sum for the last 3 axis.\n",
        "                    # This results in a 1-D tensor with the total variation for each image.\n",
        "                    reduce_axes = (-3, -2, -1)\n",
        "                    batch_size = x.size()[0]\n",
        "                #Compute the element-wise magnitude of a vector array\n",
        "                # Calculates the TV for each image in the batch\n",
        "                # Calculate the total variation by taking the absolute value of the\n",
        "                # pixel-differences and summing over the appropriate axis.\n",
        "                if self.p == 1:\n",
        "                    loss = (dy.abs().sum(dim=reduce_axes) + dx.abs().sum(dim=reduce_axes) + dp.abs().sum(dim=reduce_axes) + dn.abs().sum(dim=reduce_axes)) # Calculates the TV loss for each image in the batch\n",
        "                elif self.p == 2:\n",
        "                    loss = torch.pow(dy,2).sum(dim=reduce_axes) + torch.pow(dx,2).sum(dim=reduce_axes) + torch.pow(dp,2).sum(dim=reduce_axes) + torch.pow(dn,2).sum(dim=reduce_axes)\n",
        "                # calculate the scalar loss-value for tv loss\n",
        "                loss = loss.sum()/(2.0*batch_size) # averages the TV loss all the images in the batch (note: the division is not in TF version, only the sum reduction)\n",
        "                return loss\n",
        "            else: #'tv'\n",
        "                dy, dx  = get_image_gradients(x)\n",
        "\n",
        "                if len(dy.shape) == 3:\n",
        "                    # Sum for all axis. (None is an alias for all axis.)\n",
        "                    reduce_axes = None\n",
        "                    batch_size = 1\n",
        "                elif len(dy.shape) == 4:\n",
        "                    # Only sum for the last 3 axis.\n",
        "                    # This results in a 1-D tensor with the total variation for each image.\n",
        "                    reduce_axes = (-3, -2, -1)\n",
        "                    batch_size = x.size()[0]\n",
        "                #Compute the element-wise magnitude of a vector array\n",
        "                # Calculates the TV for each image in the batch\n",
        "                # Calculate the total variation by taking the absolute value of the\n",
        "                # pixel-differences and summing over the appropriate axis.\n",
        "                if self.p == 1:\n",
        "                    loss = dy.abs().sum(dim=reduce_axes) + dx.abs().sum(dim=reduce_axes)\n",
        "                elif self.p == 2:\n",
        "                    loss = torch.pow(dy,2).sum(dim=reduce_axes) + torch.pow(dx,2).sum(dim=reduce_axes)\n",
        "                # calculate the scalar loss-value for tv loss\n",
        "                loss = loss.sum()/batch_size # averages the TV loss all the images in the batch (note: the division is not in TF version, only the sum reduction)\n",
        "                return loss\n",
        "        else:\n",
        "            raise ValueError(\"Expected input tensor to be of ndim 3 or 4, but got \" + str(len(img_shape)))\n",
        "    \n",
        "\n",
        "class GradientLoss(nn.Module):\n",
        "    def __init__(self, loss_f = None, reduction='mean', gradientdir='2d'): #2d or 4d\n",
        "        super(GradientLoss, self).__init__()\n",
        "        self.criterion = loss_f\n",
        "        self.gradientdir = gradientdir\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        if self.gradientdir == '4d':\n",
        "            inputdy, inputdx, inputdp, inputdn = get_4dim_image_gradients(input)\n",
        "            targetdy, targetdx, targetdp, targetdn = get_4dim_image_gradients(target) \n",
        "            return (self.criterion(inputdx, targetdx) + self.criterion(inputdy, targetdy) + \\\n",
        "                    self.criterion(inputdp, targetdp) + self.criterion(inputdn, targetdn))/4\n",
        "        else: #'2d'\n",
        "            inputdy, inputdx = get_image_gradients(input)\n",
        "            targetdy, targetdx = get_image_gradients(target) \n",
        "            return (self.criterion(inputdx, targetdx) + self.criterion(inputdy, targetdy))/2\n",
        "\n",
        "\n",
        "class ElasticLoss(nn.Module):\n",
        "    def __init__(self, a=0.2, reduction='mean'): #a=0.5 default\n",
        "        super(ElasticLoss, self).__init__()\n",
        "        self.alpha = torch.FloatTensor([a, 1 - a]) #.to('cuda:0')\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if not isinstance(input, tuple):\n",
        "            input = (input,)\n",
        "\n",
        "        for i in range(len(input)):\n",
        "            l2 = F.mse_loss(input[i].squeeze(), target.squeeze(), reduction=self.reduction).mul(self.alpha[0])\n",
        "            l1 = F.l1_loss(input[i].squeeze(), target.squeeze(), reduction=self.reduction).mul(self.alpha[1])\n",
        "            loss = l1 + l2\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "#TODO: change to RelativeNorm and set criterion as an input argument, could be any basic criterion\n",
        "class RelativeL1(nn.Module):\n",
        "    '''\n",
        "    Comparing to the regular L1, introducing the division by |c|+epsilon \n",
        "    better models the human vision systems sensitivity to variations\n",
        "    in the dark areas. (where epsilon = 0.01, to prevent values of 0 in the\n",
        "    denominator)\n",
        "    '''\n",
        "    def __init__(self, eps=.01, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.criterion = torch.nn.L1Loss(reduction=reduction)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        base = target + self.eps\n",
        "        return self.criterion(input/base, target/base)\n",
        "\n",
        "\n",
        "class L1CosineSim(nn.Module):\n",
        "    '''\n",
        "    https://github.com/dmarnerides/hdr-expandnet/blob/master/train.py\n",
        "    Can be used to replace L1 pixel loss, but includes a cosine similarity term \n",
        "    to ensure color correctness of the RGB vectors of each pixel.\n",
        "    lambda is a constant factor that adjusts the contribution of the cosine similarity term\n",
        "    It provides improved color stability, especially for low luminance values, which\n",
        "    are frequent in HDR images, since slight variations in any of theRGB components of these \n",
        "    low values do not contribute much totheL1loss, but they may however cause noticeable \n",
        "    color shifts. More in the paper: https://arxiv.org/pdf/1803.02266.pdf\n",
        "    '''\n",
        "    def __init__(self, loss_lambda=5, reduction='mean'):\n",
        "        super(L1CosineSim, self).__init__()\n",
        "        self.similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-20)\n",
        "        self.l1_loss = nn.L1Loss(reduction=reduction)\n",
        "        self.loss_lambda = loss_lambda\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        cosine_term = (1 - self.similarity(x, y)).mean()\n",
        "        return self.l1_loss(x, y) + self.loss_lambda * cosine_term\n",
        "\n",
        "\n",
        "class ClipL1(nn.Module):\n",
        "    '''\n",
        "    Clip L1 loss\n",
        "    From: https://github.com/HolmesShuan/AIM2020-Real-Super-Resolution/\n",
        "    ClipL1 Loss combines Clip function and L1 loss. self.clip_min sets the \n",
        "    gradients of well-trained pixels to zeros and clip_max works as a noise filter.\n",
        "    data range [0, 255]: (clip_min=0.0, clip_max=10.0), \n",
        "    for [0,1] set clip_min to 1/255=0.003921.\n",
        "    '''\n",
        "    def __init__(self, clip_min=0.0, clip_max=10.0):\n",
        "        super(ClipL1, self).__init__()\n",
        "        self.clip_max = clip_max\n",
        "        self.clip_min = clip_min\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        loss = torch.mean(torch.clamp(torch.abs(sr-hr), self.clip_min, self.clip_max))\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Frequency loss \n",
        "# https://github.com/lj1995-computer-vision/Trident-Dehazing-Network/blob/master/loss/fft.py\n",
        "class FFTloss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean'):\n",
        "        super(FFTloss, self).__init__()\n",
        "        self.criterion = loss_f(reduction=reduction)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        zeros=torch.zeros(img1.size()).to(img1.device)\n",
        "        return self.criterion(torch.fft(torch.stack((img1,zeros),-1),2),torch.fft(torch.stack((img2,zeros),-1),2))\n",
        "\n",
        "\n",
        "class OFLoss(torch.nn.Module):\n",
        "    '''\n",
        "    Overflow loss\n",
        "    Only use if the image range is in [0,1]. (This solves the SPL brightness problem\n",
        "    and can be useful in other cases as well)\n",
        "    https://github.com/lj1995-computer-vision/Trident-Dehazing-Network/blob/master/loss/brelu.py\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(OFLoss, self).__init__()\n",
        "\n",
        "    def forward(self, img1):\n",
        "        img_clamp = img1.clamp(0,1)\n",
        "        b,c,h,w = img1.shape\n",
        "        return torch.log((img1 - img_clamp).abs() + 1).sum()/b/c/h/w\n",
        "\n",
        "\n",
        "#TODO: testing\n",
        "# Color loss \n",
        "class ColorLoss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean', ds_f=None):\n",
        "        super(ColorLoss, self).__init__()\n",
        "        self.ds_f = ds_f\n",
        "        self.criterion = loss_f\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_uv = rgb_to_yuv(self.ds_f(input), consts='uv')\n",
        "        target_uv = rgb_to_yuv(self.ds_f(target), consts='uv')\n",
        "        return self.criterion(input_uv, target_uv)\n",
        "\n",
        "#TODO: testing\n",
        "# Averaging Downscale loss \n",
        "class AverageLoss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean', ds_f=None):\n",
        "        super(AverageLoss, self).__init__()\n",
        "        self.ds_f = ds_f\n",
        "        self.criterion = loss_f\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_uv = rgb_to_yuv(self.ds_f(input), consts='uv')\n",
        "        target_uv = rgb_to_yuv(self.ds_f(target), consts='uv')\n",
        "        return self.criterion(input_uv, target_uv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# Spatial Profile Loss\n",
        "########################\n",
        "\n",
        "class GPLoss(nn.Module):\n",
        "    '''\n",
        "    https://github.com/ssarfraz/SPL/blob/master/SPL_Loss/\n",
        "    Gradient Profile (GP) loss\n",
        "    The image gradients in each channel can easily be computed \n",
        "    by simple 1-pixel shifted image differences from itself. \n",
        "    '''\n",
        "    def __init__(self, trace=False, spl_denorm=False):\n",
        "        super(GPLoss, self).__init__()\n",
        "        self.spl_denorm = spl_denorm\n",
        "        if trace == True: # Alternate behavior: use the complete calculation with SPL_ComputeWithTrace()\n",
        "            self.trace = SPL_ComputeWithTrace()\n",
        "        else: # Default behavior: use the more efficient SPLoss()\n",
        "            self.trace = SPLoss()\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        ## Use \"spl_denorm\" when reading a [-1,1] input, but you want to compute the loss over a [0,1] range\n",
        "        # Note: only rgb_to_yuv() requires image in the [0,1], so this denorm is optional, depending on the net\n",
        "        if self.spl_denorm == True:\n",
        "            input = denorm(input)\n",
        "            reference = denorm(reference)\n",
        "        input_h, input_v = get_image_gradients(input)\n",
        "        ref_h, ref_v = get_image_gradients(reference)\n",
        "\n",
        "        trace_v = self.trace(input_v,ref_v)\n",
        "        trace_h = self.trace(input_h,ref_h)\n",
        "        return trace_v + trace_h\n",
        "\n",
        "class CPLoss(nn.Module):\n",
        "    '''\n",
        "    Color Profile (CP) loss\n",
        "    '''\n",
        "    def __init__(self, rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False):\n",
        "        super(CPLoss, self).__init__()\n",
        "        self.rgb = rgb\n",
        "        self.yuv = yuv\n",
        "        self.yuvgrad = yuvgrad\n",
        "        self.spl_denorm = spl_denorm\n",
        "        self.yuv_denorm = yuv_denorm\n",
        "        \n",
        "        if trace == True: # Alternate behavior: use the complete calculation with SPL_ComputeWithTrace()\n",
        "            self.trace = SPL_ComputeWithTrace()\n",
        "            self.trace_YUV = SPL_ComputeWithTrace()\n",
        "        else: # Default behavior: use the more efficient SPLoss()\n",
        "            self.trace = SPLoss()\n",
        "            self.trace_YUV = SPLoss()\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        ## Use \"spl_denorm\" when reading a [-1,1] input, but you want to compute the loss over a [0,1] range\n",
        "        # self.spl_denorm=False when your inputs and outputs are in [0,1] range already\n",
        "        # Note: only rgb_to_yuv() requires image in the [0,1], so this denorm is optional, depending on the net\n",
        "        if self.spl_denorm:\n",
        "            input = denorm(input)\n",
        "            reference = denorm(reference)\n",
        "        total_loss= 0\n",
        "        if self.rgb:\n",
        "            total_loss += self.trace(input,reference)\n",
        "        if self.yuv:\n",
        "            # rgb_to_yuv() needs images in [0,1] range to work\n",
        "            if not self.spl_denorm and self.yuv_denorm:\n",
        "                input = denorm(input)\n",
        "                reference = denorm(reference)\n",
        "            input_yuv = rgb_to_yuv(input)\n",
        "            reference_yuv = rgb_to_yuv(reference)\n",
        "            total_loss += self.trace(input_yuv,reference_yuv)\n",
        "        if self.yuvgrad:\n",
        "            input_h, input_v = get_image_gradients(input_yuv)\n",
        "            ref_h, ref_v = get_image_gradients(reference_yuv)\n",
        "\n",
        "            total_loss +=  self.trace(input_v,ref_v)\n",
        "            total_loss +=  self.trace(input_h,ref_h)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "## Spatial Profile Loss (SPL) with trace\n",
        "class SPL_ComputeWithTrace(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial Profile Loss (SPL)\n",
        "    Both loss versions equate to the cosine similarity of rows/columns. \n",
        "    'SPL_ComputeWithTrace()' uses the trace (sum over the diagonal) of matrix multiplication \n",
        "    of L2-normalized input/target rows/columns.\n",
        "    Slow implementation of the trace loss using the same formula as stated in the paper. \n",
        "    In principle, we compute the loss between a source and target image by considering such \n",
        "    pattern differences along the image x and y-directions. Considering a row or a column \n",
        "    spatial profile of an image as a vector, we can compute the similarity between them in \n",
        "    this induced vector space. Formally, this similarity is measured over each image channel c.\n",
        "    The first term computes similarity among row profiles and the second among column profiles \n",
        "    of an image pair (x, y) of size H W. These image pixels profiles are L2-normalized to \n",
        "    have a normalized cosine similarity loss.\n",
        "    \"\"\"\n",
        "    def __init__(self,weight = [1.,1.,1.]): # The variable 'weight' was originally intended to weigh color channels differently. In our experiments, we found that an equal weight between all channels gives the best results. As such, this variable is a leftover from that time and can be removed.\n",
        "        super(SPL_ComputeWithTrace, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        a = 0\n",
        "        b = 0\n",
        "        for i in range(input.shape[0]):\n",
        "            for j in range(input.shape[1]):\n",
        "                a += torch.trace(torch.matmul(F.normalize(input[i,j,:,:],p=2,dim=1),torch.t(F.normalize(reference[i,j,:,:],p=2,dim=1))))/input.shape[2]*self.weight[j]\n",
        "                b += torch.trace(torch.matmul(torch.t(F.normalize(input[i,j,:,:],p=2,dim=0)),F.normalize(reference[i,j,:,:],p=2,dim=0)))/input.shape[3]*self.weight[j]\n",
        "        a = -torch.sum(a)/input.shape[0]\n",
        "        b = -torch.sum(b)/input.shape[0]\n",
        "        return a+b\n",
        "\n",
        "## Spatial Profile Loss (SPL) without trace, prefered\n",
        "class SPLoss(nn.Module):\n",
        "    ''' \n",
        "    Spatial Profile Loss (SPL)\n",
        "    'SPLoss()' L2-normalizes the rows/columns, performs piece-wise multiplication \n",
        "    of the two tensors and then sums along the corresponding axes. This variant \n",
        "    needs less operations since it can be performed batchwise.\n",
        "    Note: SPLoss() makes image results too bright, when using images in the [0,1] \n",
        "    range and no activation as output of the Generator.\n",
        "    SPL_ComputeWithTrace() does not have this problem, but results are very blurry. \n",
        "    Adding the Overflow Loss fixes this problem.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(SPLoss, self).__init__()\n",
        "        #self.weight = weight\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        a = torch.sum(torch.sum(F.normalize(input, p=2, dim=2) * F.normalize(reference, p=2, dim=2),dim=2, keepdim=True))\n",
        "        b = torch.sum(torch.sum(F.normalize(input, p=2, dim=3) * F.normalize(reference, p=2, dim=3),dim=3, keepdim=True))\n",
        "        return -(a + b) / (input.size(2) * input.size(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# Contextual Loss\n",
        "########################\n",
        "\n",
        "DIS_TYPES = ['cosine', 'l1', 'l2']\n",
        "\n",
        "class Contextual_Loss(nn.Module):\n",
        "    '''\n",
        "    Contextual loss for unaligned images (https://arxiv.org/abs/1803.02077)\n",
        "    https://github.com/roimehrez/contextualLoss\n",
        "    https://github.com/S-aiueo32/contextual_loss_pytorch\n",
        "    https://github.com/z-bingo/Contextual-Loss-PyTorch\n",
        "    layers_weights: is a dict, e.g., {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    crop_quarter: boolean\n",
        "    '''\n",
        "    def __init__(self, layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type: str = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg: bool = True, net: str = 'vgg19', calc_type: str =  'regular'):\n",
        "        super(Contextual_Loss, self).__init__()\n",
        "\n",
        "        assert band_width > 0, 'band_width parameter must be positive.'\n",
        "        assert distance_type in DIS_TYPES,\\\n",
        "            f'select a distance type from {DIS_TYPES}.'\n",
        "\n",
        "        listen_list = []\n",
        "        self.layers_weights = {}\n",
        "        try:\n",
        "            listen_list = layers_weights.keys()\n",
        "            self.layers_weights = layers_weights\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.crop_quarter = crop_quarter\n",
        "        self.distanceType = distance_type\n",
        "        self.max_1d_size = max_1d_size\n",
        "        self.b = b\n",
        "        self.band_width = band_width #self.h = h, #sigma\n",
        "        \n",
        "        if use_vgg:\n",
        "            self.vgg_model = VGG_Model(listen_list=listen_list, net=net)\n",
        "\n",
        "        if calc_type == 'bilateral':\n",
        "            self.calculate_loss = self.bilateral_CX_Loss\n",
        "        elif calc_type == 'symetric':\n",
        "            self.calculate_loss = self.symetric_CX_Loss\n",
        "        else: #if calc_type == 'regular':\n",
        "            self.calculate_loss = self.calculate_CX_Loss\n",
        "\n",
        "    def forward(self, images, gt):\n",
        "        device = images.device\n",
        "        \n",
        "        if hasattr(self, 'vgg_model'):\n",
        "            assert images.shape[1] == 3 and gt.shape[1] == 3,\\\n",
        "                'VGG model takes 3 channel images.'\n",
        "            \n",
        "            loss = 0\n",
        "            vgg_images = self.vgg_model(images)\n",
        "            vgg_images = {k: v.clone().to(device) for k, v in vgg_images.items()}\n",
        "            vgg_gt = self.vgg_model(gt)\n",
        "            vgg_gt = {k: v.to(device) for k, v in vgg_gt.items()}\n",
        "\n",
        "            for key in self.layers_weights.keys():\n",
        "                if self.crop_quarter:\n",
        "                    vgg_images[key] = self._crop_quarters(vgg_images[key])\n",
        "                    vgg_gt[key] = self._crop_quarters(vgg_gt[key])\n",
        "\n",
        "                N, C, H, W = vgg_images[key].size()\n",
        "                if H*W > self.max_1d_size**2:\n",
        "                    vgg_images[key] = self._random_pooling(vgg_images[key], output_1d_size=self.max_1d_size)\n",
        "                    vgg_gt[key] = self._random_pooling(vgg_gt[key], output_1d_size=self.max_1d_size)\n",
        "\n",
        "                loss_t = self.calculate_loss(vgg_images[key], vgg_gt[key])\n",
        "                loss += loss_t * self.layers_weights[key]\n",
        "                # del vgg_images[key], vgg_gt[key]\n",
        "        #TODO: without VGG it runs, but results are not looking right\n",
        "        else:\n",
        "            if self.crop_quarter:\n",
        "                images = self._crop_quarters(images)\n",
        "                gt = self._crop_quarters(gt)\n",
        "\n",
        "            N, C, H, W = images.size()\n",
        "            if H*W > self.max_1d_size**2:\n",
        "                images = self._random_pooling(images, output_1d_size=self.max_1d_size)\n",
        "                gt = self._random_pooling(gt, output_1d_size=self.max_1d_size)\n",
        "\n",
        "            loss = self.calculate_loss(images, gt)\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_sampling(tensor, n, indices):\n",
        "        N, C, H, W = tensor.size()\n",
        "        S = H * W\n",
        "        tensor = tensor.view(N, C, S)\n",
        "        device=tensor.device\n",
        "        if indices is None:\n",
        "            indices = torch.randperm(S)[:n].contiguous().type_as(tensor).long()\n",
        "            indices = indices.clamp(indices.min(), tensor.shape[-1]-1) #max = indices.max()-1\n",
        "            indices = indices.view(1, 1, -1).expand(N, C, -1)\n",
        "        indices = indices.to(device)\n",
        "\n",
        "        res = torch.gather(tensor, index=indices, dim=-1)\n",
        "        return res, indices\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_pooling(feats, output_1d_size=100):\n",
        "        single_input = type(feats) is torch.Tensor\n",
        "\n",
        "        if single_input:\n",
        "            feats = [feats]\n",
        "\n",
        "        N, C, H, W = feats[0].size()\n",
        "        feats_sample, indices = Contextual_Loss._random_sampling(feats[0], output_1d_size**2, None)\n",
        "        res = [feats_sample]\n",
        "\n",
        "        for i in range(1, len(feats)):\n",
        "            feats_sample, _ = Contextual_Loss._random_sampling(feats[i], -1, indices)\n",
        "            res.append(feats_sample)\n",
        "\n",
        "        res = [feats_sample.view(N, C, output_1d_size, output_1d_size) for feats_sample in res]\n",
        "\n",
        "        if single_input:\n",
        "            return res[0]\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def _crop_quarters(feature_tensor):\n",
        "        N, fC, fH, fW = feature_tensor.size()\n",
        "        quarters_list = []\n",
        "        quarters_list.append(feature_tensor[..., 0:round(fH / 2), 0:round(fW / 2)])\n",
        "        quarters_list.append(feature_tensor[..., 0:round(fH / 2), round(fW / 2):])\n",
        "        quarters_list.append(feature_tensor[..., round(fH / 2):, 0:round(fW / 2)])\n",
        "        quarters_list.append(feature_tensor[..., round(fH / 2):, round(fW / 2):])\n",
        "\n",
        "        feature_tensor = torch.cat(quarters_list, dim=0)\n",
        "        return feature_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_L2(I_features, T_features):\n",
        "        \"\"\"\n",
        "        Calculating the distance between each feature of I and T\n",
        "        :param I_features:\n",
        "        :param T_features:\n",
        "        :return: raw_distance: [N, C, H, W, H*W], each element of which is the distance between I and T at each position\n",
        "        \"\"\"\n",
        "        assert I_features.size() == T_features.size()\n",
        "        N, C, H, W = I_features.size()\n",
        "\n",
        "        Ivecs = I_features.view(N, C, -1)\n",
        "        Tvecs = T_features.view(N, C, -1)\n",
        "        #\n",
        "        square_I = torch.sum(Ivecs*Ivecs, dim=1, keepdim=False)\n",
        "        square_T = torch.sum(Tvecs*Tvecs, dim=1, keepdim=False)\n",
        "        # raw_distance\n",
        "        raw_distance = []\n",
        "        for i in range(N):\n",
        "            Ivec, Tvec, s_I, s_T = Ivecs[i, ...], Tvecs[i, ...], square_I[i, ...], square_T[i, ...]\n",
        "            # matrix multiplication\n",
        "            AB = Ivec.permute(1, 0) @ Tvec\n",
        "            dist = s_I.view(-1, 1) + s_T.view(1, -1) - 2*AB\n",
        "            raw_distance.append(dist.view(1, H, W, H*W))\n",
        "        raw_distance = torch.cat(raw_distance, dim=0)\n",
        "        raw_distance = torch.clamp(raw_distance, 0.0)\n",
        "        return raw_distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_L1(I_features, T_features):\n",
        "        assert I_features.size() == T_features.size()\n",
        "        N, C, H, W = I_features.size()\n",
        "\n",
        "        Ivecs = I_features.view(N, C, -1)\n",
        "        Tvecs = T_features.view(N, C, -1)\n",
        "\n",
        "        raw_distance = []\n",
        "        for i in range(N):\n",
        "            Ivec, Tvec = Ivecs[i, ...], Tvecs[i, ...]\n",
        "            dist = torch.sum(\n",
        "                torch.abs(Ivec.view(C, -1, 1) - Tvec.view(C, 1, -1)), dim=0, keepdim=False\n",
        "            )\n",
        "            raw_distance.append(dist.view(1, H, W, H*W))\n",
        "        raw_distance = torch.cat(raw_distance, dim=0)\n",
        "        return raw_distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_dotP(I_features, T_features):\n",
        "        assert I_features.size() == T_features.size()\n",
        "        # prepare feature before calculating cosine distance\n",
        "        # mean shifting by channel-wise mean of `y`.\n",
        "        mean_T = T_features.mean(dim=(0, 2, 3), keepdim=True)        \n",
        "        I_features = I_features - mean_T\n",
        "        T_features = T_features - mean_T\n",
        "\n",
        "        # L2 channelwise normalization\n",
        "        I_features = F.normalize(I_features, p=2, dim=1)\n",
        "        T_features = F.normalize(T_features, p=2, dim=1)\n",
        "        \n",
        "        N, C, H, W = I_features.size()\n",
        "        cosine_dist = []\n",
        "        # work seperatly for each example in dim 1\n",
        "        for i in range(N):\n",
        "            # channel-wise vectorization\n",
        "            T_features_i = T_features[i].view(1, 1, C, H*W).permute(3, 2, 0, 1).contiguous() # 1CHW --> 11CP, with P=H*W\n",
        "            I_features_i = I_features[i].unsqueeze(0)\n",
        "            dist = F.conv2d(I_features_i, T_features_i).permute(0, 2, 3, 1).contiguous()\n",
        "            #cosine_dist.append(dist) # back to 1CHW\n",
        "            #TODO: temporary hack to workaround AMP bug:\n",
        "            cosine_dist.append(dist.to(torch.float32)) # back to 1CHW\n",
        "        cosine_dist = torch.cat(cosine_dist, dim=0)\n",
        "        cosine_dist = (1 - cosine_dist) / 2\n",
        "        cosine_dist = cosine_dist.clamp(min=0.0)\n",
        "\n",
        "        return cosine_dist\n",
        "\n",
        "    #compute_relative_distance\n",
        "    @staticmethod\n",
        "    def _calculate_relative_distance(raw_distance, epsilon=1e-5):\n",
        "        \"\"\"\n",
        "        Normalizing the distances first as Eq. (2) in paper\n",
        "        :param raw_distance:\n",
        "        :param epsilon:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        div = torch.min(raw_distance, dim=-1, keepdim=True)[0]\n",
        "        relative_dist = raw_distance / (div + epsilon) # Eq 2\n",
        "        return relative_dist\n",
        "\n",
        "    def symetric_CX_Loss(self, I_features, T_features):\n",
        "        loss = (self.calculate_CX_Loss(T_features, I_features) + self.calculate_CX_Loss(I_features, T_features)) / 2\n",
        "        return loss #score\n",
        "\n",
        "    def bilateral_CX_Loss(self, I_features, T_features, weight_sp: float = 0.1):\n",
        "        def compute_meshgrid(shape):\n",
        "            N, C, H, W = shape\n",
        "            rows = torch.arange(0, H, dtype=torch.float32) / (H + 1)\n",
        "            cols = torch.arange(0, W, dtype=torch.float32) / (W + 1)\n",
        "\n",
        "            feature_grid = torch.meshgrid(rows, cols)\n",
        "            feature_grid = torch.stack(feature_grid).unsqueeze(0)\n",
        "            feature_grid = torch.cat([feature_grid for _ in range(N)], dim=0)\n",
        "\n",
        "            return feature_grid\n",
        "\n",
        "        # spatial loss\n",
        "        grid = compute_meshgrid(I_features.shape).to(T_features.device)\n",
        "        raw_distance = Contextual_Loss._create_using_L2(grid, grid) # calculate raw distance\n",
        "        dist_tilde = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        exp_distance = torch.exp((self.b - dist_tilde) / self.band_width) # Eq(3)\n",
        "        cx_sp = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "\n",
        "        # feature loss\n",
        "        # calculate raw distances\n",
        "        if self.distanceType == 'l1':\n",
        "            raw_distance = Contextual_Loss._create_using_L1(I_features, T_features)\n",
        "        elif self.distanceType == 'l2':\n",
        "            raw_distance = Contextual_Loss._create_using_L2(I_features, T_features)\n",
        "        else: # self.distanceType == 'cosine':\n",
        "            raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)\n",
        "        dist_tilde = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        exp_distance = torch.exp((self.b - dist_tilde) / self.band_width) # Eq(3)\n",
        "        cx_feat = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "\n",
        "        # combined loss\n",
        "        cx_combine = (1. - weight_sp) * cx_feat + weight_sp * cx_sp\n",
        "        k_max_NC, _ = torch.max(cx_combine, dim=2, keepdim=True)\n",
        "        cx = k_max_NC.mean(dim=1)\n",
        "        cx_loss = torch.mean(-torch.log(cx + 1e-5))\n",
        "        return cx_loss\n",
        "\n",
        "    def calculate_CX_Loss(self, I_features, T_features):\n",
        "        device = I_features.device\n",
        "        T_features = T_features.to(device)\n",
        "\n",
        "        if torch.sum(torch.isnan(I_features)) == torch.numel(I_features) or torch.sum(torch.isinf(I_features)) == torch.numel(I_features):\n",
        "            print(I_features)\n",
        "            raise ValueError('NaN or Inf in I_features')\n",
        "        if torch.sum(torch.isnan(T_features)) == torch.numel(T_features) or torch.sum(\n",
        "                torch.isinf(T_features)) == torch.numel(T_features):\n",
        "            print(T_features)\n",
        "            raise ValueError('NaN or Inf in T_features')\n",
        "\n",
        "        # calculate raw distances\n",
        "        if self.distanceType == 'l1':\n",
        "            raw_distance = Contextual_Loss._create_using_L1(I_features, T_features)\n",
        "        elif self.distanceType == 'l2':\n",
        "            raw_distance = Contextual_Loss._create_using_L2(I_features, T_features)\n",
        "        else: # self.distanceType == 'cosine':\n",
        "            raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)\n",
        "        if torch.sum(torch.isnan(raw_distance)) == torch.numel(raw_distance) or torch.sum(\n",
        "                torch.isinf(raw_distance)) == torch.numel(raw_distance):\n",
        "            print(raw_distance)\n",
        "            raise ValueError('NaN or Inf in raw_distance')\n",
        "\n",
        "        # normalizing the distances\n",
        "        relative_distance = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        if torch.sum(torch.isnan(relative_distance)) == torch.numel(relative_distance) or torch.sum(\n",
        "                torch.isinf(relative_distance)) == torch.numel(relative_distance):\n",
        "            print(relative_distance)\n",
        "            raise ValueError('NaN or Inf in relative_distance')\n",
        "        del raw_distance\n",
        "\n",
        "        #compute_sim()\n",
        "        # where h>0 is a band-width parameter\n",
        "        exp_distance = torch.exp((self.b - relative_distance) / self.band_width) # Eq(3)\n",
        "        if torch.sum(torch.isnan(exp_distance)) == torch.numel(exp_distance) or torch.sum(\n",
        "                torch.isinf(exp_distance)) == torch.numel(exp_distance):\n",
        "            print(exp_distance)\n",
        "            raise ValueError('NaN or Inf in exp_distance')\n",
        "        del relative_distance\n",
        "        \n",
        "        # Similarity\n",
        "        contextual_sim = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "        if torch.sum(torch.isnan(contextual_sim)) == torch.numel(contextual_sim) or torch.sum(\n",
        "                torch.isinf(contextual_sim)) == torch.numel(contextual_sim):\n",
        "            print(contextual_sim)\n",
        "            raise ValueError('NaN or Inf in contextual_sim')\n",
        "        del exp_distance\n",
        "        \n",
        "        #contextual_loss()\n",
        "        max_gt_sim = torch.max(torch.max(contextual_sim, dim=1)[0], dim=1)[0] # Eq(1)\n",
        "        del contextual_sim\n",
        "        CS = torch.mean(max_gt_sim, dim=1)\n",
        "        CX_loss = torch.mean(-torch.log(CS)) # Eq(5)\n",
        "        if torch.isnan(CX_loss):\n",
        "            raise ValueError('NaN in computing CX_loss')\n",
        "        return CX_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InpaintingLoss(nn.Module):\n",
        "    def __init__(self, p=[0, 1, 2], q=[0, 1, 2],\n",
        "                 w=[6., 0.1, 240., 0.1]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.l1 = nn.L1Loss()\n",
        "        self.content = PerceptualLoss()\n",
        "        self.style = StyleLoss()\n",
        "\n",
        "        # new loss\n",
        "        \"\"\"\n",
        "        if self.config.HFEN_TYPE == 'L1':\n",
        "          l_hfen_type = nn.L1Loss()\n",
        "        if self.config.HFEN_TYPE == 'MSE': \n",
        "          l_hfen_type = nn.MSELoss()\n",
        "        if self.config.HFEN_TYPE == 'Charbonnier':\n",
        "          l_hfen_type = CharbonnierLoss()\n",
        "        if self.config.HFEN_TYPE == 'ElasticLoss':\n",
        "          l_hfen_type = ElasticLoss()\n",
        "        if self.config.HFEN_TYPE == 'RelativeL1':\n",
        "          l_hfen_type = RelativeL1()        \n",
        "        if self.config.HFEN_TYPE == 'L1CosineSim':\n",
        "          l_hfen_type = L1CosineSim()\n",
        "        \"\"\"\n",
        "\n",
        "        l_hfen_type = L1CosineSim()\n",
        "        self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "\n",
        "        self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "\n",
        "        self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "\n",
        "        self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "\n",
        "        self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "\n",
        "        self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "\n",
        "        self.OFLoss = OFLoss()\n",
        "\n",
        "        self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "\n",
        "        self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, input, gt):\n",
        "\n",
        "        # just one loop\n",
        "        total_loss = 0.0\n",
        "        #loss_text = 0.0\n",
        "\n",
        "        for i in self.p:\n",
        "          out = input[i]\n",
        "          gt_res = resize_like(gt, out)\n",
        "\n",
        "          (b, ch, h, w) = out.size()\n",
        "          loss_rec = self.l1(out, gt_res) / (ch * h * w)\n",
        "\n",
        "          total_loss += (self.w[0] * loss_rec)\n",
        "\n",
        "          #for i in self.q:\n",
        "          #out = input[i]\n",
        "          #gt_res = resize_like(gt, out)\n",
        "\n",
        "          total_loss += self.content(out, gt_res) # loss_PerceptualLoss\n",
        "\n",
        "          total_loss += self.style(out, gt_res)\n",
        "\n",
        "          total_loss += total_variation_loss(out) #tv\n",
        "\n",
        "          # new loss\n",
        "\n",
        "          total_loss += self.HFENLoss(out, gt_res)\n",
        "\n",
        "          total_loss += self.ElasticLoss(out, gt_res)\n",
        "\n",
        "          total_loss += self.RelativeL1(out, gt_res)\n",
        "\n",
        "          total_loss += self.L1CosineSim(out, gt_res)\n",
        "\n",
        "          total_loss += self.ClipL1(out, gt_res)\n",
        "\n",
        "          total_loss += self.FFTloss(out, gt_res)\n",
        "\n",
        "          total_loss += self.OFLoss(out)\n",
        "\n",
        "          total_loss += self.GPLoss(out, gt_res)\n",
        "\n",
        "          total_loss += self.CPLoss(out, gt_res)\n",
        "\n",
        "          #total_loss += self.Contextual_Loss(out, gt_res)\n",
        "\n",
        "          #total_loss += loss_rec + loss_PerceptualLoss + loss_style\n",
        "        #loss_text += (self.w[1] * loss_prc) + (self.w[2] * loss_style) + (self.w[3] * loss_tv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        # two loops, like the original code here: https://github.com/Yukariin/DFNet/blob/master/loss.py\n",
        "        total_loss = 0.0\n",
        "        for i in self.p:\n",
        "            out = input[i]\n",
        "            gt_res = resize_like(gt, out)\n",
        "\n",
        "            (b, ch, h, w) = out.size()\n",
        "            #loss_rec = self.l1(out, gt_res) / (ch * h * w)\n",
        "            #total_loss += (self.w[0] * loss_rec)\n",
        "\n",
        "            total_loss += self.L1CosineSim(out, gt_res)\n",
        "\n",
        "            #total_loss += self.RelativeL1(out, gt_res)\n",
        "\n",
        "            #total_loss += self.ClipL1(out, gt_res)\n",
        "\n",
        "        \n",
        "        for i in self.q:\n",
        "            out = input[i]\n",
        "            gt_res = resize_like(gt, out)\n",
        "\n",
        "            total_loss += self.content(out, gt_res) # loss_PerceptualLoss\n",
        "\n",
        "            total_loss += self.style(out, gt_res)\n",
        "\n",
        "            total_loss += total_variation_loss(out) #tv\n",
        "\n",
        "            # new loss\n",
        "\n",
        "            total_loss += self.HFENLoss(out, gt_res)\n",
        "\n",
        "            total_loss += self.ElasticLoss(out, gt_res)\n",
        "\n",
        "            #total_loss += self.RelativeL1(out, gt_res)\n",
        "\n",
        "            #total_loss += self.L1CosineSim(out, gt_res)\n",
        "\n",
        "            #total_loss += self.ClipL1(out, gt_res)\n",
        "\n",
        "            total_loss += self.FFTloss(out, gt_res)\n",
        "\n",
        "            total_loss += self.OFLoss(out)\n",
        "\n",
        "            total_loss += self.GPLoss(out, gt_res)\n",
        "\n",
        "            total_loss += self.CPLoss(out, gt_res)\n",
        "        \"\"\"\n",
        "\n",
        "        #return loss_struct + loss_text\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1IVD-B4iKdb",
        "cellView": "form"
      },
      "source": [
        "#@title differentiable augmentation in train.py + amp (untested) + new loss\n",
        "%%writefile /content/DFNet/train.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data import DS\n",
        "from loss import InpaintingLoss\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class InfiniteSampler(data.sampler.Sampler):\n",
        "    def __init__(self, num_samples):\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.loop())\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "    def loop(self):\n",
        "        i = 0\n",
        "        order = np.random.permutation(self.num_samples)\n",
        "        while True:\n",
        "            yield order[i]\n",
        "            i += 1\n",
        "            if i >= self.num_samples:\n",
        "                np.random.seed()\n",
        "                order = np.random.permutation(self.num_samples)\n",
        "                i = 0\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, default='/srv/datasets/Places2')\n",
        "parser.add_argument('--save_dir', type=str, default='./snapshots/default')\n",
        "parser.add_argument('--log_dir', type=str, default='./logs/default')\n",
        "parser.add_argument('--lr', type=float, default=2e-3)\n",
        "parser.add_argument('--max_iter', type=int, default=200000)\n",
        "parser.add_argument('--batch_size', type=int, default=6)\n",
        "parser.add_argument('--n_threads', type=int, default=16)\n",
        "parser.add_argument('--save_model_interval', type=int, default=1000)\n",
        "parser.add_argument('--vis_interval', type=int, default=1000)\n",
        "parser.add_argument('--log_interval', type=int, default=10)\n",
        "parser.add_argument('--image_size', type=int, default=256)\n",
        "parser.add_argument('--resume', type=str)\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs('{:s}/images'.format(args.save_dir))\n",
        "    os.makedirs('{:s}/ckpt'.format(args.save_dir))\n",
        "\n",
        "writer = SummaryWriter(logdir=args.log_dir)\n",
        "\n",
        "size = (args.image_size, args.image_size)\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize(size=size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = DS(args.root, img_tf)\n",
        "\n",
        "iterator_train = iter(data.DataLoader(\n",
        "    dataset, batch_size=args.batch_size,\n",
        "    sampler=InfiniteSampler(len(dataset)),\n",
        "    num_workers=args.n_threads\n",
        "))\n",
        "print(len(dataset))\n",
        "model = DFNet().to(device)\n",
        "\n",
        "lr = args.lr\n",
        "\n",
        "start_iter = 0\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "criterion = InpaintingLoss().to(device)\n",
        "\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "for i in tqdm(range(start_iter, args.max_iter)):\n",
        "    model.train()\n",
        "    \n",
        "    img, mask = [x.to(device) for x in next(iterator_train)]\n",
        "    masked = img * mask\n",
        "\n",
        "    results, alpha, raw = model(masked, mask)\n",
        "\n",
        "    # Diffaugment\n",
        "    img0 = DiffAugment(img[0], policy=policy)\n",
        "    img1 = DiffAugment(img[1], policy=policy)\n",
        "    #img2 = DiffAugment(img[2], policy=policy)\n",
        "    #img3 = DiffAugment(img[3], policy=policy)\n",
        "    #img4 = DiffAugment(img[4], policy=policy)\n",
        "    #img5 = DiffAugment(img[5], policy=policy)\n",
        "    \"\"\"\n",
        "    results0 = DiffAugment(results[0], policy=policy)\n",
        "    results1 = DiffAugment(results[1], policy=policy)\n",
        "    results2 = DiffAugment(results[2], policy=policy)\n",
        "    results3 = DiffAugment(results[3], policy=policy)\n",
        "    results4 = DiffAugment(results[4], policy=policy)\n",
        "    results5 = DiffAugment(results[5], policy=policy)\n",
        "    results[0] = results0\n",
        "    results[1] = results1\n",
        "    results[2] = results2\n",
        "    results[3] = results3\n",
        "    results[4] = results4\n",
        "    results[5] = results5\n",
        "    \"\"\"\n",
        "    #test_img = torch.stack((img0[0], img1[0], img2[0], img3[0], img4[0], img5[0]))\n",
        "    test_img = torch.stack((img0[0], img1[0]))\n",
        "\n",
        "    with torch.cuda.amp.autocast(): \n",
        "      loss = criterion(results, test_img)\n",
        "\n",
        "    #optimizer.zero_grad()\n",
        "    #loss.backward()\n",
        "    scaler.scale(loss).backward() \n",
        "    #optimizer.step()\n",
        "    scaler.step(optimizer) \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (i + 1) % args.log_interval == 0:\n",
        "        writer.add_scalar('loss', loss.item(), i + 1)\n",
        "\n",
        "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n",
        "        torch.save(model.state_dict(), '{:s}/ckpt/{:d}.pth'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % args.vis_interval == 0:\n",
        "        s_img = torch.cat([img, masked, results[0]])\n",
        "        s_img = make_grid(s_img, nrow=args.batch_size)\n",
        "        save_image(s_img, '{:s}/images/test_{:d}.png'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % 10000:\n",
        "        scheduler.step()\n",
        "\n",
        "    scaler.update() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQQrK_JTyMc"
      },
      "source": [
        "%cd /content/DFNet\n",
        "!python train.py --root /content/train_data --batch_size 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH6JZisX6LRk"
      },
      "source": [
        "# depricated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxsl2_ohXk-z",
        "cellView": "form"
      },
      "source": [
        "#@title differentiable augmentation in train.py + amp (untested)\n",
        "%%writefile /content/DFNet/train.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data import DS\n",
        "from loss import InpaintingLoss\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class InfiniteSampler(data.sampler.Sampler):\n",
        "    def __init__(self, num_samples):\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.loop())\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "    def loop(self):\n",
        "        i = 0\n",
        "        order = np.random.permutation(self.num_samples)\n",
        "        while True:\n",
        "            yield order[i]\n",
        "            i += 1\n",
        "            if i >= self.num_samples:\n",
        "                np.random.seed()\n",
        "                order = np.random.permutation(self.num_samples)\n",
        "                i = 0\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, default='/srv/datasets/Places2')\n",
        "parser.add_argument('--save_dir', type=str, default='./snapshots/default')\n",
        "parser.add_argument('--log_dir', type=str, default='./logs/default')\n",
        "parser.add_argument('--lr', type=float, default=2e-3)\n",
        "parser.add_argument('--max_iter', type=int, default=200000)\n",
        "parser.add_argument('--batch_size', type=int, default=6)\n",
        "parser.add_argument('--n_threads', type=int, default=16)\n",
        "parser.add_argument('--save_model_interval', type=int, default=1000)\n",
        "parser.add_argument('--vis_interval', type=int, default=1000)\n",
        "parser.add_argument('--log_interval', type=int, default=10)\n",
        "parser.add_argument('--image_size', type=int, default=256)\n",
        "parser.add_argument('--resume', type=str)\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs('{:s}/images'.format(args.save_dir))\n",
        "    os.makedirs('{:s}/ckpt'.format(args.save_dir))\n",
        "\n",
        "writer = SummaryWriter(logdir=args.log_dir)\n",
        "\n",
        "size = (args.image_size, args.image_size)\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize(size=size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = DS(args.root, img_tf)\n",
        "\n",
        "iterator_train = iter(data.DataLoader(\n",
        "    dataset, batch_size=args.batch_size,\n",
        "    sampler=InfiniteSampler(len(dataset)),\n",
        "    num_workers=args.n_threads\n",
        "))\n",
        "print(len(dataset))\n",
        "model = DFNet().to(device)\n",
        "\n",
        "lr = args.lr\n",
        "\n",
        "start_iter = 0\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "criterion = InpaintingLoss().to(device)\n",
        "\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "for i in tqdm(range(start_iter, args.max_iter)):\n",
        "    model.train()\n",
        "    \n",
        "    img, mask = [x.to(device) for x in next(iterator_train)]\n",
        "    masked = img * mask\n",
        "\n",
        "    results, alpha, raw = model(masked, mask)\n",
        "\n",
        "    # Diffaugment\n",
        "    img0 = DiffAugment(img[0], policy=policy)\n",
        "    img1 = DiffAugment(img[1], policy=policy)\n",
        "    img2 = DiffAugment(img[2], policy=policy)\n",
        "    img3 = DiffAugment(img[3], policy=policy)\n",
        "    img4 = DiffAugment(img[4], policy=policy)\n",
        "    img5 = DiffAugment(img[5], policy=policy)\n",
        "    \"\"\"\n",
        "    results0 = DiffAugment(results[0], policy=policy)\n",
        "    results1 = DiffAugment(results[1], policy=policy)\n",
        "    results2 = DiffAugment(results[2], policy=policy)\n",
        "    results3 = DiffAugment(results[3], policy=policy)\n",
        "    results4 = DiffAugment(results[4], policy=policy)\n",
        "    results5 = DiffAugment(results[5], policy=policy)\n",
        "    results[0] = results0\n",
        "    results[1] = results1\n",
        "    results[2] = results2\n",
        "    results[3] = results3\n",
        "    results[4] = results4\n",
        "    results[5] = results5\n",
        "    \"\"\"\n",
        "    test_img = torch.stack((img0[0], img1[0], img2[0], img3[0], img4[0], img5[0]))\n",
        "    with torch.cuda.amp.autocast(): \n",
        "      loss = criterion(results, test_img)\n",
        "\n",
        "    #optimizer.zero_grad()\n",
        "    #loss.backward()\n",
        "    scaler.scale(loss).backward() \n",
        "    #optimizer.step()\n",
        "    scaler.step(optimizer) \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (i + 1) % args.log_interval == 0:\n",
        "        writer.add_scalar('loss', loss.item(), i + 1)\n",
        "\n",
        "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n",
        "        torch.save(model.state_dict(), '{:s}/ckpt/{:d}.pth'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % args.vis_interval == 0:\n",
        "        s_img = torch.cat([img, masked, results[0]])\n",
        "        s_img = make_grid(s_img, nrow=args.batch_size)\n",
        "        save_image(s_img, '{:s}/images/test_{:d}.png'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % 10000:\n",
        "        scheduler.step()\n",
        "\n",
        "    scaler.update() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxySKn3L3BiT",
        "cellView": "form"
      },
      "source": [
        "#@title differentiable augmentation in train.py (untested)\n",
        "%%writefile /content/DFNet/train.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data import DS\n",
        "from loss import InpaintingLoss\n",
        "from model import DFNet\n",
        "\n",
        "\n",
        "class InfiniteSampler(data.sampler.Sampler):\n",
        "    def __init__(self, num_samples):\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.loop())\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "    def loop(self):\n",
        "        i = 0\n",
        "        order = np.random.permutation(self.num_samples)\n",
        "        while True:\n",
        "            yield order[i]\n",
        "            i += 1\n",
        "            if i >= self.num_samples:\n",
        "                np.random.seed()\n",
        "                order = np.random.permutation(self.num_samples)\n",
        "                i = 0\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, default='/srv/datasets/Places2')\n",
        "parser.add_argument('--save_dir', type=str, default='./snapshots/default')\n",
        "parser.add_argument('--log_dir', type=str, default='./logs/default')\n",
        "parser.add_argument('--lr', type=float, default=2e-3)\n",
        "parser.add_argument('--max_iter', type=int, default=200000)\n",
        "parser.add_argument('--batch_size', type=int, default=6)\n",
        "parser.add_argument('--n_threads', type=int, default=16)\n",
        "parser.add_argument('--save_model_interval', type=int, default=1000)\n",
        "parser.add_argument('--vis_interval', type=int, default=1000)\n",
        "parser.add_argument('--log_interval', type=int, default=10)\n",
        "parser.add_argument('--image_size', type=int, default=256)\n",
        "parser.add_argument('--resume', type=str)\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs('{:s}/images'.format(args.save_dir))\n",
        "    os.makedirs('{:s}/ckpt'.format(args.save_dir))\n",
        "\n",
        "writer = SummaryWriter(logdir=args.log_dir)\n",
        "\n",
        "size = (args.image_size, args.image_size)\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize(size=size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = DS(args.root, img_tf)\n",
        "\n",
        "iterator_train = iter(data.DataLoader(\n",
        "    dataset, batch_size=args.batch_size,\n",
        "    sampler=InfiniteSampler(len(dataset)),\n",
        "    num_workers=args.n_threads\n",
        "))\n",
        "print(len(dataset))\n",
        "model = DFNet().to(device)\n",
        "\n",
        "lr = args.lr\n",
        "\n",
        "start_iter = 0\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "criterion = InpaintingLoss().to(device)\n",
        "\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "for i in tqdm(range(start_iter, args.max_iter)):\n",
        "    model.train()\n",
        "\n",
        "    img, mask = [x.to(device) for x in next(iterator_train)]\n",
        "    masked = img * mask\n",
        "\n",
        "    results, alpha, raw = model(masked, mask)\n",
        "\n",
        "    # Diffaugment\n",
        "    img0 = DiffAugment(img[0], policy=policy)\n",
        "    img1 = DiffAugment(img[1], policy=policy)\n",
        "    img2 = DiffAugment(img[2], policy=policy)\n",
        "    img3 = DiffAugment(img[3], policy=policy)\n",
        "    img4 = DiffAugment(img[4], policy=policy)\n",
        "    img5 = DiffAugment(img[5], policy=policy)\n",
        "    \"\"\"\n",
        "    results0 = DiffAugment(results[0], policy=policy)\n",
        "    results1 = DiffAugment(results[1], policy=policy)\n",
        "    results2 = DiffAugment(results[2], policy=policy)\n",
        "    results3 = DiffAugment(results[3], policy=policy)\n",
        "    results4 = DiffAugment(results[4], policy=policy)\n",
        "    results5 = DiffAugment(results[5], policy=policy)\n",
        "    results[0] = results0\n",
        "    results[1] = results1\n",
        "    results[2] = results2\n",
        "    results[3] = results3\n",
        "    results[4] = results4\n",
        "    results[5] = results5\n",
        "    \"\"\"\n",
        "    test_img = torch.stack((img0[0], img1[0], img2[0], img3[0], img4[0], img5[0]))\n",
        "    loss = criterion(results, test_img)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % args.log_interval == 0:\n",
        "        writer.add_scalar('loss', loss.item(), i + 1)\n",
        "\n",
        "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n",
        "        torch.save(model.state_dict(), '{:s}/ckpt/{:d}.pth'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % args.vis_interval == 0:\n",
        "        s_img = torch.cat([img, masked, results[0]])\n",
        "        s_img = make_grid(s_img, nrow=args.batch_size)\n",
        "        save_image(s_img, '{:s}/images/test_{:d}.png'.format(args.save_dir, i + 1))\n",
        "\n",
        "    if (i + 1) % 10000:\n",
        "        scheduler.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
